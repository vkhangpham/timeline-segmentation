---
description: 
globs: 
alwaysApply: true
---
## Guiding Development Principles

These principles MUST be adhered to during research and development:

1.  **Development Journal Management:**

    * **Instruction:** Maintain a comprehensive **Development Journal** to track all development items, research, decisions, and progress. This journal must be split into multiple phases for organization.
    * **Step 1:** Create journal files named `dev_journal_phase1.md`, `dev_journal_phase2.md`, and so on, corresponding to the project's development phases.
    * **Step 2:** For each new development item (problem, feature, evaluation, refactor, etc.), add an entry to the current phase's journal file using the specified format below.
    * **Step 3:** Update the status and details of journal entries as work progresses and upon completion.

    ```markdown
    ---
    ID: LETTERS-NUMBERS (LETTERS should be representing for problems, such as "OPTIMIZATION, FEATURE, EVALUATION, REFACTOR..." and numbers should be the order)
    Title: [Descriptive Title]
    Status: [Needs Research & Implementation | In Progress | Successfully Implemented | Blocked | etc.]
    Priority: [Critical | High | Medium | Low]
    Phase: [Phase 1 | Phase 2 | Phase 3 | Phase 4 | Phase 5 | Phase 6]
    DateAdded: YYYY-MM-DD
    DateCompleted: YYYY-MM-DD (only if completed)
    Impact: [Brief description of impact on the project]
    Files:
      - path/to/relevant_file1.py
      - path/to/relevant_file2.js
    ---

    **Problem Description:** [Detailed description of the issue or task, including context.]
    **Goal:** [What you aim to achieve with this development item, including specific metrics if applicable.]
    **Research & Approach:** [Summary of research findings, alternative solutions considered, and the chosen approach with justification.]
    **Solution Implemented & Verified:** [For completed items: detailed explanation of the solution, how it was implemented, and verification steps/results.]
    **Impact on Core Plan:** [How this item affects the overall project strategy, timeline, or dependencies.]
    **Reflection:** [Lessons learned, unexpected challenges, and insights gained during the process.]
    ```

---

2.  **Rigorous Research and Documentation:**

    * **Instruction:** Conduct thorough research before implementing any solution, and meticulously document all findings, considerations, and decision-making processes within the Development Journal.
    * **Step 1:** Before starting implementation, research the latest advancements and best practices in the relevant technical field using academic papers (e.g., from arXiv), reputable technical blogs, and open-source project examples.
    * **Step 2:** Document your research findings, the different approaches considered, the trade-offs of each, and the rationale behind your chosen solution in the Development Journal entry for the relevant development item.
    * **Step 3:** Clearly link your journal entries to specific development items or features you are working on.
    * **Example:** When tasked with optimizing a data query, research various indexing strategies and query optimization techniques. Document your findings, including performance benchmarks and the specific reasons for selecting a particular database index, in the corresponding journal entry.

---

3.  **No Mock Data:**

    * **Instruction:** Never use mock, hard-coded, or synthetic data for development, testing, or demonstration purposes. Always use the real-world data available in the designated project data storage.
    * **Step:** When developing or testing, point your code directly to the appropriate real data files within the project's data directory.
    * **Example:** If your task involves processing specific document types, load actual documents from the project's data storage for all development and testing phases.

---

4.  **Iterative Testing with Real Data Subsets:**

    * **Instruction:** Always test any new algorithm or significant code modification on a representative subset of real data before integrating it into the main module or running it on the full dataset.
    * **Step 1:** Select or create a representative subset of real data from the project's data storage that adequately covers typical scenarios and known edge cases.
    * **Step 2:** Execute your new algorithm or modified code on this specific data subset.
    * **Step 3:** Meticulously document the exact data subset used, the observed behavior, performance characteristics, and any adjustments made to the code based on the testing results.
    * **Example:** If developing a new parsing component, test it on a subset of 5-10 diverse source files (e.g., varying lengths, different structures, known exceptions) from the real data, documenting how each file was handled and any parsing errors encountered.

---

5.  **Critical Quality Evaluation:**

    * **Instruction:** Perform a **careful, thorough, and analytical evaluation** of every new result or completed task. **DO NOT easily accept results.** Set an exceptionally high bar for everything produced. An implementation *working* does not equate to it being *good*; it **MUST** bring real, concrete value to the project and demonstrate clear superiority over existing approaches.
    * **Step 1:** Establish precise quantitative metrics (e.g., accuracy scores, processing time, resource consumption) and qualitative criteria (e.g., output clarity, code maintainability, user experience) for evaluating success.
    * **Step 2:** Conduct a rigorous comparison of the new solution's performance against these defined metrics and, critically, against the established baseline or any previous implementations.
    * **Step 3:** Document the complete evaluation process and its results in detail, providing undeniable evidence such as objective measurements, specific examples of improvement, or comparative analyses.
    * **Step 4:** Clearly articulate the specific, measurable value and demonstrable superiority that the new solution provides to the project. If the new approach does not unequivocally improve upon or significantly enhance the project beyond the current state, it must not be integrated.
    * **Example:** When evaluating a new machine learning model, calculate its F1-score, precision, and recall on a held-out test set. Present these alongside the previous model's metrics. Additionally, perform a qualitative error analysis on a sample of its predictions, documenting specific instances where the new model performs better (or worse) than the old. Conclude by stating, "The new model provides concrete value by reducing error rate by X% and achieving a Y% improvement in [specific metric] over the baseline, thereby enhancing [specific project goal]."

---

6.  **Strict Error Handling (Fail Fast):**

    * **Instruction:** In this experimental project, **do not use any fallbacks or error catching mechanisms** (e.g., `try-catch` blocks, default values for failed operations) under any circumstances. If a problem occurs, the system **must immediately throw an error** to allow for direct tracing to the root cause.
    * **Step 1:** Whenever an unexpected condition, invalid input, or erroneous state is detected, explicitly raise an error or exception.
    * **Step 2:** Avoid wrapping code in `try-catch` blocks that would mask errors or allow execution to continue with potentially corrupted data.
    * **Step 3:** Let the program terminate if an unhandled error occurs; this is the intended behavior for this project phase.
    * **Example:** If a critical configuration file is missing or malformed, instead of attempting to load default values or gracefully degrade functionality, the program should throw a `FileNotFoundError` or `ValueError` and exit.
    * **Correspondence:** This rule directly supports **Rule 8 (Always Find Fundamental Solutions)** by demanding that issues are surfaced immediately for root-cause analysis, and **Rule 7 (Always Check Terminal Logs Carefully)** by ensuring that critical error information is present and unmasked in the logs.

---

7.  **Always Check Terminal Logs Carefully:**

    * **Instruction:** Every time you execute a script, run a test suite, or initiate a data processing pipeline, you **MUST** carefully watch and thoroughly analyze the complete terminal logs.
    * **Step 1:** After starting any process, actively observe the real-time terminal output as it unfolds.
    * **Step 2:** Once the process completes, scroll through the entire log from beginning to end, rather than just looking at the final message.
    * **Step 3:** Actively look for and investigate any `ERROR` messages, `WARNING` messages, unexpected print statements, abnormal patterns in output, or deviations from the anticipated execution flow. These often indicate underlying issues that may not cause an immediate crash but can lead to incorrect results or future problems.
    * **Example:** If running a data migration script, even if the final output states "Migration complete," scrutinize the logs for warnings about skipped records, errors in individual data transformations, or messages indicating transient network issues that might have impacted data integrity.

---

8.  **Always Find Fundamental Solutions:**

    * **Instruction:** Implement robust, fundamental solutions that address the root cause of any problem. Strictly avoid "hotfixes" or manual editing of results.
    * **Step 1:** When a problem arises, analyze its underlying cause thoroughly to understand why it occurs.
    * **Step 2:** Design a solution that modifies the core algorithm or data processing step to prevent recurrence.
    * **Step 3:** Implement the solution directly within the relevant script or module that produces the problematic output.
    * **Example:** If a component consistently generates verbose or repetitive output, modify the generation logic itself to produce concise output directly, rather than adding a post-processing step to trim the output after it's generated.

---

9.  **Prefer Functional Programming to OOP:**

    * **Instruction:** Prioritize functional programming paradigms over Object-Oriented Programming (OOP) where possible.
    * **Step 1:** When designing new code or refactoring existing modules, evaluate if the logic can be expressed using **pure functions** (functions that produce the same output for the same input and have no side effects).
    * **Step 2:** Aim to minimize mutable state and actively use **immutable data structures** where appropriate.
    * **Step 3:** Avoid creating classes or complex object hierarchies unless they provide a clear, undeniable organizational or structural benefit that cannot be achieved functionally.
    * **Example:** Instead of creating a class with methods for sequential data transformation steps (e.g., `load()`, `transform()`, `save()`), implement these as independent functions that take inputs and return outputs, like `load_data(source)`, `apply_transformation(raw_data)`, and `save_data(processed_data, destination)`.

---

10. **Minimal and Well-Organized Codebase:**

    * **Instruction:** Maintain a minimal, clean, and well-organized codebase.
    * **Step 1:** Before adding any new script or module, review the existing project directory structure and integrate your new component logically into an appropriate location.
    * **Step 2:** If you develop an algorithm that is superior to an existing one, commit the current algorithm to version control *before* modifying or replacing it.
    * **Step 3:** Ensure no redundant files or unnecessarily complex structures are introduced. Each file, module, and directory should have a clear, justified purpose.
    * **Example:** Place all utility functions in a designated `utils/` directory and core processing logic in a `processor/` directory. If you improve a file named `core_logic.py`, commit the existing `core_logic.py` and then update that same file; do not create a new file named `new_core_logic.py`.