id,title,content,year,cited_by_count,keywords,children
https://openalex.org/W2250539671,Glove: Global Vectors for Word Representation,"Glove: Global Vectors for Word Representation

Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using arithmetic, but the origin these has remained opaque. We analyze make explicit model properties needed such to emerge word vectors. The result is a new global logbilinear regression that combines advantages two major families literature: matrix factorization local context window methods. Our efficiently leverages statistical information by training only on nonzero elements word-word cooccurrence matrix, rather than entire sparse or individual windows large corpus. produces with meaningful substructure, as evidenced its performance 75% recent analogy task. It also outperforms related models similarity tasks named entity recognition.

word embeddings, language resource, computer science, linguistics, computational linguistics, semantic representation, information fusion, language, semantic evaluation, vector space model, word representation, image representation, machine translation, language model, global vectors, deep learning, distributional semantics, text mining, natural language processing, large language model",2014,29891,word embeddings|language resource|computer science|linguistics|computational linguistics|semantic representation|information fusion|language|semantic evaluation|vector space model|word representation|image representation|machine translation|language model|global vectors|deep learning|distributional semantics|text mining|natural language processing|large language model,https://openalex.org/W2896457183|https://openalex.org/W2962739339|https://openalex.org/W2970641574|https://openalex.org/W2911489562|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W1840435438|https://openalex.org/W2884001105|https://openalex.org/W2893425640|https://openalex.org/W2963918774|https://openalex.org/W2562607067|https://openalex.org/W3030163527|https://openalex.org/W2963625095|https://openalex.org/W2787560479|https://openalex.org/W2517194566|https://openalex.org/W2936695845|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W2413794162|https://openalex.org/W2963969878|https://openalex.org/W2612675303|https://openalex.org/W2742947407|https://openalex.org/W2972324944|https://openalex.org/W3105625590|https://openalex.org/W3019166713|https://openalex.org/W3105966348|https://openalex.org/W2953356739|https://openalex.org/W3185341429|https://openalex.org/W2251135946|https://openalex.org/W2880875857|https://openalex.org/W2964010806|https://openalex.org/W2963042536|https://openalex.org/W2948947170
https://openalex.org/W2896457183,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent models, BERT is designed to pre-train deep bidirectional representations unlabeled text by jointly conditioning on both left and right context in all layers. As result, the pre-trained can be fine-tuned with just one additional output layer create state-of-the-art models wide range of tasks, such as question answering inference, without substantial task-specific architecture modifications. conceptually simple empirically powerful. It obtains results eleven natural processing including pushing GLUE score 80.5% (7.7% point absolute improvement), MultiNLI accuracy 86.7% (4.6% SQuAD v1.1 Test F1 93.2 (1.5 improvement) v2.0 83.1 (5.1 improvement).

computer science, language model, natural language processing, language, language engineering, computational linguistics, deep bidirectional transformers, deep learning, machine translation, language understanding",2018,29822,computer science|language model|natural language processing|language|language engineering|computational linguistics|deep bidirectional transformers|deep learning|machine translation|language understanding,https://openalex.org/W2970641574|https://openalex.org/W2979826702|https://openalex.org/W2911489562|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W2980282514|https://openalex.org/W2964110616|https://openalex.org/W3170841864|https://openalex.org/W2970771982|https://openalex.org/W2963809228|https://openalex.org/W3036601975|https://openalex.org/W3099700870|https://openalex.org/W2970476646|https://openalex.org/W3098824823|https://openalex.org/W2972324944|https://openalex.org/W3019166713|https://openalex.org/W3105966348|https://openalex.org/W2953356739|https://openalex.org/W3098605233|https://openalex.org/W2948947170
https://openalex.org/W1880262756,Latent dirichlet allocation,"Latent dirichlet allocation

We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is three-level hierarchical Bayesian model, in which each item collection modeled finite mixture over an underlying set topics. Each topic is, turn, infinite probabilities. In the context modeling, probabilities provide explicit representation document. present efficient approximate inference techniques based on variational methods and EM algorithm empirical Bayes parameter estimation. report results document classification, collaborative filtering, comparing to unigrams LSI model.

computer science, linguistics, bayesian analysis, automatic summarization, content analysis, information visualization, informetrics, communication, recommender system, content similarity detection, latent dirichlet allocation, keyword extraction, document clustering, text mining, natural language processing, information extraction, clustering, knowledge discovery, information retrieval",2003,26622,computer science|linguistics|bayesian analysis|automatic summarization|content analysis|information visualization|informetrics|communication|recommender system|content similarity detection|latent dirichlet allocation|keyword extraction|document clustering|text mining|natural language processing|information extraction|clustering|knowledge discovery|information retrieval,https://openalex.org/W2174706414|https://openalex.org/W2158266063|https://openalex.org/W2108646579|https://openalex.org/W168564468|https://openalex.org/W2113459411|https://openalex.org/W2095655043|https://openalex.org/W2884001105|https://openalex.org/W2072644219|https://openalex.org/W2158139315|https://openalex.org/W2436001372|https://openalex.org/W2158108973|https://openalex.org/W658020064|https://openalex.org/W2038043464|https://openalex.org/W2098062695|https://openalex.org/W2140124448|https://openalex.org/W2251803266|https://openalex.org/W2165599843|https://openalex.org/W2153848201|https://openalex.org/W2742947407|https://openalex.org/W2130339025|https://openalex.org/W71795751|https://openalex.org/W2250879510|https://openalex.org/W2740168486|https://openalex.org/W2334889010|https://openalex.org/W1714665356|https://openalex.org/W2888482885|https://openalex.org/W2108420397
https://openalex.org/W1536680647,Fast R-CNN,"Fast R-CNN

This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. R-CNN builds on previous work to efficiently classify proposals using deep convolutional networks. Compared work, employs several innovations improve training and testing speed while also increasing detection accuracy. trains the very VGG16 network 9x faster than R-CNN, is 213x at test-time, achieves higher mAP PASCAL VOC 2012. SPPnet, 3x faster, tests 10x more accurate. implemented in Python C++ (using Caffe) available under open-source MIT License https://github.com/rbgirshick/fast-rcnn.

computational imaging, machine vision, natural language processing, neural network (machine learning), cognitive science, computer science, recurrent neural network, convolutional neural network, machine learning, deep reinforcement learning, machine learning research, object detection, deep learning, data science, object recognition, image analysis",2015,20208,computational imaging|machine vision|natural language processing|neural network (machine learning)|cognitive science|computer science|recurrent neural network|convolutional neural network|machine learning|deep reinforcement learning|machine learning research|object detection|deep learning|data science|object recognition|image analysis,https://openalex.org/W2277195237|https://openalex.org/W2605982830|https://openalex.org/W2343052201|https://openalex.org/W2963758027
https://openalex.org/W2157331557,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation

Kyunghyun Cho, Bart van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.

knowledge discovery, word embeddings, computer science, recurrent neural network, language model, machine learning research, deep learning, sequence modelling, natural language processing, machine translation, rnn encoder-decoder, language learning, machine learning, linguistics, data science, computational linguistics, statistical machine translation, neural machine translation, convolutional neural network, phrase representations",2014,16242,knowledge discovery|word embeddings|computer science|recurrent neural network|language model|machine learning research|deep learning|sequence modelling|natural language processing|machine translation|rnn encoder-decoder|language learning|machine learning|linguistics|data science|computational linguistics|statistical machine translation|neural machine translation|convolutional neural network|phrase representations,https://openalex.org/W2964308564|https://openalex.org/W1902237438|https://openalex.org/W1514535095|https://openalex.org/W2964199361|https://openalex.org/W2745461083|https://openalex.org/W2884001105|https://openalex.org/W2963216553|https://openalex.org/W2962965405|https://openalex.org/W2327501763|https://openalex.org/W2193413348|https://openalex.org/W2963084599|https://openalex.org/W2550821151|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W1527575280|https://openalex.org/W1938755728|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W3098605233|https://openalex.org/W2962826786|https://openalex.org/W2963042536|https://openalex.org/W2963963856|https://openalex.org/W2962706528
https://openalex.org/W2964308564,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural Machine Translation by Jointly Learning to Align and Translate

Abstract: Neural machine translation is a recently proposed approach to translation. Unlike the traditional statistical translation, neural aims at building single network that can be jointly tuned maximize performance. The models for often belong family of encoder-decoders and consists an encoder encodes source sentence into fixed-length vector from which decoder generates In this paper, we conjecture use bottleneck in improving performance basic encoder-decoder architecture, propose extend by allowing model automatically (soft-)search parts are relevant predicting target word, without having form these as hard segment explicitly. With new approach, achieve comparable existing state-of-the-art phrase-based system on task English-to-French Furthermore, qualitative analysis reveals (soft-)alignments found agree well with our intuition.

computer science, linguistics, transfer learning, natural language processing, language, neural machine translation, machine learning, neural computation, computational intelligence, deep learning, machine learning research, machine translation, neural network (machine learning), computer-assisted translation",2015,15258,computer science|linguistics|transfer learning|natural language processing|language|neural machine translation|machine learning|neural computation|computational intelligence|deep learning|machine learning research|machine translation|neural network (machine learning)|computer-assisted translation,https://openalex.org/W2130942839|https://openalex.org/W1902237438|https://openalex.org/W1514535095|https://openalex.org/W2949888546|https://openalex.org/W2606974598|https://openalex.org/W2964110616|https://openalex.org/W2963250244|https://openalex.org/W2884001105|https://openalex.org/W2963216553|https://openalex.org/W1544827683|https://openalex.org/W2962965405|https://openalex.org/W2327501763|https://openalex.org/W2963929190|https://openalex.org/W2562607067|https://openalex.org/W2963809228|https://openalex.org/W2613904329|https://openalex.org/W2963206148|https://openalex.org/W2550553598|https://openalex.org/W2963223306|https://openalex.org/W2517194566|https://openalex.org/W2962883855|https://openalex.org/W2550821151|https://openalex.org/W2723293840|https://openalex.org/W2963966654|https://openalex.org/W1527575280|https://openalex.org/W2413794162|https://openalex.org/W2612675303|https://openalex.org/W1663984431|https://openalex.org/W2951534261|https://openalex.org/W2742947407|https://openalex.org/W2886641317|https://openalex.org/W2972324944|https://openalex.org/W3019166713|https://openalex.org/W2963506925|https://openalex.org/W2938704169|https://openalex.org/W2515462165|https://openalex.org/W2962826786|https://openalex.org/W2963963856|https://openalex.org/W2963167310|https://openalex.org/W2888482885|https://openalex.org/W2963096510
https://openalex.org/W2153579005,Distributed Representations of Words and Phrases and their Compositionality,"Distributed Representations of Words and Phrases and their Compositionality

The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions improve both the quality vectors training speed. By subsampling frequent words obtain significant speedup also learn more regular representations. We describe simple alternative to hierarchical softmax called negative sampling.

An inherent limitation their indifference order inability represent idiomatic phrases. For example, meanings Canada Air cannot be easily combined Canada. Motivated by finding phrases in text, show good millions possible.

distributional semantics, word embeddings, natural language processing, distributed learning, distributed representations, cognitive science, semantics, language model, text mining, linguistics, language, representation analysis, semantic representation, computational linguistics",2013,14589,distributional semantics|word embeddings|natural language processing|distributed learning|distributed representations|cognitive science|semantics|language model|text mining|linguistics|language|representation analysis|semantic representation|computational linguistics,https://openalex.org/W2250539671|https://openalex.org/W2896457183|https://openalex.org/W2157331557|https://openalex.org/W1832693441|https://openalex.org/W2962739339|https://openalex.org/W2493916176|https://openalex.org/W2950635152|https://openalex.org/W2911489562|https://openalex.org/W2296283641|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W2963012544|https://openalex.org/W2963026768|https://openalex.org/W2131744502|https://openalex.org/W2884001105|https://openalex.org/W2893425640|https://openalex.org/W2963918774|https://openalex.org/W2562607067|https://openalex.org/W2265846598|https://openalex.org/W2250861254|https://openalex.org/W2963625095|https://openalex.org/W2517194566|https://openalex.org/W2936695845|https://openalex.org/W2125031621|https://openalex.org/W658020064|https://openalex.org/W2250966211|https://openalex.org/W2251803266|https://openalex.org/W2131571251|https://openalex.org/W3105625590|https://openalex.org/W2946417913|https://openalex.org/W3019166713|https://openalex.org/W2250879510|https://openalex.org/W2953356739|https://openalex.org/W2964167098|https://openalex.org/W2880875857|https://openalex.org/W2891177506|https://openalex.org/W2340954483|https://openalex.org/W2963042536|https://openalex.org/W2962706528
https://openalex.org/W1576632330,How to do things with words,"How to do things with words

* Lecture I II III IV V VI VII VIII IX X XI XII

applied linguistics, language grounding, linguistics, natural language processing, speech processing, language, general linguistics, language-based approach, syntax, communication, lexical semantics, language acquisition, semantics, semantic processing, lexicon, narrative, language learning, context (linguistics)",1962,14490,applied linguistics|language grounding|linguistics|natural language processing|speech processing|language|general linguistics|language-based approach|syntax|communication|lexical semantics|language acquisition|semantics|semantic processing|lexicon|narrative|language learning|context (linguistics),https://openalex.org/W2102381086
https://openalex.org/W1614298861,Efficient Estimation of Word Representations in Vector Space,"Efficient Estimation of Word Representations in Vector Space

We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality these is measured in a word similarity task, and the results are compared to previously best performing techniques based on different types neural networks. observe improvements accuracy at much lower computational cost, i.e. it takes less than day learn high vectors 1.6 billion set. Furthermore, we show that provide state-of-the-art performance our test set measuring syntactic semantic similarities.

word representations, natural language processing, computer science, efficient estimation, vector space",2013,14484,word representations|natural language processing|computer science|efficient estimation|vector space,https://openalex.org/W2250539671|https://openalex.org/W2153579005|https://openalex.org/W3034999214|https://openalex.org/W2296283641|https://openalex.org/W2963626623|https://openalex.org/W2884001105|https://openalex.org/W2561715562|https://openalex.org/W2125031621|https://openalex.org/W1527575280|https://openalex.org/W1938755728|https://openalex.org/W2251803266|https://openalex.org/W3105625590|https://openalex.org/W3019166713|https://openalex.org/W2740168486|https://openalex.org/W2170738476|https://openalex.org/W2251135946|https://openalex.org/W2252215182|https://openalex.org/W2340954483|https://openalex.org/W2963042536
https://openalex.org/W2130942839,Sequence to Sequence Learning with Neural Networks,"Sequence to Sequence Learning with Neural Networks

Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets available, they cannot be used to map sequences sequences. In this paper, we present a general end-to-end approach sequence makes minimal assumptions the structure. Our method uses multilayered Long Short-Term Memory (LSTM) input vector of fixed dimensionality, and then another deep LSTM decode target from vector. main result is an English French translation task WMT'14 dataset, translations produced by achieve BLEU score 34.8 entire test set, where LSTM's was penalized out-of-vocabulary words. Additionally, did not difficulty long sentences. For comparison, phrase-based SMT system achieves 33.3 same dataset. When rerank 1000 hypotheses aforementioned system, its increases 36.5, which close previous best task. The also learned sensible phrase sentence representations sensitive word order relatively invariant active passive voice. Finally, found reversing words in all source sentences (but sentences) improved markedly, because doing so introduced many short term dependencies between made optimization problem easier.

neural networks, knowledge discovery, sparse neural network, neural network (machine learning), sequential learning, computer science, recurrent neural network, deep reinforcement learning, machine learning research, deep learning, sequence modelling, machine vision, natural language processing, natural language generation, machine learning, data science, neural computation, cognitive science, computational intelligence",2014,12686,neural networks|knowledge discovery|sparse neural network|neural network (machine learning)|sequential learning|computer science|recurrent neural network|deep reinforcement learning|machine learning research|deep learning|sequence modelling|machine vision|natural language processing|natural language generation|machine learning|data science|neural computation|cognitive science|computational intelligence,https://openalex.org/W1902237438|https://openalex.org/W1514535095|https://openalex.org/W2964199361|https://openalex.org/W2950178297|https://openalex.org/W2606974598|https://openalex.org/W2884001105|https://openalex.org/W2963216553|https://openalex.org/W2962965405|https://openalex.org/W2327501763|https://openalex.org/W2963918774|https://openalex.org/W2963206148|https://openalex.org/W2963223306|https://openalex.org/W2550821151|https://openalex.org/W1591706642|https://openalex.org/W2250966211|https://openalex.org/W1527575280|https://openalex.org/W1938755728|https://openalex.org/W2139501017|https://openalex.org/W1663984431|https://openalex.org/W2886641317|https://openalex.org/W3105625590|https://openalex.org/W3019166713|https://openalex.org/W3098605233|https://openalex.org/W2880875857|https://openalex.org/W2515462165|https://openalex.org/W2962826786|https://openalex.org/W2963042536|https://openalex.org/W2963963856|https://openalex.org/W2962706528|https://openalex.org/W2963167310|https://openalex.org/W2963096510
https://openalex.org/W2950577311,Efficient Estimation of Word Representations in Vector Space,"Efficient Estimation of Word Representations in Vector Space

We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality these is measured in a word similarity task, and the results are compared to previously best performing techniques based on different types neural networks. observe improvements accuracy at much lower computational cost, i.e. it takes less than day learn high vectors 1.6 billion set. Furthermore, we show that provide state-of-the-art performance our test set measuring syntactic semantic similarities.

computer science, efficient estimation, vector space, natural language processing, word representations",2013,12371,computer science|efficient estimation|vector space|natural language processing|word representations,https://openalex.org/W2493916176|https://openalex.org/W2950133940|https://openalex.org/W3034999214|https://openalex.org/W2949547296|https://openalex.org/W2296283641|https://openalex.org/W2277195237|https://openalex.org/W2949888546|https://openalex.org/W2963626623|https://openalex.org/W2131744502|https://openalex.org/W2141599568|https://openalex.org/W2884001105|https://openalex.org/W3030163527|https://openalex.org/W2561715562|https://openalex.org/W658020064|https://openalex.org/W2126725946|https://openalex.org/W2964236337|https://openalex.org/W1938755728|https://openalex.org/W2251803266|https://openalex.org/W2131571251|https://openalex.org/W2742947407|https://openalex.org/W3105625590|https://openalex.org/W3019166713|https://openalex.org/W2740168486
https://openalex.org/W4295312788,"PyTorch: An Imperative Style, High-Performance Deep Learning Library","PyTorch: An Imperative Style, High-Performance Deep Learning Library

Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine library that shows these two goals are in fact compatible: it provides an imperative and Pythonic programming style supports code as model, makes debugging easy consistent with other popular scientific computing libraries, while remaining efficient supporting hardware accelerators such GPUs. In this paper, we detail the principles drove implementation of how they reflected its architecture. We emphasize every aspect regular Python program under full control user. also explain careful pragmatic key components runtime enables them to work together achieve compelling performance. demonstrate efficiency individual subsystems, well overall speed several common benchmarks.

imperative style, natural language processing, computer science, machine learning, large language model, deep learning",2019,11899,imperative style|natural language processing|computer science|machine learning|large language model|deep learning,
https://openalex.org/W1832693441,Convolutional Neural Networks for Sentence Classification,"Convolutional Neural Networks for Sentence Classification

We report on a series of experiments with convolutional neural networks (CNN) trained top pre-trained word vectors for sentence-level classification tasks.We show that simple CNN little hyperparameter tuning and static achieves excellent results multiple benchmarks.Learning task-specific through fine-tuning offers further gains in performance.We additionally propose modification to the architecture allow use both vectors.The models discussed herein improve upon state art 4 out 7 tasks, which include sentiment analysis question classification.

knowledge discovery, nlp task, neural network (machine learning), sequential learning, computer science, machine learning research, deep learning, neuroscience, natural language processing, machine translation, semantic interpretation, text processing, sentence classification, machine learning, neural computation, cognitive science, computational intelligence, semantic evaluation, convolutional neural network",2014,11322,knowledge discovery|nlp task|neural network (machine learning)|sequential learning|computer science|machine learning research|deep learning|neuroscience|natural language processing|machine translation|semantic interpretation|text processing|sentence classification|machine learning|neural computation|cognitive science|computational intelligence|semantic evaluation|convolutional neural network,https://openalex.org/W2963626623|https://openalex.org/W2963012544|https://openalex.org/W2884001105|https://openalex.org/W2963223306|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W1938755728|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2170738476|https://openalex.org/W3185341429|https://openalex.org/W2251135946|https://openalex.org/W3098605233|https://openalex.org/W2891177506|https://openalex.org/W2963042536
https://openalex.org/W4294170691,Distributed Representations of Words and Phrases and their Compositionality,"Distributed Representations of Words and Phrases and their Compositionality

The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions improve both the quality vectors training speed. By subsampling frequent words obtain significant speedup also learn more regular representations. We describe simple alternative to hierarchical softmax called negative sampling. An inherent limitation their indifference order inability represent idiomatic phrases. For example, meanings ""Canada"" ""Air"" cannot be easily combined ""Air Canada"". Motivated by finding phrases in text, show good millions possible.

distributed learning, linguistics, representation analysis, semantic representation, language, semantics, natural language processing, text mining, computational linguistics, distributional semantics, word embeddings, distributed representations, cognitive science, language model",2013,11210,distributed learning|linguistics|representation analysis|semantic representation|language|semantics|natural language processing|text mining|computational linguistics|distributional semantics|word embeddings|distributed representations|cognitive science|language model,https://openalex.org/W1832693441|https://openalex.org/W2962739339|https://openalex.org/W2911489562|https://openalex.org/W2296283641|https://openalex.org/W3035390927|https://openalex.org/W2963026768|https://openalex.org/W2884001105|https://openalex.org/W2893425640|https://openalex.org/W2963929190|https://openalex.org/W2963918774|https://openalex.org/W2562607067|https://openalex.org/W2250861254|https://openalex.org/W2251803266|https://openalex.org/W3105625590|https://openalex.org/W3019166713|https://openalex.org/W2953356739|https://openalex.org/W2964167098|https://openalex.org/W2891177506|https://openalex.org/W2963042536
https://openalex.org/W2147800946,Backpropagation Applied to Handwritten Zip Code Recognition,"Backpropagation Applied to Handwritten Zip Code Recognition

The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such integrated into a backpropagation network through architecture network. approach has been successfully applied recognition handwritten zip code digits provided U.S. Postal Service. A single learns entire operation, going normalized image character final classification.

pattern recognition, computer science, natural language processing, character recognition, text recognition, machine learning, deep learning, automatic classification, zip code recognition",1989,9899,pattern recognition|computer science|natural language processing|character recognition|text recognition|machine learning|deep learning|automatic classification|zip code recognition,https://openalex.org/W1536680647|https://openalex.org/W3094502228|https://openalex.org/W2963012544|https://openalex.org/W2951548327|https://openalex.org/W3171007011|https://openalex.org/W1922655562|https://openalex.org/W3019166713|https://openalex.org/W1942214758
https://openalex.org/W2962739339,Deep Contextualized Word Representations,"Deep Contextualized Word Representations

Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Proceedings of the 2018 Conference North American Chapter Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.

language learning, language science, knowledge discovery, word representations, computational semantics, word embeddings, natural language processing, semantic similarity, cognitive science, language model, text mining, linguistics, language, deep learning, semantic representation, computational linguistics",2018,9732,language learning|language science|knowledge discovery|word representations|computational semantics|word embeddings|natural language processing|semantic similarity|cognitive science|language model|text mining|linguistics|language|deep learning|semantic representation|computational linguistics,https://openalex.org/W2896457183|https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W3035390927|https://openalex.org/W2963026768|https://openalex.org/W2923014074|https://openalex.org/W2964110616|https://openalex.org/W2970771982|https://openalex.org/W2963809228|https://openalex.org/W3036601975|https://openalex.org/W2970476646|https://openalex.org/W2970419734|https://openalex.org/W2972324944|https://openalex.org/W3105625590|https://openalex.org/W2946417913|https://openalex.org/W3019166713|https://openalex.org/W2953356739|https://openalex.org/W3185341429|https://openalex.org/W2963691697|https://openalex.org/W3098605233|https://openalex.org/W2880875857
https://openalex.org/W1574901103,Foundations of Statistical Natural Language Processing,"Foundations of Statistical Natural Language Processing

Statistical approaches to processing natural language text have become dominant in recent years. This foundational is the first comprehensive introduction statistical (NLP) appear. The book contains all theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical linguistic foundations, as well detailed discussion methods, allowing students researchers construct their own implementations. covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, other applications.

linguistics, statistical methodology, language model, natural language processing, machine learning, computational linguistics, data science, nlp task, statistics",1999,9676,linguistics|statistical methodology|language model|natural language processing|machine learning|computational linguistics|data science|nlp task|statistics,https://openalex.org/W2118020653|https://openalex.org/W1579838312|https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2092654472|https://openalex.org/W2146089916|https://openalex.org/W2436001372|https://openalex.org/W2169818249|https://openalex.org/W2141766660|https://openalex.org/W2250966211|https://openalex.org/W1916559533|https://openalex.org/W1423339008|https://openalex.org/W2148506018
https://openalex.org/W3094502228,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.

image analysis, pattern recognition, computer science, image classification, image search, natural language processing, image communication, image retrieval, text recognition, computer vision, machine learning, feature detection, data science, multimedia information processing, image representation, machine learning research, machine vision, image recognition",2020,9670,image analysis|pattern recognition|computer science|image classification|image search|natural language processing|image communication|image retrieval|text recognition|computer vision|machine learning|feature detection|data science|multimedia information processing|image representation|machine learning research|machine vision|image recognition,https://openalex.org/W3170841864
https://openalex.org/W2171960770,Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions,"Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions

This paper presents an overview of the field recommender systems and describes current generation recommendation methods that are usually classified into following three main categories: content-based, collaborative, hybrid approaches. also various limitations discusses possible extensions can improve capabilities make applicable to even broader range applications. These include, among others, improvement understanding users items, incorporation contextual information process, support for multicriteria ratings, a provision more flexible less intrusive types recommendations.

computer science, machine learning, ranking algorithm, recommender system, data science, collaborative filtering, possible extensions, online information, data mining, conversational recommender system, human-computer interaction, information retrieval, systems engineering, deep learning, natural language processing, information filtering system, knowledge discovery, learning to rank, next generation",2005,9581,computer science|machine learning|ranking algorithm|recommender system|data science|collaborative filtering|possible extensions|online information|data mining|conversational recommender system|human-computer interaction|information retrieval|systems engineering|deep learning|natural language processing|information filtering system|knowledge discovery|learning to rank|next generation,https://openalex.org/W1560147776
https://openalex.org/W4297734170,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural Machine Translation by Jointly Learning to Align and Translate

Neural machine translation is a recently proposed approach to translation. Unlike the traditional statistical translation, neural aims at building single network that can be jointly tuned maximize performance. The models for often belong family of encoder-decoders and consists an encoder encodes source sentence into fixed-length vector from which decoder generates In this paper, we conjecture use bottleneck in improving performance basic encoder-decoder architecture, propose extend by allowing model automatically (soft-)search parts are relevant predicting target word, without having form these as hard segment explicitly. With new approach, achieve comparable existing state-of-the-art phrase-based system on task English-to-French Furthermore, qualitative analysis reveals (soft-)alignments found agree well with our intuition.

transfer learning, neural computation, natural language processing, machine translation, computer-assisted translation, neural network (machine learning), computational intelligence, computer science, neural machine translation, machine learning, linguistics, language, deep learning, machine learning research",2014,9323,transfer learning|neural computation|natural language processing|machine translation|computer-assisted translation|neural network (machine learning)|computational intelligence|computer science|neural machine translation|machine learning|linguistics|language|deep learning|machine learning research,https://openalex.org/W2606974598|https://openalex.org/W2964110616|https://openalex.org/W2963250244|https://openalex.org/W2884001105|https://openalex.org/W2963216553|https://openalex.org/W2962965405|https://openalex.org/W2327501763|https://openalex.org/W2963929190|https://openalex.org/W2562607067|https://openalex.org/W2963809228|https://openalex.org/W2963206148|https://openalex.org/W2550553598|https://openalex.org/W2963223306|https://openalex.org/W2963966654|https://openalex.org/W2413794162|https://openalex.org/W2886641317|https://openalex.org/W2972324944|https://openalex.org/W3019166713|https://openalex.org/W2963506925|https://openalex.org/W2515462165|https://openalex.org/W2962826786|https://openalex.org/W2963963856|https://openalex.org/W2963167310|https://openalex.org/W2888482885|https://openalex.org/W2963096510
https://openalex.org/W2160547390,$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation,"$rm K$-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation

In recent years there has been a growing interest in the study of sparse representation signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by linear combinations these atoms. Applications use many and include compression, regularization inverse problems, feature extraction, more. Recent activity this field concentrated mainly on pursuit algorithms decompose with respect to given dictionary. Designing dictionaries better fit above model can be done either selecting one from prespecified set transforms or adapting training Both techniques have considered, but topic is largely still open. paper we propose novel algorithm for order achieve signal representations. Given signals, seek leads best each member set, under strict sparsity constraints. We present new method-the K-SVD algorithm-generalizing K-means clustering process. iterative method alternates between coding examples based current process updating atoms data. The update columns combined representations, thereby accelerating convergence. flexible work any (e.g., basis pursuit, FOCUSS, matching pursuit). analyze demonstrate its results both synthetic tests applications real image data

image analysis, pattern recognition, computer science, computational imaging, dimensionality reduction, compressive sensing, natural language processing, hierarchical classification, clustering, applied mathematics, data science, sparse representation, image representation, machine learning research, feature construction, overcomplete dictionaries, computational science",2006,8999,image analysis|pattern recognition|computer science|computational imaging|dimensionality reduction|compressive sensing|natural language processing|hierarchical classification|clustering|applied mathematics|data science|sparse representation|image representation|machine learning research|feature construction|overcomplete dictionaries|computational science,https://openalex.org/W2005876975
https://openalex.org/W4292779060,Language Models are Few-Shot Learners,"Language Models are Few-Shot Learners

Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.

language learning, natural language processing, language model, language, few-shot learners, few-shot learning",2020,8859,language learning|natural language processing|language model|language|few-shot learners|few-shot learning,
https://openalex.org/W2970971581,"PyTorch: An Imperative Style, High-Performance Deep Learning Library","PyTorch: An Imperative Style, High-Performance Deep Learning Library

Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine library that shows these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style supports code as model, makes debugging easy consistent with other popular scientific computing libraries, while remaining efficient supporting hardware accelerators such GPUs. In this paper, we detail the drove implementation of how they reflected its architecture. We emphasize every aspect regular Python program under full control user. also explain careful pragmatic key components runtime enables them work together achieve compelling performance. demonstrate efficiency individual subsystems, well overall speed several commonly used benchmarks.

imperative style, natural language processing, computer science, machine learning, large language model, deep learning",2019,8716,imperative style|natural language processing|computer science|machine learning|large language model|deep learning,
https://openalex.org/W2493916176,Enriching Word Vectors with Subword Information,"Enriching Word Vectors with Subword Information

Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is limitation, especially languages with vocabularies and rare words. In this paper, we propose new approach based skipgram model, where represented as bag character n-grams. A representation associated n-gram; words being sum these representations. Our method fast, allowing train quickly allows us compute did not appear in training data. We evaluate our nine different languages, both similarity analogy By comparing recently proposed morphological show vectors achieve state-of-the-art performance

word embeddings, computer science, linguistics, keyword extraction, text mining, language model, natural language processing, topic model, semantic evaluation, terminology extraction, computational linguistics, knowledge discovery, nlp task, machine learning research, word vectors, information retrieval, subword information",2017,8508,word embeddings|computer science|linguistics|keyword extraction|text mining|language model|natural language processing|topic model|semantic evaluation|terminology extraction|computational linguistics|knowledge discovery|nlp task|machine learning research|word vectors|information retrieval|subword information,https://openalex.org/W2962739339|https://openalex.org/W2884001105|https://openalex.org/W2787560479|https://openalex.org/W2914120296|https://openalex.org/W3105625590|https://openalex.org/W3037109418|https://openalex.org/W2963096510
https://openalex.org/W2154652894,ROUGE: A Package for Automatic Evaluation of Summaries,"ROUGE: A Package for Automatic Evaluation of Summaries

ROUGE stands for Recall-Oriented Understudy Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it other (ideal) summaries created humans. The count number overlapping units such as n-gram, word sequences, and pairs between computer-generated be evaluated ideal This paper introduces four different measures: ROUGE-N, ROUGE-L, ROUGE-W, ROUGE-S included in summarization evaluation package their evaluations. Three them have been used Document Understanding Conference (DUC) 2004, large-scale sponsored NIST.

computer science, knowledge representation and reasoning, text mining, natural language processing, method validation, automated assessment, semantic evaluation, data science, entity summarization, program evaluation, machine learning research, automatic summarization, automatic annotation tool, information retrieval, knowledge management, automatic evaluation",2004,7904,computer science|knowledge representation and reasoning|text mining|natural language processing|method validation|automated assessment|semantic evaluation|data science|entity summarization|program evaluation|machine learning research|automatic summarization|automatic annotation tool|information retrieval|knowledge management|automatic evaluation,https://openalex.org/W2745461083|https://openalex.org/W2606974598|https://openalex.org/W2962965405|https://openalex.org/W2613904329|https://openalex.org/W2963084599|https://openalex.org/W2550553598|https://openalex.org/W2936695845|https://openalex.org/W2970419734|https://openalex.org/W2139501017|https://openalex.org/W2612675303
https://openalex.org/W2118020653,Machine learning in automated text categorization,"Machine learning in automated text categorization

The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to increased availability documents digital form and ensuing need organize them. In research community dominant approach this problem is based on machine learning techniques: general inductive process automatically builds classifier by learning, from set preclassified documents, characteristics categories. advantages over knowledge engineering (consisting manual definition domain experts) are very good effectiveness, considerable savings terms expert labor power, straightforward portability different domains. This survey discusses main approaches text that fall within paradigm. We will discuss detail issues pertaining three problems, namely, document representation, construction, evaluation.

computer science, text mining, natural language processing, automated text categorization, machine learning, text segmentation, automatic classification, automated machine learning",2002,7691,computer science|text mining|natural language processing|automated text categorization|machine learning|text segmentation|automatic classification|automated machine learning,https://openalex.org/W2250539671|https://openalex.org/W2150102617|https://openalex.org/W2120779048|https://openalex.org/W3105625590|https://openalex.org/W2161793142
https://openalex.org/W1902237438,Effective Approaches to Attention-based Neural Machine Translation,"Effective Approaches to Attention-based Neural Machine Translation

An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes mechanism: a global approach which always attends all words local one that only looks at subset time.We demonstrate effectiveness both approaches WMT tasks between English German in directions.With attention, we achieve significant gain 5.0 BLEU points over non-attentional systems already incorporate known techniques such as dropout.Our ensemble model using different attention yields new state-of-the-art result WMT'15 task with 25.9 points, an improvement 1.0 existing best system backed NMT n-gram reranker. 1

language learning, neural computation, natural language processing, machine translation, attention, cognitive science, computational intelligence, computer science, neural machine translation",2015,7464,language learning|neural computation|natural language processing|machine translation|attention|cognitive science|computational intelligence|computer science|neural machine translation,https://openalex.org/W2963250244|https://openalex.org/W2963216553|https://openalex.org/W2963809228|https://openalex.org/W2550821151|https://openalex.org/W2938704169
https://openalex.org/W1632114991,Building a Large Annotated Corpus of English: The Penn Treebank,"Building a Large Annotated Corpus of English: The Penn Treebank

Abstract : As a result of this grant, the researchers have now published oil CDROM corpus over 4 million words running text annotated with part-of- speech (POS) tags, 3 that material assigned skeletal grammatical structure. This includes fully hand-parsed version classic Brown corpus. About one half papers at ACL Workshop on Using Large Text Corpora past summer were based materials generated by grant.

language resource, large annotated corpus, corpus linguistics, computational linguistics, literature, english language, narrative, speech corpus, treebanks, language, keyword extraction, language model, annotation tool, english study, language corpus, english, natural language processing, grammar, penn treebank, world english",1993,7457,language resource|large annotated corpus|corpus linguistics|computational linguistics|literature|english language|narrative|speech corpus|treebanks|language|keyword extraction|language model|annotation tool|english study|language corpus|english|natural language processing|grammar|penn treebank|world english,https://openalex.org/W2962739339|https://openalex.org/W2963748441|https://openalex.org/W1996430422|https://openalex.org/W2141599568|https://openalex.org/W1659833910|https://openalex.org/W2092654472|https://openalex.org/W2146089916|https://openalex.org/W2787560479|https://openalex.org/W2963223306|https://openalex.org/W2158108973|https://openalex.org/W2117400858|https://openalex.org/W1535015163|https://openalex.org/W2141766660|https://openalex.org/W1938755728|https://openalex.org/W2153848201|https://openalex.org/W2972324944|https://openalex.org/W3019166713|https://openalex.org/W2016534914|https://openalex.org/W2161793142
https://openalex.org/W3216401400,Praat: Doing Phonetics by Computer,"Praat: Doing Phonetics by Computer

Ear and Hearing: March 2011 - Volume 32 Issue 2 p 266 doi: 10.1097/AUD.0b013e31821473f7

computer science, linguistics, speech recognition, human-computer interaction, natural language processing, speech processing, language, spoken language processing, speech synthesis, computational linguistics, phonology, speech technology, speech communication, computer-assisted language learning, spoken language technology, phonetics",2011,7194,computer science|linguistics|speech recognition|human-computer interaction|natural language processing|speech processing|language|spoken language processing|speech synthesis|computational linguistics|phonology|speech technology|speech communication|computer-assisted language learning|spoken language technology|phonetics,
https://openalex.org/W2963684088,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks

In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised CNNs received less attention. this work we hope to help bridge the gap between success of for and learning. We introduce a class called deep generative adversarial (DCGANs), that have certain architectural constraints, demonstrate they are strong candidate Training on various image datasets, show convincing evidence our pair learns hierarchy representations from object parts scenes both generator discriminator. Additionally, use learned features novel tasks - demonstrating their applicability as general representations.

digital image processing, computer science, machine learning research, deep learning, generative ai, machine vision, natural language processing, image representation, unsupervised representation, computational imaging, machine learning, generative adversarial network, image classification, data science, cognitive science, computational intelligence, dimensionality reduction, convolutional neural network, image analysis",2015,6973,digital image processing|computer science|machine learning research|deep learning|generative ai|machine vision|natural language processing|image representation|unsupervised representation|computational imaging|machine learning|generative adversarial network|image classification|data science|cognitive science|computational intelligence|dimensionality reduction|convolutional neural network|image analysis,https://openalex.org/W2405756170|https://openalex.org/W2963966654|https://openalex.org/W2949999304
https://openalex.org/W2123442489,The Stanford CoreNLP Natural Language Processing Toolkit,"The Stanford CoreNLP Natural Language Processing Toolkit

Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, David McClosky. Proceedings of 52nd Annual Meeting the Association for Computational Linguistics: System Demonstrations. 2014.

computer science, linguistics, knowledge representation and reasoning, keyword extraction, language model, natural language processing, natural language generation, natural language interface, syntactic parsing, computational linguistics, data science, knowledge discovery, deep learning, nlp task, machine learning research, machine translation, semantic parsing, spoken language technology",2014,6760,computer science|linguistics|knowledge representation and reasoning|keyword extraction|language model|natural language processing|natural language generation|natural language interface|syntactic parsing|computational linguistics|data science|knowledge discovery|deep learning|nlp task|machine learning research|machine translation|semantic parsing|spoken language technology,https://openalex.org/W2979826702|https://openalex.org/W2277195237|https://openalex.org/W2980282514|https://openalex.org/W2962965405|https://openalex.org/W2250966211|https://openalex.org/W3098824823|https://openalex.org/W2970419734|https://openalex.org/W2963969878|https://openalex.org/W2612675303|https://openalex.org/W1663984431|https://openalex.org/W2889787757|https://openalex.org/W2963691697|https://openalex.org/W2880875857|https://openalex.org/W3037109418
https://openalex.org/W2950635152,Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation,"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation

In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent networks (RNN). One encodes sequence symbols into fixed-length vector representation, and the other decodes representation another symbols. The encoder decoder proposed are jointly trained to maximize conditional probability target given source sequence. performance statistical machine translation system is empirically found improve by using probabilities phrase pairs computed as an additional feature in existing log-linear model. Qualitatively, show learns semantically syntactically meaningful linguistic phrases.

computer science, knowledge discovery, word embeddings, convolutional neural network, phrase representations, statistical machine translation, computational linguistics, data science, deep learning, language model, linguistics, machine learning, machine learning research, natural language processing, machine translation, rnn encoder-decoder, sequence modelling, neural machine translation, recurrent neural network, language learning",2014,6555,computer science|knowledge discovery|word embeddings|convolutional neural network|phrase representations|statistical machine translation|computational linguistics|data science|deep learning|language model|linguistics|machine learning|machine learning research|natural language processing|machine translation|rnn encoder-decoder|sequence modelling|neural machine translation|recurrent neural network|language learning,https://openalex.org/W2130942839|https://openalex.org/W1902237438|https://openalex.org/W2964199361|https://openalex.org/W2950178297|https://openalex.org/W2133564696|https://openalex.org/W2949888546|https://openalex.org/W2963216553|https://openalex.org/W2962965405|https://openalex.org/W2613904329|https://openalex.org/W2962883855|https://openalex.org/W1938755728|https://openalex.org/W3105625590|https://openalex.org/W2963691697|https://openalex.org/W3098605233|https://openalex.org/W2963042536|https://openalex.org/W2963963856
https://openalex.org/W2950133940,Distributed Representations of Words and Phrases and their Compositionality,"Distributed Representations of Words and Phrases and their Compositionality

The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions improve both the quality vectors training speed. By subsampling frequent words obtain significant speedup also learn more regular representations. We describe simple alternative to hierarchical softmax called negative sampling. An inherent limitation their indifference order inability represent idiomatic phrases. For example, meanings Canada Air cannot be easily combined Canada. Motivated by finding phrases in text, show good millions possible.

distributional semantics, word embeddings, natural language processing, distributed learning, distributed representations, cognitive science, semantics, language model, text mining, linguistics, language, representation analysis, semantic representation, computational linguistics",2013,6424,distributional semantics|word embeddings|natural language processing|distributed learning|distributed representations|cognitive science|semantics|language model|text mining|linguistics|language|representation analysis|semantic representation|computational linguistics,https://openalex.org/W2173520492|https://openalex.org/W2963929190|https://openalex.org/W2787560479|https://openalex.org/W2962883855|https://openalex.org/W2914120296|https://openalex.org/W2964236337|https://openalex.org/W2794557536|https://openalex.org/W2612675303
https://openalex.org/W2963840672,Multi-Scale Context Aggregation by Dilated Convolutions,"Multi-Scale Context Aggregation by Dilated Convolutions

State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed image classification. However, dense prediction and classification structurally different. In this work, we develop a new network module is specifically prediction. The presented uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. architecture the fact support exponential expansion receptive field loss resolution or coverage. We show context increases accuracy state-of-the-art systems. addition, examine adaptation simplifying adapted can increase accuracy.

dilated convolutions, scene understanding, computer vision, machine vision, multi-scale context aggregation, natural language processing, computer science, convolutional neural network, machine learning, deep learning, data science, scene analysis, image analysis",2015,6319,dilated convolutions|scene understanding|computer vision|machine vision|multi-scale context aggregation|natural language processing|computer science|convolutional neural network|machine learning|deep learning|data science|scene analysis|image analysis,https://openalex.org/W2737258237|https://openalex.org/W3170841864
https://openalex.org/W2173520492,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks

In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised CNNs received less attention. this work we hope to help bridge the gap between success of for and learning. We introduce a class called deep generative adversarial (DCGANs), that have certain architectural constraints, demonstrate they are strong candidate Training on various image datasets, show convincing evidence our pair learns hierarchy representations from object parts scenes both generator discriminator. Additionally, use learned features novel tasks - demonstrating their applicability as general representations.

computer science, generative ai, convolutional neural network, cognitive science, dimensionality reduction, unsupervised representation, image classification, digital image processing, data science, deep learning, machine learning, generative adversarial network, computational imaging, machine learning research, natural language processing, machine vision, image analysis, computational intelligence, image representation",2015,6233,computer science|generative ai|convolutional neural network|cognitive science|dimensionality reduction|unsupervised representation|image classification|digital image processing|data science|deep learning|machine learning|generative adversarial network|computational imaging|machine learning research|natural language processing|machine vision|image analysis|computational intelligence|image representation,https://openalex.org/W2963373786|https://openalex.org/W2962879692|https://openalex.org/W2963226019|https://openalex.org/W2605135824
https://openalex.org/W2962770929,A Style-Based Generator Architecture for Generative Adversarial Networks,"A Style-Based Generator Architecture for Generative Adversarial Networks

We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new leads to automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) stochastic variation in the generated images freckles, hair), it enables intuitive, scale-specific control synthesis. improves state-of-the-art terms traditional distribution quality metrics, demonstrably better interpolation properties, also disentangles latent factors variation. To quantify disentanglement, we two new, automated methods that are applicable any architecture. Finally, introduce a highly varied high-quality dataset faces.

image analysis, pattern recognition, computer science, natural language processing, natural language generation, computer graphic, generative adversarial network, machine learning, generative ai, style transfer, architecture, deep learning, image representation, style-based generator architecture, synthetic image generation, design, procedural generation",2019,6186,image analysis|pattern recognition|computer science|natural language processing|natural language generation|computer graphic|generative adversarial network|machine learning|generative ai|style transfer|architecture|deep learning|image representation|style-based generator architecture|synthetic image generation|design|procedural generation,
https://openalex.org/W4288089799,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

Transfer learning, where a model is first pre-trained on data-rich task before being fine-tuned downstream task, has emerged as powerful technique in natural language processing (NLP). The effectiveness of transfer learning given rise to diversity approaches, methodology, and practice. In this paper, we explore the landscape techniques for NLP by introducing unified framework that converts all text-based problems into text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, other factors dozens understanding tasks. By combining insights from our exploration with scale new ``Colossal Clean Crawled Corpus'', achieve state-of-the-art results many benchmarks covering summarization, question answering, text classification, more. To facilitate future work NLP, release set, models, code.

machine learning, computer science, unified text-to-text transformer, natural language processing, text processing, transfer learning",2019,5988,machine learning|computer science|unified text-to-text transformer|natural language processing|text processing|transfer learning,https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W3035390927|https://openalex.org/W3099700870|https://openalex.org/W3105966348|https://openalex.org/W3185341429|https://openalex.org/W3098605233
https://openalex.org/W2251939518,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,"Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank

Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality tasks such as sentiment detection requires richer supervised training and evaluation resources more powerful models composition. To remedy this, we introduce Sentiment Treebank. It includes fine grained labels for 215,154 parse trees 11,855 sentences presents new challenges compositionality. address them, Recursive Neural Tensor Network. When trained on treebank, this model outperforms all previous methods several metrics. pushes state art single sentence positive/negative classification from 80% up to 85.4%. The accuracy predicting fine-grained reaches 80.7%, an improvement 9.7% over bag features baselines. Lastly, it is only that can accurately capture effects negation its scope at various tree levels both positive negative phrases.

computer science, text mining, language model, natural language processing, treebanks, machine learning, semantic evaluation, vector space model, cognitive science, computational linguistics, data science, knowledge discovery, recursive deep models, semantic compositionality, machine learning research, sentiment treebank, semantic parsing",2013,5832,computer science|text mining|language model|natural language processing|treebanks|machine learning|semantic evaluation|vector space model|cognitive science|computational linguistics|data science|knowledge discovery|recursive deep models|semantic compositionality|machine learning research|sentiment treebank|semantic parsing,https://openalex.org/W2896457183|https://openalex.org/W1832693441|https://openalex.org/W2962739339|https://openalex.org/W2123442489|https://openalex.org/W2970641574|https://openalex.org/W3034999214|https://openalex.org/W2949547296|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W1840435438|https://openalex.org/W2120615054|https://openalex.org/W2131744502|https://openalex.org/W2923014074|https://openalex.org/W2884001105|https://openalex.org/W2562607067|https://openalex.org/W2265846598|https://openalex.org/W2949541494|https://openalex.org/W2787560479|https://openalex.org/W2914120296|https://openalex.org/W3011411500|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W2794557536|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2250879510|https://openalex.org/W2306941105|https://openalex.org/W3105966348|https://openalex.org/W2953356739|https://openalex.org/W2252215182|https://openalex.org/W2964010806|https://openalex.org/W2891177506|https://openalex.org/W2963042536
https://openalex.org/W2128160875,Dynamic programming algorithm optimization for spoken word recognition,"Dynamic programming algorithm optimization for spoken word recognition

This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These forms compared with each other through theoretical discussions experimental studies. The form superiority established. A new technique, slope constraint, successfully introduced, in which warping function restricted so as to improve discrimination between words different categories. effective constraint characteristic qualitatively analyzed, condition determined experiments. optimized then extensively subjected comparison various DP-algorithms, previously applied recognition by research groups. experiment shows that present gives no more than about two-thirds errors, even best conventional algorithm.

computer science, speech recognition, dynamic programming, natural language processing, speech processing, spoken word recognition, spoken language processing, language recognition, speech technology, spoken language technology, computational optimization",1978,5680,computer science|speech recognition|dynamic programming|natural language processing|speech processing|spoken word recognition|spoken language processing|language recognition|speech technology|spoken language technology|computational optimization,https://openalex.org/W2148154194
https://openalex.org/W2970641574,Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

Nils Reimers, Iryna Gurevych. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint (EMNLP-IJCNLP). 2019.

link prediction, word embeddings, nlp task, natural language processing, siamese bert-networks, sentence embeddings, computer science, linguistics, language engineering, deep learning, computational linguistics",2019,5492,link prediction|word embeddings|nlp task|natural language processing|siamese bert-networks|sentence embeddings|computer science|linguistics|language engineering|deep learning|computational linguistics,
https://openalex.org/W2963748441,"SQuAD: 100,000+ Questions for Machine Comprehension of Text","SQuAD: 100,000+ Questions for Machine Comprehension of Text

We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on set Wikipedia articles, where answer to each question is segment text from corresponding passage. analyze understand types reasoning required questions, leaning heavily dependency and constituency trees. build strong logistic regression model, which achieves an F1 score 51.0%, significant improvement over simple baseline (20%). However, human performance (86.8%) much higher, indicating that presents good challenge problem for future research. The freely available at https://stanford-qa.com

automated reasoning, explainable ai, information retrieval, machine comprehension, knowledge discovery, nlp task, natural language processing, question answering, computer science, language model, machine learning, large language model, artificial intelligence, data science",2016,5338,automated reasoning|explainable ai|information retrieval|machine comprehension|knowledge discovery|nlp task|natural language processing|question answering|computer science|language model|machine learning|large language model|artificial intelligence|data science,https://openalex.org/W2896457183|https://openalex.org/W2962739339|https://openalex.org/W3034999214|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W2923014074|https://openalex.org/W3099700870|https://openalex.org/W2912924812|https://openalex.org/W3011411500|https://openalex.org/W2970476646|https://openalex.org/W2963969878|https://openalex.org/W2963339397|https://openalex.org/W2889787757|https://openalex.org/W3105966348|https://openalex.org/W2953356739|https://openalex.org/W3185341429|https://openalex.org/W2963691697|https://openalex.org/W3169483174
https://openalex.org/W2158899491,Natural Language Processing (almost) from Scratch,"Natural Language Processing (almost) from Scratch

We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, semantic role labeling. This versatility is achieved by trying avoid task-specific engineering therefore disregarding lot of prior knowledge. Instead exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis vast amounts mostly unlabeled training data. work then used as building freely available tagging with good performance minimal computational requirements.

machine learning, computer science, natural language, nlp task, natural language processing, computational linguistics, language model",2011,5330,machine learning|computer science|natural language|nlp task|natural language processing|computational linguistics|language model,https://openalex.org/W2250539671|https://openalex.org/W1832693441|https://openalex.org/W2962739339|https://openalex.org/W2949547296|https://openalex.org/W2296283641|https://openalex.org/W2963012544|https://openalex.org/W2131744502|https://openalex.org/W2884001105|https://openalex.org/W1544827683|https://openalex.org/W2963918774|https://openalex.org/W2265846598|https://openalex.org/W2250861254|https://openalex.org/W2963625095|https://openalex.org/W2125031621|https://openalex.org/W2126725946|https://openalex.org/W1938755728|https://openalex.org/W2251803266|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2250879510|https://openalex.org/W2170738476|https://openalex.org/W2251135946|https://openalex.org/W2252215182|https://openalex.org/W2515462165|https://openalex.org/W2963042536
https://openalex.org/W2525778437,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation

Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of weaknesses conventional phrase-based translation systems. Unfortunately, NMT systems are known be computationally expensive both in training and inference. Also, most have difficulty rare words. These issues hindered NMT's use practical deployments services, where accuracy speed essential. In this work, we present GNMT, Google's system, which attempts address these issues. Our model consists a deep LSTM network 8 encoder decoder layers using attention residual connections. To improve parallelism therefore decrease time, our mechanism connects bottom layer top encoder. accelerate final speed, employ low-precision arithmetic during inference computations. handling words, divide words into limited set common sub-word units (""wordpieces"") input output. This method provides good balance between flexibility ""character""-delimited models efficiency ""word""-delimited models, naturally handles ultimately improves overall system. beam search technique employs length-normalization procedure uses coverage penalty, encourages generation output sentence that likely cover all source sentence. On WMT'14 English-to-French English-to-German benchmarks, GNMT achieves competitive results state-of-the-art. Using human side-by-side evaluation on isolated simple sentences, it reduces errors by average 60% compared production

computer science, natural language processing, language, neural machine translation, computational linguistics, machine translation, translation studies, neural network (machine learning), language learning, computer-assisted translation",2016,5286,computer science|natural language processing|language|neural machine translation|computational linguistics|machine translation|translation studies|neural network (machine learning)|language learning|computer-assisted translation,https://openalex.org/W2896457183|https://openalex.org/W2911489562|https://openalex.org/W2963250244|https://openalex.org/W2884001105|https://openalex.org/W2970771982|https://openalex.org/W2613904329|https://openalex.org/W2936695845|https://openalex.org/W2550821151|https://openalex.org/W3034238904|https://openalex.org/W2970419734|https://openalex.org/W2612675303|https://openalex.org/W2742947407|https://openalex.org/W2966715458|https://openalex.org/W3019166713|https://openalex.org/W2963506925|https://openalex.org/W3098605233|https://openalex.org/W2968124245
https://openalex.org/W1514535095,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We how can train this a deterministic manner using standard backpropagation techniques stochastically maximizing variational lower bound. also show through visualization is able learn fix its gaze on salient objects while generating corresponding words output sequence. validate use with state-of-the-art performance three benchmark datasets: Flickr9k, Flickr30k MS COCO.

vision language model, computer science, machine learning research, deep learning, language generation, information fusion, natural language processing, machine translation, natural language generation, scene understanding, scene interpretation, visual perception, data science, computer vision, retrieval augmented generation, cognitive science, visual attention, image communication, image analysis",2015,5247,vision language model|computer science|machine learning research|deep learning|language generation|information fusion|natural language processing|machine translation|natural language generation|scene understanding|scene interpretation|visual perception|data science|computer vision|retrieval augmented generation|cognitive science|visual attention|image communication|image analysis,https://openalex.org/W1902237438|https://openalex.org/W2745461083|https://openalex.org/W2606974598|https://openalex.org/W2884001105|https://openalex.org/W2327501763|https://openalex.org/W2963084599|https://openalex.org/W2550553598|https://openalex.org/W2517194566|https://openalex.org/W2405756170|https://openalex.org/W2963966654|https://openalex.org/W2250966211|https://openalex.org/W2963339397|https://openalex.org/W2425121537|https://openalex.org/W2886641317|https://openalex.org/W2949999304|https://openalex.org/W2963758027|https://openalex.org/W2515462165|https://openalex.org/W2962826786
https://openalex.org/W2970597249,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"XLNet: Generalized Autoregressive Pretraining for Language Understanding

With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than approaches on autoregressive language modeling. However, relying corrupting input with masks, neglects dependency between masked positions and suffers from a pretrain-finetune discrepancy. In light these pros cons, we propose XLNet, generalized method that (1) enables learning contexts by maximizing expected likelihood over all permutations factorization order (2) overcomes limitations thanks to its formulation. Furthermore, XLNet integrates ideas Transformer-XL, state-of-the-art model, into pretraining. Empirically, under comparable experiment settings, outperforms 20 tasks, often large margin, including question answering, natural inference, sentiment analysis, document ranking.

knowledge discovery, concept drift, principal component analysis, computer science, language model, machine learning research, deep learning, language understanding, natural language processing, language, bootstrap resampling, language learning, multilingual pretraining, machine learning, linguistics, computational linguistics, autoregressive pretraining, general linguistics, computational intelligence, large language model",2019,5125,knowledge discovery|concept drift|principal component analysis|computer science|language model|machine learning research|deep learning|language understanding|natural language processing|language|bootstrap resampling|language learning|multilingual pretraining|machine learning|linguistics|computational linguistics|autoregressive pretraining|general linguistics|computational intelligence|large language model,https://openalex.org/W2970641574|https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W2980282514|https://openalex.org/W3170841864|https://openalex.org/W3030163527|https://openalex.org/W3034238904|https://openalex.org/W3098824823|https://openalex.org/W3105966348|https://openalex.org/W3098605233
https://openalex.org/W179875071,Recurrent neural network based language model,"Recurrent neural network based language model

A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it possible obtain around 50% reduction of perplexity by using mixture several RNN LMs, compared a state the art backoff model. Speech experiments show 18% word error rate on Wall Street Journal task when comparing models trained same amount data, and 5% much harder NIST RT05 task, even more data than LM. We provide ample empirical evidence suggest connectionist are superior standard n-gram techniques, except their high computational (training) complexity. Index Terms: modeling, networks,

computer science, linguistics, large language model, language model, natural language processing, language, neural machine translation, machine learning, recurrent neural network, sequential learning, computational intelligence, deep learning, machine learning research, machine translation, neural network (machine learning), language learning",2010,5099,computer science|linguistics|large language model|language model|natural language processing|language|neural machine translation|machine learning|recurrent neural network|sequential learning|computational intelligence|deep learning|machine learning research|machine translation|neural network (machine learning)|language learning,https://openalex.org/W2130942839|https://openalex.org/W2950577311|https://openalex.org/W2949888546|https://openalex.org/W2141599568|https://openalex.org/W2964110616|https://openalex.org/W2884001105|https://openalex.org/W2562607067|https://openalex.org/W2402268235|https://openalex.org/W2517194566|https://openalex.org/W2962883855|https://openalex.org/W2171928131|https://openalex.org/W1591706642|https://openalex.org/W1938755728|https://openalex.org/W2783272285|https://openalex.org/W2742947407|https://openalex.org/W196214544|https://openalex.org/W3019166713|https://openalex.org/W2963758027|https://openalex.org/W2170738476|https://openalex.org/W2963042536|https://openalex.org/W2963963856|https://openalex.org/W2962706528|https://openalex.org/W2963970792
https://openalex.org/W2117130368,A unified architecture for natural language processing,"A unified architecture for natural language processing

We describe a single convolutional neural network architecture that, given sentence, outputs host of language processing predictions: part-of-speech tags, chunks, named entity semantic roles, semantically similar words and the likelihood that sentence makes sense (grammatically semantically) using model. The entire is trained jointly on all these tasks weight-sharing, an instance multitask learning. All use labeled data except model which learnt from unlabeled text represents novel form semi-supervised learning for shared tasks. show how both improve generalization tasks, resulting in state-of-the-art-performance.

computer science, artificial intelligence, natural language interface, language model, natural language processing, language engineering, unified architecture, computational linguistics, deep learning, nlp task, machine translation, knowledge architecture",2008,5021,computer science|artificial intelligence|natural language interface|language model|natural language processing|language engineering|unified architecture|computational linguistics|deep learning|nlp task|machine translation|knowledge architecture,https://openalex.org/W2250539671|https://openalex.org/W2896457183|https://openalex.org/W2153579005|https://openalex.org/W2950577311|https://openalex.org/W2493916176|https://openalex.org/W2950133940|https://openalex.org/W2251939518|https://openalex.org/W2949547296|https://openalex.org/W2113459411|https://openalex.org/W2147768505|https://openalex.org/W2120615054|https://openalex.org/W2131744502|https://openalex.org/W2141599568|https://openalex.org/W2884001105|https://openalex.org/W2158139315|https://openalex.org/W2963918774|https://openalex.org/W2963625095|https://openalex.org/W22861983|https://openalex.org/W2125031621|https://openalex.org/W658020064|https://openalex.org/W2126725946|https://openalex.org/W2251803266|https://openalex.org/W2742947407|https://openalex.org/W1423339008|https://openalex.org/W71795751|https://openalex.org/W3019166713|https://openalex.org/W2953356739|https://openalex.org/W2963042536
https://openalex.org/W2964199361,On the Properties of Neural Machine Translation: Encoder–Decoder Approaches,"On the Properties of Neural Machine Translation: Encoder–Decoder Approaches

Neural machine translation is a relatively new approach to statistical based purely on neural networks.The models often consist of an encoder and decoder.The extracts fixed-length representation from variable-length input sentence, the decoder generates correct this representation.In paper, we focus analyzing properties using two models; RNN Encoder-Decoder newly proposed gated recursive convolutional network.We show that performs well short sentences without unknown words, but its performance degrades rapidly as length sentence number words increase.Furthermore, find network learns grammatical structure automatically.

language learning, neural computation, natural language processing, machine translation, iterative decoding, autoencoders, computational intelligence, computer science, large language model, language model, machine learning, neural machine translation, linguistics, language, deep learning, recurrent neural network, machine learning research",2014,5017,language learning|neural computation|natural language processing|machine translation|iterative decoding|autoencoders|computational intelligence|computer science|large language model|language model|machine learning|neural machine translation|linguistics|language|deep learning|recurrent neural network|machine learning research,https://openalex.org/W2964308564|https://openalex.org/W2962739339|https://openalex.org/W2963918774|https://openalex.org/W2963625095|https://openalex.org/W2139501017|https://openalex.org/W3019166713|https://openalex.org/W2963506925|https://openalex.org/W2963042536
https://openalex.org/W2979826702,Transformers: State-of-the-Art Natural Language Processing,"Transformers: State-of-the-Art Natural Language Processing

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.

natural language, nlp task, natural language processing, computer science, language model, machine learning, artificial intelligence, computational linguistics",2020,5013,natural language|nlp task|natural language processing|computer science|language model|machine learning|artificial intelligence|computational linguistics,
https://openalex.org/W3034999214,"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. Proceedings of the 58th Annual Meeting Association for Computational Linguistics. 2020.

computer science, language model, natural language processing, language, natural language generation, language engineering, neural machine translation, machine learning, denoising sequence-to-sequence pre-training, computational linguistics, deep learning, machine translation, image denoising, language generation",2020,4980,computer science|language model|natural language processing|language|natural language generation|language engineering|neural machine translation|machine learning|denoising sequence-to-sequence pre-training|computational linguistics|deep learning|machine translation|image denoising|language generation,https://openalex.org/W3030163527|https://openalex.org/W3099700870|https://openalex.org/W3185341429|https://openalex.org/W3098605233|https://openalex.org/W3169483174
https://openalex.org/W2949547296,Distributed Representations of Sentences and Documents,"Distributed Representations of Sentences and Documents

Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes texts, one of most common features is bag-of-words. Despite their popularity, bag-of-words have two major weaknesses: they lose ordering words and also ignore semantics words. For example, ""powerful,"" ""strong"" ""Paris"" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns representations from variable-length pieces such sentences, paragraphs, documents. Our represents each document by dense vector which trained predict in document. Its construction gives our potential overcome weaknesses models. Empirical results show Vectors outperform models well other techniques for text representations. Finally, achieve new state-of-the-art on several classification sentiment analysis tasks.

word embeddings, nlp task, computer science, language model, text mining, information retrieval, information fusion, natural language processing, natural language generation, image representation, language, distributional semantics, distributed system, language resource, linguistics, distributed representations, computational linguistics, distributed learning, content representation",2014,4970,word embeddings|nlp task|computer science|language model|text mining|information retrieval|information fusion|natural language processing|natural language generation|image representation|language|distributional semantics|distributed system|language resource|linguistics|distributed representations|computational linguistics|distributed learning|content representation,https://openalex.org/W2996428491|https://openalex.org/W2131744502|https://openalex.org/W2884001105|https://openalex.org/W2963918774|https://openalex.org/W2963223306|https://openalex.org/W2964236337|https://openalex.org/W2742947407|https://openalex.org/W2740168486
https://openalex.org/W1524333225,The Kaldi Speech Recognition Toolkit,"The Kaldi Speech Recognition Toolkit

We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides system based on finite-state automata (using freely available OpenFst), together with detailed documentation and comprehensive set scripts building complete systems. is written C++, core library supports modeling arbitrary phonetic-context sizes, acoustic subspace Gaussian mixture models (SGMM) as well standard models, all commonly used linear affine transforms. released under Apache License v2.0, which highly nonrestrictive, making it suitable wide community users.

computer science, speech recognition, voice recognition, language model, natural language processing, speech processing, speech analysis, spoken language processing, communication, assistive technology, computational linguistics, speech communication, speech technology, spoken language technology, robust speech recognition, computer engineering",2011,4866,computer science|speech recognition|voice recognition|language model|natural language processing|speech processing|speech analysis|spoken language processing|communication|assistive technology|computational linguistics|speech communication|speech technology|spoken language technology|robust speech recognition|computer engineering,https://openalex.org/W2327501763|https://openalex.org/W2890964092|https://openalex.org/W2102113734|https://openalex.org/W1922655562
https://openalex.org/W2963373786,Improved Techniques for Training GANs,"Improved Techniques for Training GANs

We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. focus on two applications GANs: semi-supervised learning, generation images humans find visually realistic. Unlike most work models, our primary goal is not train model assigns high likelihood test data, nor do require be able learn well without using any labels. Using techniques, achieve state-of-the-art results in classification MNIST, CIFAR-10 SVHN. The generated are quality as confirmed by visual Turing test: generates MNIST samples cannot distinguish from real yield human error rate 21.3%. also ImageNet with unprecedented resolution show methods enable recognizable classes.

generative ai, neural computation, natural language processing, recommender system, aural augmentation, neural network (machine learning), adversarial machine learning, training gans, computer science, machine learning, generative adversarial network, deep learning",2016,4803,generative ai|neural computation|natural language processing|recommender system|aural augmentation|neural network (machine learning)|adversarial machine learning|training gans|computer science|machine learning|generative adversarial network|deep learning,https://openalex.org/W2962770929|https://openalex.org/W2963966654
https://openalex.org/W2174706414,Probabilistic topic models,"Probabilistic topic models

Surveying a suite of algorithms that offer solution to managing large document archives.

probabilistic topic models, disinformation detection, topic model, text mining, natural language processing, probabilistic system, machine learning, probability theory, statistics, machine learning research, probabilistic reasoning",2012,4779,probabilistic topic models|disinformation detection|topic model|text mining|natural language processing|probabilistic system|machine learning|probability theory|statistics|machine learning research|probabilistic reasoning,https://openalex.org/W2095655043
https://openalex.org/W2135621733,HMMER web server: interactive sequence similarity searching,"HMMER web server: interactive sequence similarity searching

HMMER is a software suite for protein sequence similarity searches using probabilistic methods. Previously, has mainly been available only as computationally intensive UNIX command-line tool, restricting its use. Recent advances in the software, HMMER3, have resulted 100-fold speed gain relative to previous versions. It now feasible make efficient profile hidden Markov model (profile HMM) via web. A web server ( http://hmmer.janelia.org ) designed and implemented such that most database return within few seconds. Methods are searching either single sequence, multiple alignment or HMM against target database, Pfam. The cater range of different user expertise accepts batch uploading queries at once. All search methods also RESTful services, thereby allowing them be readily integrated remotely executed tasks locally scripted workflows. We focused on minimizing times ability rapidly display tabular results, regardless number matches found, developing graphical summaries results provide quick, intuitive appraisement them.

pattern recognition, computer science, interactive sequence similarity, recommender system, similarity search, data science, content similarity detection, data mining, computer engineering, hmmer web server, information retrieval, interactive search, machine learning research, natural language processing, sequence analysis, web search, clustering, knowledge discovery, semantic web, interactive information retrieval",2011,4776,pattern recognition|computer science|interactive sequence similarity|recommender system|similarity search|data science|content similarity detection|data mining|computer engineering|hmmer web server|information retrieval|interactive search|machine learning research|natural language processing|sequence analysis|web search|clustering|knowledge discovery|semantic web|interactive information retrieval,
https://openalex.org/W2140910804,The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods,"The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods

We are in the midst of a technological revolution whereby, for first time, researchers can link daily word use to broad array real-world behaviors. This article reviews several computerized text analysis methods and describes how Linguistic Inquiry Word Count (LIWC) was created validated. LIWC is transparent program that counts words psychologically meaningful categories. Empirical results using demonstrate its ability detect meaning wide variety experimental settings, including show attentional focus, emotionality, social relationships, thinking styles, individual differences.

linguistics, psycholinguistics, natural language processing, language, semantics, psychology, psychological meaning",2009,4764,linguistics|psycholinguistics|natural language processing|language|semantics|psychology|psychological meaning,https://openalex.org/W1663984431
https://openalex.org/W4242761397,How To Do Things With Words,"How To Do Things With Words

Abstract This work sets out the author's conclusions in field to which he directed his main efforts for at least last ten years of life. Starting from an examination already well-known distinction between performative utterances and statements, finally abandons that distinction, replacing it with a more general theory ‘illocutionary forces’ utterances, has important bearings on wide variety philosophical problems.

applied linguistics, language grounding, linguistics, natural language processing, speech processing, language, general linguistics, language-based approach, syntax, communication, lexical semantics, language acquisition, semantics, semantic processing, lexicon, narrative, language learning, context (linguistics)",1975,4754,applied linguistics|language grounding|linguistics|natural language processing|speech processing|language|general linguistics|language-based approach|syntax|communication|lexical semantics|language acquisition|semantics|semantic processing|lexicon|narrative|language learning|context (linguistics),
https://openalex.org/W2148154194,Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,"Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences

Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech system. The vocabulary included many phonetically similar monosyllabic words, therefore emphasis was on ability retain significant information face syntactic and duration variations. For each parameter set (based mel-frequency cepstrum, linear frequency prediction spectrum, or reflection coefficients), templates generated using an efficient dynamic warping method, test data time registered templates. A ten cepstrum coefficients computed every 6.4 ms resulted best performance, namely 96.5 percent 95.0 two speakers. superior may be attributed fact that they better represent perceptually relevant aspects short-term spectrum.

linguistics, speech recognition, parametric representations, natural language processing, language model, speech processing, spoken language processing, spoken sentences, language recognition, monosyllabic word recognition, spoken language technology",1980,4752,linguistics|speech recognition|parametric representations|natural language processing|language model|speech processing|spoken language processing|spoken sentences|language recognition|monosyllabic word recognition|spoken language technology,https://openalex.org/W2165880886|https://openalex.org/W2102113734
https://openalex.org/W2950178297,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We how can train this a deterministic manner using standard backpropagation techniques stochastically maximizing variational lower bound. also show through visualization is able learn fix its gaze on salient objects while generating corresponding words output sequence. validate use with state-of-the-art performance three benchmark datasets: Flickr8k, Flickr30k MS COCO.

image communication, computer science, information fusion, scene interpretation, cognitive science, visual perception, vision language model, data science, deep learning, computer vision, language generation, machine learning research, scene understanding, natural language processing, retrieval augmented generation, image analysis, visual attention, natural language generation, machine translation",2015,4667,image communication|computer science|information fusion|scene interpretation|cognitive science|visual perception|vision language model|data science|deep learning|computer vision|language generation|machine learning research|scene understanding|natural language processing|retrieval augmented generation|image analysis|visual attention|natural language generation|machine translation,https://openalex.org/W1902237438|https://openalex.org/W2745461083|https://openalex.org/W2277195237|https://openalex.org/W2606974598|https://openalex.org/W2884001105|https://openalex.org/W2327501763|https://openalex.org/W2963084599|https://openalex.org/W2963206148|https://openalex.org/W2550553598|https://openalex.org/W2963966654|https://openalex.org/W2963339397|https://openalex.org/W2425121537|https://openalex.org/W2742947407|https://openalex.org/W2886641317|https://openalex.org/W2963758027|https://openalex.org/W2515462165|https://openalex.org/W2962826786
https://openalex.org/W2039107287,Toward a model of text comprehension and production.,"Toward a model of text comprehension and production.

The semantic structure of texts can be described both at the local microlevel and a more global macrolevel. A model for text comprehension based on this notion accounts formation coherent base in terms cyclical process constrained by limitations working memory. Furthermore, includes macro-operators, whose purpose is to reduce information its gist, that is, theoretical macrostructure. These operations are under control schema, which formulation comprehender's goals. macroprocesses predictable only when schema made explicit. On production side, concerned with generation recall summarization protocols. This partly reproductive constructive, involving inverse operation macro-operators. applied paragraph from psychological research report, methods empirical testing developed.

program comprehension, psycholinguistics, text comprehension, nlp task, natural language processing, speech production, text processing, cognitive science, language comprehension, computer science, language model, language, reading research",1978,4562,program comprehension|psycholinguistics|text comprehension|nlp task|natural language processing|speech production|text processing|cognitive science|language comprehension|computer science|language model|language|reading research,https://openalex.org/W1990100773|https://openalex.org/W1990689692
https://openalex.org/W4251501134,Ways with Words,"Ways with Words

Ways with Words, first published in 1983, is a classic study of children learning to use language at home and school two communities only few miles apart the south-eastern United States. 'Roadville' white working-class community families steeped for generations life textile mills; 'Trackton' an African-American whose older grew up farming land, but existent members work mills. In tracing children's development author shows deep cultural differences between communities, ways words differ as strikingly from each other either does pattern townspeople, 'mainstream' blacks whites who hold power schools workplaces region. Employing combined skills ethnographer, social historian, teacher, raises fundamental questions about nature development, effects literacy on oral habits, sources communication problems workplaces.

applied linguistics, morphology (linguistics), linguistics, keyword extraction, natural language processing, language, general linguistics, language-based approach, syntax, communication, lexical semantics, sociolinguistics, semantics, social semiotics, linguistic theory, narrative, language learning, context (linguistics)",1983,4498,applied linguistics|morphology (linguistics)|linguistics|keyword extraction|natural language processing|language|general linguistics|language-based approach|syntax|communication|lexical semantics|sociolinguistics|semantics|social semiotics|linguistic theory|narrative|language learning|context (linguistics),
https://openalex.org/W2911489562,BioBERT: a pre-trained biomedical language representation model for biomedical text mining,"BioBERT: a pre-trained biomedical language representation model for biomedical text mining

Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With progress in natural language processing (NLP), extracting valuable information from literature has gained popularity among researchers, and deep learning boosted development effective models. However, directly applying advancements NLP to often yields unsatisfactory results due a word distribution shift general domain corpora corpora. In this article, we investigate how recently introduced pre-trained model BERT can be adapted for We introduce BioBERT (Bidirectional Encoder Representations Transformers Text Mining), which domain-specific representation on large-scale almost same architecture across tasks, largely outperforms previous state-of-the-art models variety tasks when While obtains performance comparable that models, significantly them following three representative tasks: named entity recognition (0.62% F1 score improvement), relation extraction (2.80% improvement) question answering (12.24% MRR improvement). Our analysis show pre-training helps it understand complex texts. make weights freely available at https://github.com/naver/biobert-pretrained, source code fine-tuning https://github.com/dmis-lab/biobert.

computer science, biomedical ontology, text mining, language model, natural language processing, biomedical text mining, machine learning, biomedical analysis, biomedical engineering, biomedical computing, data science, biomedical data analysis, machine learning research, biomedical data science, biomedical informatics, bioinformatics",2019,4290,computer science|biomedical ontology|text mining|language model|natural language processing|biomedical text mining|machine learning|biomedical analysis|biomedical engineering|biomedical computing|data science|biomedical data analysis|machine learning research|biomedical data science|biomedical informatics|bioinformatics,https://openalex.org/W2970771982|https://openalex.org/W3034238904
https://openalex.org/W2102381086,Introduction to WordNet: An On-line Lexical Database<sup>*</sup>,"Introduction to WordNet: An On-line Lexical Database<sup>*</sup>

WordNet is an on-line lexical reference system whose design inspired by current psycholinguistic theories of human memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying concept. Different relations link the sets.

applied linguistics, language resource, computer science, linguistics, text mining, language model, natural language processing, language, lexical semantics, computational lexicology, lexical resource, computational linguistics, knowledge discovery, lexicography, lexicon, semantic web, corpus linguistics",1990,4193,applied linguistics|language resource|computer science|linguistics|text mining|language model|natural language processing|language|lexical semantics|computational lexicology|lexical resource|computational linguistics|knowledge discovery|lexicography|lexicon|semantic web|corpus linguistics,https://openalex.org/W2108646579|https://openalex.org/W2068737686|https://openalex.org/W2101210369|https://openalex.org/W1659833910|https://openalex.org/W2436001372|https://openalex.org/W2166776180|https://openalex.org/W2117400858|https://openalex.org/W2199803028|https://openalex.org/W1828401780|https://openalex.org/W2016534914|https://openalex.org/W2164455818
https://openalex.org/W2133564696,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural Machine Translation by Jointly Learning to Align and Translate

Neural machine translation is a recently proposed approach to translation. Unlike the traditional statistical translation, neural aims at building single network that can be jointly tuned maximize performance. The models for often belong family of encoder-decoders and consists an encoder encodes source sentence into fixed-length vector from which decoder generates In this paper, we conjecture use bottleneck in improving performance basic encoder-decoder architecture, propose extend by allowing model automatically (soft-)search parts are relevant predicting target word, without having form these as hard segment explicitly. With new approach, achieve comparable existing state-of-the-art phrase-based system on task English-to-French Furthermore, qualitative analysis reveals (soft-)alignments found agree well with our intuition.

linguistics, computer science, machine learning, computational intelligence, transfer learning, computer-assisted translation, machine translation, language, machine learning research, natural language processing, neural machine translation, deep learning, neural network (machine learning), neural computation",2014,4033,linguistics|computer science|machine learning|computational intelligence|transfer learning|computer-assisted translation|machine translation|language|machine learning research|natural language processing|neural machine translation|deep learning|neural network (machine learning)|neural computation,https://openalex.org/W1902237438|https://openalex.org/W2950178297|https://openalex.org/W1591706642
https://openalex.org/W2745461083,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering

Top-down visual attention mechanisms have been used extensively in image captioning and question answering (VQA) to enable deeper understanding through fine-grained analysis even multiple steps of reasoning. In this work, we propose a combined bottom-up top-down mechanism that enables be calculated at the level objects other salient regions. This is natural basis for considered. Within our approach, (based on Faster R-CNN) proposes regions, each with an associated feature vector, while determines weightings. Applying approach captioning, results MSCOCO test server establish new state-of-the-art task, achieving CIDEr / SPICE BLEU-4 scores 117.9, 21.5 36.9, respectively. Demonstrating broad applicability method, applying same VQA obtain first place 2017 Challenge.

image search, visual question, computer science, image captioning, machine learning, top-down attention, image analysis, visual reasoning, information visualization, cognitive science, data science, image representation, image communication, scene understanding, deep learning, machine vision, visual question answering, natural language processing, computer vision, multimedia retrieval, question answering",2018,3974,image search|visual question|computer science|image captioning|machine learning|top-down attention|image analysis|visual reasoning|information visualization|cognitive science|data science|image representation|image communication|scene understanding|deep learning|machine vision|visual question answering|natural language processing|computer vision|multimedia retrieval|question answering,https://openalex.org/W2966715458|https://openalex.org/W2968124245
https://openalex.org/W2296283641,Neural Architectures for Named Entity Recognition,"Neural Architectures for Named Entity Recognition

Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer. Proceedings of the 2016 Conference North American Chapter Association for Computational Linguistics: Human Language Technologies. 2016.

pattern recognition, knowledge discovery, automatic classification, neural computation, neuroscience, natural language processing, neural architecture search, nlp task, cognitive science, computational intelligence, computer science, recurrent neural network, machine learning, machine learning research, deep learning, entity recognition, named-entity recognition, neural architectures",2016,3972,pattern recognition|knowledge discovery|automatic classification|neural computation|neuroscience|natural language processing|neural architecture search|nlp task|cognitive science|computational intelligence|computer science|recurrent neural network|machine learning|machine learning research|deep learning|entity recognition|named-entity recognition|neural architectures,https://openalex.org/W2962739339|https://openalex.org/W3035390927|https://openalex.org/W2884001105|https://openalex.org/W2562607067|https://openalex.org/W2787560479|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2880875857
https://openalex.org/W2277195237,Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations

Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive description and question answering. Cognition is core to that involve not just recognizing, but reasoning about our visual world. However, models used tackle the rich content images for are being trained using same datasets designed tasks. To achieve success at tasks, need understand interactions relationships between objects an image. When asked ""What vehicle person riding?"", will identify well riding(man, carriage) pulling(horse, answer correctly ""the riding a horse-drawn carriage."" In this paper, we present Visual Genome dataset enable modeling of relationships. We collect dense annotations objects, attributes, within each learn these models. Specifically, contains over 108K where has average $$35$$ $$26$$ $$21$$ pairwise objects. canonicalize relationships, noun phrases region descriptions questions pairs WordNet synsets. Together, represent densest largest descriptions, pairs.

image analysis, natural language processing, language, vision language model, computer vision, visualization, visual perception, image representation, visual genome, annotation tool, connecting language, dense image annotations",2017,3943,image analysis|natural language processing|language|vision language model|computer vision|visualization|visual perception|image representation|visual genome|annotation tool|connecting language|dense image annotations,https://openalex.org/W2745461083|https://openalex.org/W2550553598|https://openalex.org/W2561715562|https://openalex.org/W2963758027|https://openalex.org/W2968124245
https://openalex.org/W4301357669,Using Language,"Using Language

This book, first published in 1996, argues that language use is more than the sum of a speaker speaking and listener listening. It joint action emerges when speakers listeners - writers readers perform their individual actions coordination, as ensembles. The author strongly embodies both social processes.

applied linguistics, linguistics, natural language processing, multilingualism, language, general linguistics, language technology, communication, interactional linguistics, computational linguistics, semantics, language recognition, language teaching, linguistic theory, spoken language technology, language science, language learning, context (linguistics)",1996,3927,applied linguistics|linguistics|natural language processing|multilingualism|language|general linguistics|language technology|communication|interactional linguistics|computational linguistics|semantics|language recognition|language teaching|linguistic theory|spoken language technology|language science|language learning|context (linguistics),
https://openalex.org/W2130041324,Studies in the Way of Words,"Studies in the Way of Words

This volume, Grice's first hook, includes the long-delayed publication of his enormously influential 1967 William James Lectures. But there is much, much more in this work. Paul Grice himself has carefully arranged and framed sequence essays to emphasize not a certain set ideas but habit mind, style philosophizing. has, be sure, provided philosophy with crucial ideas. His account speaker-meaning standard that others use define their own minor divergences or future elaborations. discussion conversational implicatures given philosophers an important tool for investigation all sorts problems; it also laid foundation great deal work by other linguists about presupposition. metaphysical defense absolute values starting considered beginning new phase philosophy. vital book who are interested Anglo-American

textual practice, linguistics, psycholinguistics, natural language processing, pragmatic, language-based approach, literary criticism, communication, reading research, literary theory, literature, literary study, narrative, intellectual discourse, corpus linguistics, experimental pragmatic",1989,3806,textual practice|linguistics|psycholinguistics|natural language processing|pragmatic|language-based approach|literary criticism|communication|reading research|literary theory|literature|literary study|narrative|intellectual discourse|corpus linguistics|experimental pragmatic,
https://openalex.org/W1579838312,"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition","Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition

From the Publisher:
This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples demonstrate main idea of Covers fundamental various fields, whether originally proposed for spoken written how same algorithm can be used speech recognition word-sense disambiguation. Emphasis web practical applications. scientific evaluation. Useful as a reference professionals any areas processing.

computer science, linguistics, speech recognition, natural language processing, speech processing, language, speech analysis, speech perception, language processing, spoken language processing, computational linguistics, language recognition, language processing in the brain, speech communication, speech technology, spoken language technology, language science",2000,3782,computer science|linguistics|speech recognition|natural language processing|speech processing|language|speech analysis|speech perception|language processing|spoken language processing|computational linguistics|language recognition|language processing in the brain|speech communication|speech technology|spoken language technology|language science,https://openalex.org/W2095655043|https://openalex.org/W2436001372
https://openalex.org/W4299828299,Proceedings of the 29th International Conference on Machine Learning (ICML-12),"Proceedings of the 29th International Conference on Machine Learning (ICML-12)

This is an index to the papers that appear in Proceedings of 29th International Conference on Machine Learning (ICML-12). The conference was held Edinburgh, Scotland, June 27th - July 3rd, 2012.

pattern recognition, computer science, computational learning theory, supervised learning, natural language processing, unsupervised machine learning, machine learning, embedded machine learning, data science, reinforcement learning, deep learning, computational intelligence, knowledge discovery, machine learning research, automated machine learning",2012,3776,pattern recognition|computer science|computational learning theory|supervised learning|natural language processing|unsupervised machine learning|machine learning|embedded machine learning|data science|reinforcement learning|deep learning|computational intelligence|knowledge discovery|machine learning research|automated machine learning,https://openalex.org/W3185341429
https://openalex.org/W3146306708,Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews,"Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews

This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not down). The classification of review is predicted by the average semantic orientation phrases in that contain adjectives adverbs. A phrase has positive when it good associations (e.g., “subtle nuances”) and negative bad “very cavalier”). In this paper, calculated mutual information between given word “excellent” minus “poor”. classified if its positive. achieves an accuracy 74% evaluated on 410 from Epinions, sampled four different domains (reviews automobiles, banks, movies, travel destinations). ranges 84% automobile to 66% movie reviews.

semantic orientation, knowledge discovery, automatic classification, principal component analysis, computer science, machine learning research, text mining, unsupervised classification, affective computing, information retrieval, machine vision, natural language processing, distributional semantics, multimodal sentiment analysis, semantic interpretation, machine learning, linguistics, computational linguistics, cognitive science, semantic evaluation",2002,3775,semantic orientation|knowledge discovery|automatic classification|principal component analysis|computer science|machine learning research|text mining|unsupervised classification|affective computing|information retrieval|machine vision|natural language processing|distributional semantics|multimodal sentiment analysis|semantic interpretation|machine learning|linguistics|computational linguistics|cognitive science|semantic evaluation,https://openalex.org/W2108646579|https://openalex.org/W2022204871|https://openalex.org/W2084046180|https://openalex.org/W2019759670|https://openalex.org/W2163302275|https://openalex.org/W2126581182|https://openalex.org/W2250966211|https://openalex.org/W1743243001|https://openalex.org/W1964613733|https://openalex.org/W2031998113|https://openalex.org/W71795751|https://openalex.org/W2250879510|https://openalex.org/W2148506018|https://openalex.org/W2159457224|https://openalex.org/W2088622183|https://openalex.org/W1987425720|https://openalex.org/W2108420397
https://openalex.org/W2996428491,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques lower consumption increase the speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead models scale much better compared original We also use a self-supervised loss focuses modeling inter-sentence coherence, show it consistently helps tasks with multi-sentence inputs. As result, best establishes new state-of-the-art GLUE, RACE, \squad benchmarks while having fewer parameters BERT-large. The code pretrained are available https://github.com/google-research/ALBERT.

self-supervised learning, language representations, natural language processing, lite bert, computer science, machine learning, deep learning",2019,3750,self-supervised learning|language representations|natural language processing|lite bert|computer science|machine learning|deep learning,https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W3105966348
https://openalex.org/W2949888546,Sequence to Sequence Learning with Neural Networks,"Sequence to Sequence Learning with Neural Networks

Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets available, they cannot be used to map sequences sequences. In this paper, we present a general end-to-end approach sequence makes minimal assumptions the structure. Our method uses multilayered Long Short-Term Memory (LSTM) input vector of fixed dimensionality, and then another deep LSTM decode target from vector. main result is an English French translation task WMT'14 dataset, translations produced by achieve BLEU score 34.8 entire test set, where LSTM's was penalized out-of-vocabulary words. Additionally, did not difficulty long sentences. For comparison, phrase-based SMT system achieves 33.3 same dataset. When rerank 1000 hypotheses aforementioned system, its increases 36.5, which close previous best task. The also learned sensible phrase sentence representations sensitive word order relatively invariant active passive voice. Finally, found reversing words in all source sentences (but sentences) improved markedly, because doing so introduced many short term dependencies between made optimization problem easier.

computer science, knowledge discovery, deep reinforcement learning, sparse neural network, cognitive science, neural computation, sequential learning, data science, deep learning, neural network (machine learning), machine learning, neural networks, machine learning research, natural language processing, machine vision, computational intelligence, natural language generation, sequence modelling, recurrent neural network",2014,3747,computer science|knowledge discovery|deep reinforcement learning|sparse neural network|cognitive science|neural computation|sequential learning|data science|deep learning|neural network (machine learning)|machine learning|neural networks|machine learning research|natural language processing|machine vision|computational intelligence|natural language generation|sequence modelling|recurrent neural network,https://openalex.org/W2133564696|https://openalex.org/W1544827683|https://openalex.org/W2613904329|https://openalex.org/W2962883855|https://openalex.org/W1922655562|https://openalex.org/W1938755728|https://openalex.org/W2172140247|https://openalex.org/W2612675303|https://openalex.org/W2951534261
https://openalex.org/W2159583324,The Unified Medical Language System (UMLS): integrating biomedical terminology,"The Unified Medical Language System (UMLS): integrating biomedical terminology

The Unified Medical Language System ( http://umlsks.nlm.nih.gov ) is a repository of biomedical vocabularies developed by the US National Library Medicine. UMLS integrates over 2 million names for some 900 000 concepts from more than 60 families vocabularies, as well 12 relations among these concepts. Vocabularies integrated in Metathesaurus include NCBI taxonomy, Gene Ontology, Subject Headings (MeSH), OMIM and Digital Anatomist Symbolic Knowledge Base. are not only inter‐related, but may also be linked to external resources such GenBank. In addition data, includes tools customizing (MetamorphoSys), generating lexical variants concept (lvg) extracting text (MetaMap). knowledge sources updated quarterly. All available at no fee research purposes within an institution, users required sign license agreement. distributed on CD‐ROM FTP.

biomedical terminology, health informatics, biomedical system, biomedical ontology, medical ontology, natural language processing, biomedical text mining, systems biology, health science, medicine, medical system, biomedical data integration, biomedical informatics, biomedical analysis",2004,3705,biomedical terminology|health informatics|biomedical system|biomedical ontology|medical ontology|natural language processing|biomedical text mining|systems biology|health science|medicine|medical system|biomedical data integration|biomedical informatics|biomedical analysis,
https://openalex.org/W3034552520,ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks,"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks

Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate developing more sophisticated modules for achieving better performance, which inevitably increase model complexity. To overcome paradox and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, only involves a handful parameters while bringing clear gain. By dissecting module SENet, we empirically show avoiding dimensionality reduction is important learning attention, appropriate cross-channel interaction can preserve significantly decreasing Therefore, propose local strategy without reduction, be efficiently implemented via 1D convolution. Furthermore, develop method adaptively select kernel size convolution, determining coverage interaction. The proposed ECA both efficient effective, e.g., computations our against backbone ResNet50 are 80 vs. 24.37M 4.7e-4 GFlops 3.86 GFlops, respectively, boost than 2% terms Top-1 accuracy. We extensively evaluate on image classification, object detection instance segmentation with backbones ResNets MobileNetV2. experimental results performing favorably its counterparts.

computer science, neuroimaging, machine learning, image analysis, information fusion, convolutional neural network, neuroscience, cognitive science, object detection, relevance feedback, computational imaging, efficient channel attention, scene understanding, sparse neural network, deep learning, machine vision, computational optimization, natural language processing, wireless communication",2020,3610,computer science|neuroimaging|machine learning|image analysis|information fusion|convolutional neural network|neuroscience|cognitive science|object detection|relevance feedback|computational imaging|efficient channel attention|scene understanding|sparse neural network|deep learning|machine vision|computational optimization|natural language processing|wireless communication,
https://openalex.org/W2022963108,DRC: A dual route cascaded model of visual word recognition and reading aloud.,"DRC: A dual route cascaded model of visual word recognition and reading aloud.

This article describes the Dual Route Cascaded (DRC) model, a computational model of visual word recognition and reading aloud. The DRC is realization dual-route theory reading, only that can perform 2 tasks most commonly used to study reading: lexical decision For both tasks, authors show wide variety variables influence human latencies model's in exactly same way. simulates number such effects other models do not, but there appear be no any current simulate cannot. conclude successful existing reading.

vision language model, iterative decoding, computer science, language model, deep learning, spoken language technology, pattern recognition, visual word recognition, information fusion, machine vision, natural language processing, speech recognition, reading aloud, reading research, language recognition, dual route, data science, cognitive science, language comprehension, reference frame",2001,3561,vision language model|iterative decoding|computer science|language model|deep learning|spoken language technology|pattern recognition|visual word recognition|information fusion|machine vision|natural language processing|speech recognition|reading aloud|reading research|language recognition|dual route|data science|cognitive science|language comprehension|reference frame,https://openalex.org/W2016534914
https://openalex.org/W2128017662,Scalable Recognition with a Vocabulary Tree,"Scalable Recognition with a Vocabulary Tree

A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality exhibited in live demonstration recognizes CD-covers from database 40000 images popular music CD’s. builds upon techniques indexing descriptors extracted local regions, robust background clutter occlusion. region are hierarchically quantized vocabulary tree. tree allows larger more discriminatory be used efficiently, which we show experimentally leads dramatic improvement retrieval quality. most significant property the directly defines quantization. quantization therefore fully integrated, essentially being one same. evaluated through on with ground truth, showing power approach, going as high 1 million images.

pattern recognition, computer science, linguistics, text recognition, machine learning, automatic classification, information fusion, feature detection, object detection, data science, image representation, language model, vocabulary tree, deep learning, scalable recognition, machine vision, natural language processing, object recognition, knowledge discovery",2006,3531,pattern recognition|computer science|linguistics|text recognition|machine learning|automatic classification|information fusion|feature detection|object detection|data science|image representation|language model|vocabulary tree|deep learning|scalable recognition|machine vision|natural language processing|object recognition|knowledge discovery,https://openalex.org/W1989484209
https://openalex.org/W2997617958,Natural Language Processing (Almost) from Scratch,"Natural Language Processing (Almost) from Scratch

We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity rec...

natural language, nlp task, natural language processing, computer science, language model, machine learning, computational linguistics",2011,3498,natural language|nlp task|natural language processing|computer science|language model|machine learning|computational linguistics,https://openalex.org/W1832693441|https://openalex.org/W2962739339|https://openalex.org/W2296283641|https://openalex.org/W2884001105|https://openalex.org/W2963929190|https://openalex.org/W2963918774|https://openalex.org/W2250861254|https://openalex.org/W2964236337|https://openalex.org/W1938755728|https://openalex.org/W2251803266|https://openalex.org/W3019166713|https://openalex.org/W3185341429|https://openalex.org/W2515462165|https://openalex.org/W2963042536
https://openalex.org/W2606974598,Get To The Point: Summarization with Pointer-Generator Networks,"Get To The Point: Summarization with Pointer-Generator Networks

Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these two shortcomings: liable reproduce factual details inaccurately, tend repeat themselves. In this work we propose novel architecture that augments standard attentional model in orthogonal ways. First, use hybrid pointer-generator network can copy words source via pointing, which aids accurate reproduction of information, while retaining ability produce through generator. Second, coverage keep track what has been summarized, discourages repetition. We apply our CNN / Daily Mail task, outperforming current state-of-the-art by at least 2 ROUGE points.

narrative summarization, pointer-generator networks, machine learning, computer science, machine learning research, natural language processing, text mining, computational linguistics, data science, video summarization, multi-modal summarization, entity summarization, automatic summarization",2017,3465,narrative summarization|pointer-generator networks|machine learning|computer science|machine learning research|natural language processing|text mining|computational linguistics|data science|video summarization|multi-modal summarization|entity summarization|automatic summarization,https://openalex.org/W3034999214|https://openalex.org/W2970419734|https://openalex.org/W2612675303|https://openalex.org/W2888482885
https://openalex.org/W3035390927,Unsupervised Cross-lingual Representation Learning at Scale,"Unsupervised Cross-lingual Representation Learning at Scale

Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. Proceedings of the 58th Annual Meeting Association for Computational Linguistics. 2020.

computer science, cross-language perspective, text mining, language model, natural language processing, language, large language model, cross-language retrieval, second language learning, multilingual pretraining, computational linguistics, data science, knowledge discovery, machine translation, cross-lingual representation, language learning",2020,3463,computer science|cross-language perspective|text mining|language model|natural language processing|language|large language model|cross-language retrieval|second language learning|multilingual pretraining|computational linguistics|data science|knowledge discovery|machine translation|cross-lingual representation|language learning,https://openalex.org/W3169483174
https://openalex.org/W1990100773,Why a Diagram is (Sometimes) Worth Ten Thousand Words,"Why a Diagram is (Sometimes) Worth Ten Thousand Words

We distinguish diagrammatic from sentential paper‐and‐pencil representations of information by developing alternative models information‐processing systems that are informationally equivalent and can be characterized as or diagrammatic. Sentential sequential, like the propositions in a text. Diagrammatic indexed location plane. also typically display is only implicit therefore has to computed, sometimes at great cost, make it explicit for use. then contrast computational efficiency these solving several illustrative problems mathematics physics. When two equivalent, their depends on operators act them. Two sets may differ capabilities recognizing patterns, inferences they carry out directly, control strategies (in particular, search). support all respects. Operators working one representation recognize features readily directly difficult realize other representation. Most important, however, differences search explicitness information. In we call diagrammatic, organized location, often much needed an inference present single location. addition, cues next logical step problem adjacent Therefore proceed through smooth traversal diagram, require very little computation elements had been implicit.

visualization, worth ten, computer science, design, information visualization, computer graphic, natural language processing, semantic graph, image representation, graph theory, graphical model, diagrammatic reasoning, graphics, graph drawing, conceptual graph, narrative, semantic evaluation, communication, image analysis",1987,3460,visualization|worth ten|computer science|design|information visualization|computer graphic|natural language processing|semantic graph|image representation|graph theory|graphical model|diagrammatic reasoning|graphics|graph drawing|conceptual graph|narrative|semantic evaluation|communication|image analysis,
https://openalex.org/W2963626623,Bag of Tricks for Efficient Text Classification,"Bag of Tricks for Efficient Text Classification

Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov. Proceedings of the 15th Conference European Chapter Association for Computational Linguistics: Volume 2, Short Papers. 2017.

computer science, text mining, natural language processing, data classification, efficient text classification, machine learning, document classification, data science, intelligent classification, deep learning, machine learning research, automatic classification, classification method, data mining",2017,3459,computer science|text mining|natural language processing|data classification|efficient text classification|machine learning|document classification|data science|intelligent classification|deep learning|machine learning research|automatic classification|classification method|data mining,https://openalex.org/W3035390927
https://openalex.org/W2158266063,Hierarchical Dirichlet Processes,"Hierarchical Dirichlet Processes

AbstractWe consider problems involving groups of data where each observation within a group is draw from mixture model and it desirable to share components between groups. We assume that the number unknown priori be inferred data. In this setting natural sets Dirichlet processes, one for group, well-known clustering property process provides nonparametric prior group. Given our desire tie models in various groups, we hierarchical model, specifically which base measure child processes itself distributed according process. Such being discrete, necessarily atoms. Thus, as desired, different components. discuss representations terms stick-breaking process, generalization Chinese restaurant refer “Chinese franchise.” present Markov chain Monte Carlo algorithms posterior inference mixtures describe applications information retrieval text modeling.KEY WORDS: ClusteringHierarchical modelMarkov CarloMixture modelNonparametric Bayesian statistics

dirichlet form, computer science, linguistics, text mining, natural language processing, informetrics, information structure, deep learning, hierarchical dirichlet processes",2006,3453,dirichlet form|computer science|linguistics|text mining|natural language processing|informetrics|information structure|deep learning|hierarchical dirichlet processes,https://openalex.org/W2174706414|https://openalex.org/W168564468|https://openalex.org/W2334889010|https://openalex.org/W1714665356
https://openalex.org/W2962879692,Improved training of wasserstein GANs,"Improved training of wasserstein GANs

Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable of GANs, sometimes can still generate only poor samples or fail to converge. We find that these problems often due the use weight clipping in WGAN enforce a Lipschitz constraint on critic, which lead undesired behavior. propose an alternative weights: penalize norm gradient critic with respect its input. Our method performs better than standard and enables wide variety architectures almost no hyperparameter tuning, including 101-layer ResNets language models continuous generators. also achieve high quality generations CIFAR-10 LSUN bedrooms.

improved training, natural language processing, gan power device, adversarial machine learning, scene interpretation, computer science, machine learning, generative adversarial network, deep learning, wasserstein gans",2017,3367,improved training|natural language processing|gan power device|adversarial machine learning|scene interpretation|computer science|machine learning|generative adversarial network|deep learning|wasserstein gans,
https://openalex.org/W1525595230,TextRank: Bringing Order into Text,"TextRank: Bringing Order into Text

In this paper, the authors introduce TextRank, a graph-based ranking model for text processing, and show how can be successfully used in natural language applications.

textual practice, linguistics, narrative extraction, text mining, natural language processing, automatic summarization, bringing order, text indexing, communication, computational linguistics, entity summarization, narrative",2004,3366,textual practice|linguistics|narrative extraction|text mining|natural language processing|automatic summarization|bringing order|text indexing|communication|computational linguistics|entity summarization|narrative,https://openalex.org/W3101913037
https://openalex.org/W2108646579,Sentiment Analysis and Opinion Mining,"Sentiment Analysis and Opinion Mining

With the rapid growth of social media, sentiment analysis, also called opinion mining, has become one most active research areas in natural language processing. Its application is widespread, from business services to political campaigns. This article gives an introduction this important area and presents some recent developments.

opinion mining, linguistics, opinion aggregation, social medium mining, narrative, content analysis, disinformation detection, communication, semantic evaluation, data science, data mining, machine learning research, text mining, natural language processing, affective computing, news analytics, knowledge discovery, social medium data, social media, sentiment analysis",2017,3306,opinion mining|linguistics|opinion aggregation|social medium mining|narrative|content analysis|disinformation detection|communication|semantic evaluation|data science|data mining|machine learning research|text mining|natural language processing|affective computing|news analytics|knowledge discovery|social medium data|social media|sentiment analysis,https://openalex.org/W2019759670|https://openalex.org/W2562607067|https://openalex.org/W2250966211|https://openalex.org/W2250879510|https://openalex.org/W2306941105|https://openalex.org/W2340954483
https://openalex.org/W2153653739,Statistical phrase-based translation,"Statistical phrase-based translation

We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate compare several, previously proposed models. Within our framework, we carry out large number of experiments understand better explain why models out-perform word-based Our empirical results, which hold for all examined language pairs, suggest the highest levels performance can be obtained through relatively simple means: heuristic learning phrase translations from alignments lexical weighting translations. Surprisingly, phrases longer than three words high-accuracy word-level alignment does not have strong impact on performance. Learning only syntactically motivated degrades systems.

terminology management, speech translation, applied linguistics, statistics, natural language processing, machine translation, statistical inference, multimodal translation, semantic evaluation, language model, neural machine translation, terminology extraction, linguistics, language, translation studies, statistical phrase-based translation, computational linguistics",2003,3299,terminology management|speech translation|applied linguistics|statistics|natural language processing|machine translation|statistical inference|multimodal translation|semantic evaluation|language model|neural machine translation|terminology extraction|linguistics|language|translation studies|statistical phrase-based translation|computational linguistics,https://openalex.org/W2157331557|https://openalex.org/W2964308564|https://openalex.org/W1902237438|https://openalex.org/W2950635152|https://openalex.org/W2964199361|https://openalex.org/W2133564696|https://openalex.org/W2884001105|https://openalex.org/W2172140247|https://openalex.org/W1663984431|https://openalex.org/W2742947407|https://openalex.org/W2152263452
https://openalex.org/W168564468,Software Framework for Topic Modelling with Large Corpora,"Software Framework for Topic Modelling with Large Corpora

Large corpora are ubiquitous in today's world and memory
quickly becomes the limiting factor practical applications
of Vector Space Model (VSM). We identify gap existing
VSM implementations, which is their scalability ease of
use. describe a Natural Language Processing software
framework based on idea of document streaming,
i.e. processing after document, memory
independent fashion. In this framework, we implement several
popular algorithms for topical inference, including Latent
Semantic Analysis Latent Dirichlet Allocation, way
that makes them completely independent training corpus
size. Particular emphasis placed straightforward and
intuitive framework design, so that modifications and
extensions methods and/or application by
interested practitioners effortless. demonstrate the
usefulness our approach real-world scenario of
computing similarities within an existing digital
library DML-CZ.

computer science, topic model, text mining, natural language processing, large language model, machine learning, knowledge discovery, large corpora, information retrieval, annotation tool",2010,3298,computer science|topic model|text mining|natural language processing|large language model|machine learning|knowledge discovery|large corpora|information retrieval|annotation tool,https://openalex.org/W2158139315
https://openalex.org/W4295521014,Improved Training of Wasserstein GANs,"Improved Training of Wasserstein GANs

Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable of GANs, sometimes can still generate only low-quality samples or fail to converge. We find that these problems often due the use weight clipping in WGAN enforce a Lipschitz constraint on critic, which lead undesired behavior. propose an alternative weights: penalize norm gradient critic with respect its input. Our method performs better than standard and enables wide variety architectures almost no hyperparameter tuning, including 101-layer ResNets language models over discrete data. also achieve high quality generations CIFAR-10 LSUN bedrooms.

improved training, natural language processing, gan power device, adversarial machine learning, scene interpretation, computer science, machine learning, generative adversarial network, deep learning, wasserstein gans",2017,3285,improved training|natural language processing|gan power device|adversarial machine learning|scene interpretation|computer science|machine learning|generative adversarial network|deep learning|wasserstein gans,https://openalex.org/W2962770929
https://openalex.org/W2022204871,Recognizing contextual polarity in phrase-level sentiment analysis,"Recognizing contextual polarity in phrase-level sentiment analysis

This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of expressions. With this approach, system able automatically identify contextual for large subset expressions, achieving results are significantly better than baseline.

linguistics, natural language processing, phrase-level sentiment analysis, deception detection, contextual polarity",2005,3278,linguistics|natural language processing|phrase-level sentiment analysis|deception detection|contextual polarity,https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2084046180|https://openalex.org/W2019759670|https://openalex.org/W2126581182|https://openalex.org/W1743243001|https://openalex.org/W2031998113|https://openalex.org/W71795751|https://openalex.org/W2250879510|https://openalex.org/W2306941105
https://openalex.org/W2068737686,Automatic acquisition of hyponyms from large text corpora,"Automatic acquisition of hyponyms from large text corpora

We describe a method for the automatic acquisition of hyponymy lexical relation from unrestricted text. Two goals motivate approach: (i) avoidance need pre-encoded knowledge and (ii) applicability across wide range identify set lexico-syntactic patterns that are easily recognizable, occur frequently text genre boundaries, indisputably indicate interest. discovering these suggest other relations will also be acquirable in this way. A subset algorithm is implemented results used to augment critique structure large hand-built thesaurus. Extensions applications areas such as information retrieval suggested.

pattern recognition, computer science, linguistics, computational linguistics, automatic classification, information fusion, semantic evaluation, terminology extraction, data science, machine translation, keyword extraction, large text corpora, machine learning research, terminology management, text mining, natural language processing, information extraction, automatic acquisition, knowledge discovery, corpus linguistics",1992,3274,pattern recognition|computer science|linguistics|computational linguistics|automatic classification|information fusion|semantic evaluation|terminology extraction|data science|machine translation|keyword extraction|large text corpora|machine learning research|terminology management|text mining|natural language processing|information extraction|automatic acquisition|knowledge discovery|corpus linguistics,https://openalex.org/W2107598941|https://openalex.org/W2020278455|https://openalex.org/W2436001372|https://openalex.org/W2912924812|https://openalex.org/W2252136820|https://openalex.org/W86887328
https://openalex.org/W1521626219,Natural Language Processing with Python,"Natural Language Processing with Python

This book offers a highly accessible introduction to natural language processing, the field that supports variety of technologies, from predictive text and email filtering automatic summarization translation. With it, you'll learn how write Python programs work with large collections unstructured text. You'll access richly annotated datasets using comprehensive range linguistic data structures, understand main algorithms for analyzing content structure written communication. Packed examples exercises, Natural Language Processing will help you: Extract information text, either guess topic or identify named entities Analyze in including parsing semantic analysis Access popular databases, WordNet treebanks Integrate techniques drawn fields as diverse linguistics artificial intelligence you gain practical skills processing programming Toolkit (NLTK) open source library. If you're interested developing web applications, multilingual news sources, documenting endangered languages -- if simply curious have programmer's perspective on human works find both fascinating immensely useful.

computer science, text mining, natural language processing, language model, language engineering, computational linguistics, natural language, nlp task",2009,3252,computer science|text mining|natural language processing|language model|language engineering|computational linguistics|natural language|nlp task,https://openalex.org/W2123442489
https://openalex.org/W2113459411,Learning Word Vectors for Sentiment Analysis,"Learning Word Vectors for Sentiment Analysis

Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail capture sentiment information that is central many word meanings and important for a wide range of NLP tasks. We present uses mix unsupervised supervised techniques learn vectors capturing semantic term--document as well content. The proposed leverage both continuous multi-dimensional non-sentiment annotations. instantiate the utilize document-level polarity annotations in online documents (e.g. star ratings). evaluate using small, widely used subjectivity corpora find it out-performs several previously introduced methods classification. also introduce large dataset movie reviews serve more robust benchmark work this area.

computer science, keyword extraction, text mining, natural language processing, machine learning, computational linguistics, nlp task, sentiment analysis, word vectors",2011,3243,computer science|keyword extraction|text mining|natural language processing|machine learning|computational linguistics|nlp task|sentiment analysis|word vectors,https://openalex.org/W2950577311|https://openalex.org/W2949547296|https://openalex.org/W2108646579|https://openalex.org/W2963026768|https://openalex.org/W2131744502|https://openalex.org/W3034238904|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W2250879510
https://openalex.org/W2096175520,A maximum entropy approach to natural language processing,"A maximum entropy approach to natural language processing

The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough permit the widescale application this real world problems in statistical estimation and pattern recognition. In paper, we describe a method for modeling based on entropy. We present maximum-likelihood approach automatically constructing models how implement efficiently, using as examples several natural language processing.

computer science, artificial intelligence, linguistics, language model, natural language processing, language, machine learning, entropy, computational linguistics, nlp task",1996,3141,computer science|artificial intelligence|linguistics|language model|natural language processing|language|machine learning|entropy|computational linguistics|nlp task,https://openalex.org/W2146089916|https://openalex.org/W1535015163
https://openalex.org/W2165094119,Fuzzy logic = computing with words,"Fuzzy logic = computing with words

As its name suggests, computing with words (CW) is a methodology in which are used place of numbers for and reasoning. The point this note that fuzzy logic plays pivotal role CW vice-versa. Thus, as an approximation, may be equated to CW. There two major imperatives words. First, necessity when the available information too imprecise justify use numbers, second, there tolerance imprecision can exploited achieve tractability, robustness, low solution cost, better rapport reality. Exploitation issue central importance In CW, word viewed label granule; is, set points drawn together by similarity, playing constraint on variable. premises assumed expressed propositions natural language. coming years, likely evolve into basic own right wide-ranging ramifications both applied levels.

computer science, knowledge representation and reasoning, fuzzy logic, natural language processing, automated reasoning, computational linguistics, fuzzy computing, fuzzy mathematics, logic in computer science",1996,3060,computer science|knowledge representation and reasoning|fuzzy logic|natural language processing|automated reasoning|computational linguistics|fuzzy computing|fuzzy mathematics|logic in computer science,
https://openalex.org/W1993750641,The Language Instinct,"The Language Instinct

In this extremely valuable book, very informative, and well written (Noam Chomsky), one of the greatest thinkers in field linguistics explains how language works--how people, ny making noises with their mouths, can cause ideas to arise other people's minds.

linguistics, narrative, second language acquisition, psycholinguistics, language, language comprehension, cognitive science, endangered language, language science, language adaptation, cognitive linguistics, philosophy of language, general linguistics, language processing in the brain, language learning, applied linguistics, language instinct, natural language processing, language acquisition",1994,3054,linguistics|narrative|second language acquisition|psycholinguistics|language|language comprehension|cognitive science|endangered language|language science|language adaptation|cognitive linguistics|philosophy of language|general linguistics|language processing in the brain|language learning|applied linguistics|language instinct|natural language processing|language acquisition,https://openalex.org/W2092654472
https://openalex.org/W2097606805,Accurate unlexicalized parsing,"Accurate unlexicalized parsing

We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance 86.36% (LP/LR F1) is better early lexicalized models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing strong lower bound on maximum possible accuracy models: compact, easier replicate, interpret complex lexical parsing algorithms are simpler, widely understood, asymptotic complexity, optimize.

computer science, linguistics, keyword extraction, natural language processing, language, grammar, syntax, computational linguistics, semantics, accurate unlexicalized parsing, semantic parsing, syntactic parsing",2003,3051,computer science|linguistics|keyword extraction|natural language processing|language|grammar|syntax|computational linguistics|semantics|accurate unlexicalized parsing|semantic parsing|syntactic parsing,https://openalex.org/W2251939518|https://openalex.org/W2949547296|https://openalex.org/W1840435438|https://openalex.org/W2131744502|https://openalex.org/W2169818249|https://openalex.org/W1743243001|https://openalex.org/W2948947170
https://openalex.org/W938539187,"Algorithms on Strings, Trees and Sequences","Algorithms on Strings, Trees and Sequences

String algorithms are a traditional area of study in computer science. In recent years their importance has grown dramatically with the huge increase electronically stored text and molecular sequence data (DNA or protein sequences) produced by various genome projects. This book is general on for string processing. addition to pure science, contains extensive discussions biological problems that cast as problems, methods developed solve them. It emphasises fundamental ideas techniques central today's applications. New approaches this complex material simplify up now have been specialist alone. With over 400 exercises reinforce develop additional topics, suitable graduate advanced undergraduate students computational biology, bio-informatics. Its discussion current also makes it reference professionals.

pattern recognition, computer science, fuzzy logic, tree language, natural language processing, homological algebra, sequence analysis, treebanks, theory of computation, combinatorial theory, machine learning, applied mathematics, numerical algorithm, automated reasoning, string-searching algorithm, string processing",1997,2970,pattern recognition|computer science|fuzzy logic|tree language|natural language processing|homological algebra|sequence analysis|treebanks|theory of computation|combinatorial theory|machine learning|applied mathematics|numerical algorithm|automated reasoning|string-searching algorithm|string processing,https://openalex.org/W2001496424
https://openalex.org/W2963012544,Character-level Convolutional Networks for Text Classification,"Character-level Convolutional Networks for Text Classification

This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag words, n-grams and their TFIDF variants, deep learning word-based ConvNets recurrent neural networks.

knowledge discovery, automatic classification, text recognition, computer science, machine learning research, text mining, deep learning, document classification, character recognition, pattern recognition, machine vision, natural language processing, image representation, character-level convolutional networks, machine learning, linguistics, data science, text classification, computational intelligence, convolutional neural network",2015,2966,knowledge discovery|automatic classification|text recognition|computer science|machine learning research|text mining|deep learning|document classification|character recognition|pattern recognition|machine vision|natural language processing|image representation|character-level convolutional networks|machine learning|linguistics|data science|text classification|computational intelligence|convolutional neural network,https://openalex.org/W2493916176|https://openalex.org/W2296283641|https://openalex.org/W2963626623|https://openalex.org/W2963026768|https://openalex.org/W3034238904|https://openalex.org/W1938755728|https://openalex.org/W3105625590
https://openalex.org/W1970026646,The String-to-String Correction Problem,"The String-to-String Correction Problem

The string-to-string correction problem is to determine the distance between two strings as measured by minimum cost sequence of “edit operations” needed change one string into other. edit operations investigated allow changing symbol a another single symbol, deleting from string, or inserting string. An algorithm presented which solves this in time proportional product lengths strings. Possible applications are problems automatic spelling and determining longest subsequence characters common

computer science, natural language processing, string-to-string correction problem, error correction code, string-searching algorithm, machine learning research, error correction, string processing",1974,2945,computer science|natural language processing|string-to-string correction problem|error correction code|string-searching algorithm|machine learning research|error correction|string processing,https://openalex.org/W2001496424|https://openalex.org/W2010595692
https://openalex.org/W4211186029,Sentiment Analysis and Opinion Mining,"Sentiment Analysis and Opinion Mining

Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, emotions from written language. It one most active resear

knowledge discovery, news analytics, machine learning research, text mining, affective computing, social medium data, data mining, natural language processing, social media, opinion aggregation, linguistics, content analysis, data science, disinformation detection, sentiment analysis, social medium mining, narrative, semantic evaluation, communication, opinion mining",2012,2942,knowledge discovery|news analytics|machine learning research|text mining|affective computing|social medium data|data mining|natural language processing|social media|opinion aggregation|linguistics|content analysis|data science|disinformation detection|sentiment analysis|social medium mining|narrative|semantic evaluation|communication|opinion mining,https://openalex.org/W2019759670
https://openalex.org/W4236521339,ELIZA—a computer program for the study of natural language communication between man and machine,"ELIZA—a computer program for the study of natural language communication between man and machine

article Free Access Share on ELIZA—a computer program for the study of natural language communication between man and machine Author: Joseph Weizenbaum Massachusetts Institute Technology, Cambridge CambridgeView Profile Authors Info & Claims Communications ACMVolume 9Issue 1Jan. 1966 pp 36–45https://doi.org/10.1145/365153.365168Published:01 January 1966Publication History 2,074citation19,466DownloadsMetricsTotal Citations2,074Total Downloads19,466Last 12 Months5,621Last 6 weeks843 Get Citation AlertsNew Alert added!This alert has been successfully added will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log in to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

computer science, artificial intelligence, speech recognition, human-machine interaction, computational linguistics, human-ai interaction, language, communication, machine translation, human-computer interaction, language model, computer program, natural language communication, natural language processing, chatbot, language technology, speech communication, spoken language technology, language generation",1966,2938,computer science|artificial intelligence|speech recognition|human-machine interaction|computational linguistics|human-ai interaction|language|communication|machine translation|human-computer interaction|language model|computer program|natural language communication|natural language processing|chatbot|language technology|speech communication|spoken language technology|language generation,https://openalex.org/W1663984431|https://openalex.org/W3019166713
https://openalex.org/W2147768505,Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition,"Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition

We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks phone recognition. describe pre-trained neural network hidden Markov (DNN-HMM) hybrid architecture trains the DNN to produce distribution over senones (tied triphone states) as its output. The pre-training algorithm is robust and often helpful way initialize generatively can aid optimization reduce generalization error. illustrate key components of our model, procedure applying CD-DNN-HMMs LVSR, analyze effects various modeling choices on performance. Experiments challenging business search dataset demonstrate significantly outperform conventional Gaussian mixture (GMM)-HMMs, with an absolute sentence accuracy improvement 5.8% 9.2% (or relative error reduction 16.0% 23.2%) CD-GMM-HMMs trained minimum rate (MPE) maximum-likelihood (ML) criteria, respectively.

computer science, speech recognition, voice recognition, language model, natural language processing, speech processing, spoken language processing, machine learning, cognitive science, data science, language recognition, deep learning, speech communication, large-vocabulary speech recognition, speech technology, spoken language technology, language learning, speech corpus",2012,2938,computer science|speech recognition|voice recognition|language model|natural language processing|speech processing|spoken language processing|machine learning|cognitive science|data science|language recognition|deep learning|speech communication|large-vocabulary speech recognition|speech technology|spoken language technology|language learning|speech corpus,https://openalex.org/W2157331557|https://openalex.org/W2130942839|https://openalex.org/W2950635152|https://openalex.org/W2949888546|https://openalex.org/W2193413348|https://openalex.org/W1922655562|https://openalex.org/W2951534261
https://openalex.org/W1550206324,A comparison of event models for naive bayes text classification,"A comparison of event models for naive bayes text classification

Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, Bayesian Network with no dependencies between words and binary word features (e.g. Larkey Croft 1996; Koller Sahami 1997). Others multinomial uni-gram language model integer counts Lewis Gale 1994; Mitchell This paper aims to clarify confusion by describing differences details these models, empirically comparing their performance on five corpora. We find performs well small vocabulary sizes, but usually even better at larger sizes--providing average 27% reduction error over any size.

computer science, text mining, natural language processing, machine learning, bayesian analysis, document classification, data science, machine learning research, automatic classification, event models, classification method",1998,2933,computer science|text mining|natural language processing|machine learning|bayesian analysis|document classification|data science|machine learning research|automatic classification|event models|classification method,https://openalex.org/W2963626623|https://openalex.org/W2005422315|https://openalex.org/W2127480961|https://openalex.org/W3105625590
https://openalex.org/W1508522130,The Foucault Reader,"The Foucault Reader

This is an introduction to Foucault's thought, which includes some previously unpublished material.

english, information fusion, natural language processing, character recognition, machine-readable dictionary, principal component analysis, computer vision, clustering, digital scholarship, biometrics, reading research, victorian literature, foucault reader, multimedia information processing, sociology, comparative literature, reference frame, document processing",1984,2920,english|information fusion|natural language processing|character recognition|machine-readable dictionary|principal component analysis|computer vision|clustering|digital scholarship|biometrics|reading research|victorian literature|foucault reader|multimedia information processing|sociology|comparative literature|reference frame|document processing,
https://openalex.org/W1840435438,A large annotated corpus for learning natural language inference,"A large annotated corpus for learning natural language inference

Understanding entailment and contradiction is fundamental to understanding natural language, inference about a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by lack large-scale resources. To address this, we introduce Stanford Natural Language Inference corpus, new, freely available collection labeled sentence pairs, written humans doing novel grounded task based on image captioning. At 570K it two orders magnitude larger than all other resources its type. This increase scale allows lexicalized classifiers outperform some sophisticated existing models, neural network-based model perform competitively language benchmarks first time.

language learning, corpus linguistics, language corpus, nlp task, natural language processing, keyword extraction, large annotated corpus, natural language inference, computer science, machine learning, large language model, deep learning, computational linguistics",2015,2919,language learning|corpus linguistics|language corpus|nlp task|natural language processing|keyword extraction|large annotated corpus|natural language inference|computer science|machine learning|large language model|deep learning|computational linguistics,https://openalex.org/W2896457183|https://openalex.org/W2962739339|https://openalex.org/W2970641574|https://openalex.org/W3035390927|https://openalex.org/W2963918774|https://openalex.org/W2912924812|https://openalex.org/W2413794162|https://openalex.org/W2953356739|https://openalex.org/W2963691697|https://openalex.org/W2891177506|https://openalex.org/W2948947170
https://openalex.org/W2120615054,A Convolutional Neural Network for Modelling Sentences,"A Convolutional Neural Network for Modelling Sentences

The ability to accurately represent sentences is central language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for semantic modelling of sentences. network uses k-Max Pooling, global pooling operation over linear sequences. handles input varying length and induces feature graph sentence capable explicitly capturing short long-range relations. does not rely on parse tree easily applicable any language. test DCNN in four experiments: small scale binary multi-class sentiment prediction, six-way question classification Twitter prediction by distant supervision. achieves excellent performance first three tasks greater than 25% error reduction last task with respect strongest baseline.

linguistics, computer science, machine learning, computational intelligence, natural language generation, large language model, machine translation, neural network (machine learning), machine learning research, semantic evaluation, natural language processing, neural machine translation, convolutional neural network, nlp task, deep learning, sequence modelling, cognitive science, language model",2014,2918,linguistics|computer science|machine learning|computational intelligence|natural language generation|large language model|machine translation|neural network (machine learning)|machine learning research|semantic evaluation|natural language processing|neural machine translation|convolutional neural network|nlp task|deep learning|sequence modelling|cognitive science|language model,https://openalex.org/W1832693441|https://openalex.org/W2884001105|https://openalex.org/W1544827683|https://openalex.org/W2963625095|https://openalex.org/W2964236337|https://openalex.org/W2250966211|https://openalex.org/W1938755728|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2170738476|https://openalex.org/W3185341429
https://openalex.org/W2963026768,Universal Language Model Fine-tuning for Text Classification,"Universal Language Model Fine-tuning for Text Classification

Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective method that can be applied to any task NLP, introduce techniques are key for fine-tuning a language model. Our significantly outperforms the state-of-the-art on six text classification tasks, reducing error by 18-24% majority of datasets. Furthermore, with only 100 labeled examples, it matches performance scratch times more data. open-source our pretrained models code.

model tuning, natural language processing, llm fine-tuning, text processing, language model, machine learning, language, text classification",2018,2912,model tuning|natural language processing|llm fine-tuning|text processing|language model|machine learning|language|text classification,https://openalex.org/W2896457183|https://openalex.org/W2979826702|https://openalex.org/W2996428491|https://openalex.org/W2980282514|https://openalex.org/W2970771982|https://openalex.org/W3030163527|https://openalex.org/W3034238904|https://openalex.org/W3098824823|https://openalex.org/W2953356739|https://openalex.org/W3185341429|https://openalex.org/W3169483174
https://openalex.org/W2131744502,Distributed Representations of Sentences and Documents,"Distributed Representations of Sentences and Documents

Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes texts, one of most common features is bag-of-words. Despite their popularity, bag-of-words have two major weaknesses: they lose ordering words and also ignore semantics words. For example, powerful, strong Paris are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns representations from variable-length pieces such sentences, paragraphs, documents. Our represents each document by dense vector which trained predict in document. Its construction gives our potential overcome weaknesses models. Empirical results show Vectors outperforms models well other techniques for text representations. Finally, achieve new state-of-the-art on several classification sentiment analysis tasks.

computer science, information fusion, distributed system, nlp task, text mining, word embeddings, distributed representations, language resource, computational linguistics, distributed learning, linguistics, language, natural language processing, content representation, natural language generation, information retrieval, distributional semantics, image representation, language model",2014,2897,computer science|information fusion|distributed system|nlp task|text mining|word embeddings|distributed representations|language resource|computational linguistics|distributed learning|linguistics|language|natural language processing|content representation|natural language generation|information retrieval|distributional semantics|image representation|language model,https://openalex.org/W2896457183|https://openalex.org/W2884001105|https://openalex.org/W2963918774|https://openalex.org/W2265846598|https://openalex.org/W2963223306|https://openalex.org/W658020064|https://openalex.org/W2250966211|https://openalex.org/W3019166713|https://openalex.org/W2340954483
https://openalex.org/W2946657092,GEPIA2: an enhanced web server for large-scale expression profiling and interactive analysis,"GEPIA2: an enhanced web server for large-scale expression profiling and interactive analysis

Abstract Introduced in 2017, the GEPIA (Gene Expression Profiling Interactive Analysis) web server has been a valuable and highly cited resource for gene expression analysis based on tumor normal samples from TCGA GTEx databases. Here, we present GEPIA2, an updated enhanced version to provide insights with higher resolution more functionalities. Featuring 198 619 isoforms 84 cancer subtypes, GEPIA2 extended quantification level transcript level, supports of specific subtype, comparison between subtypes. In addition, adopted new techniques signature inspired by single-cell sequencing studies, provides customized where users can upload their own RNA-seq data compare them samples. We also offer API batch process easy retrieval results. The is publicly accessible at http://gepia2.cancer-pku.cn/.

computer science, machine learning, molecular biology, bioinformatics, profiling technique, statistical software, gene expression profiling, computational genomics, profiling tool, enhanced web server, biostatistics, computational biology, machine learning research, pathway analysis, interactive analysis, large-scale expression profiling, text mining, natural language processing, molecular informatics",2019,2888,computer science|machine learning|molecular biology|bioinformatics|profiling technique|statistical software|gene expression profiling|computational genomics|profiling tool|enhanced web server|biostatistics|computational biology|machine learning research|pathway analysis|interactive analysis|large-scale expression profiling|text mining|natural language processing|molecular informatics,
https://openalex.org/W2099964107,Efficient string matching,"Efficient string matching

This paper describes a simple, efficient algorithm to locate all occurrences of any finite number keywords in string text. The consists constructing state pattern matching machine from the and then using process text single pass. Construction takes time proportional sum lengths keywords. transitions made by processing is independent has been used improve speed library bibliographic search program factor 5 10.

computer science, natural language processing, pattern matching, matching technique, similarity search, string-searching algorithm, string processing",1975,2873,computer science|natural language processing|pattern matching|matching technique|similarity search|string-searching algorithm|string processing,https://openalex.org/W2001496424|https://openalex.org/W2010595692
https://openalex.org/W2121227244,Class-based n -gram models of natural language,"Class-based n -gram models of natural language

We address the problem of predicting a word from previous words in sample text. In particular, we discuss n-gram models based on classes words. also several statistical algorithms for assigning to frequency their co-occurrence with other find that are able extract have flavor either syntactically groupings or semantically groupings, depending nature underlying statistics.

linguistics, large language model, natural language processing, language model, language, computational linguistics, natural language",1992,2872,linguistics|large language model|natural language processing|language model|language|computational linguistics|natural language,https://openalex.org/W2896457183|https://openalex.org/W2158899491|https://openalex.org/W2096175520|https://openalex.org/W1659833910|https://openalex.org/W2158139315|https://openalex.org/W2158108973|https://openalex.org/W2010595692|https://openalex.org/W2199803028|https://openalex.org/W2153848201|https://openalex.org/W2130337399|https://openalex.org/W2740168486|https://openalex.org/W2127314673
https://openalex.org/W2084046180,Lexicon-Based Methods for Sentiment Analysis,"Lexicon-Based Methods for Sentiment Analysis

We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), incorporates intensification negation. SO-CAL is applied the polarity classification task, process assigning positive or negative label text that captures text's opinion towards its main subject matter. show SO-CAL's performance consistent across domains in completely unseen data. Additionally, we describe dictionary creation, our use Mechanical Turk check for consistency reliability.

computer science, linguistics, text mining, natural language processing, semantic analysis (linguistics), affective computing, multimodal sentiment analysis, lexicon-based methods, lexicon, machine learning research, sentiment analysis",2011,2860,computer science|linguistics|text mining|natural language processing|semantic analysis (linguistics)|affective computing|multimodal sentiment analysis|lexicon-based methods|lexicon|machine learning research|sentiment analysis,https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2031998113|https://openalex.org/W2612769033|https://openalex.org/W2250879510|https://openalex.org/W2964010806|https://openalex.org/W1987425720
https://openalex.org/W2165880886,Robust text-independent speaker identification using Gaussian mixture speaker models,"Robust text-independent speaker identification using Gaussian mixture speaker models

This paper introduces and motivates the use of Gaussian mixture models (GMM) for robust text-independent speaker identification. The individual components a GMM are shown to represent some general speaker-dependent spectral shapes that effective modeling identity. focus this work is on applications which require high identification rates using short utterance from unconstrained conversational speech robustness degradations produced by transmission over telephone channel. A complete experimental evaluation model conducted 49 speaker, database. experiments examine algorithmic issues (initialization, variance limiting, order selection), variability techniques, large population performance, comparisons other techniques (uni-modal Gaussian, VQ codebook, tied mixture, radial basis functions). attains 96.8% accuracy 5 second clean utterances 80.8% 15 with outperform an identical 16 task.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

speech recognition, voice recognition, natural language processing, speech processing, language, multi-speaker speech recognition, machine learning, speaker recognition, speech separation, speech communication, spoken language technology, robust speech recognition",1995,2817,speech recognition|voice recognition|natural language processing|speech processing|language|multi-speaker speech recognition|machine learning|speaker recognition|speech separation|speech communication|spoken language technology|robust speech recognition,
https://openalex.org/W2998704965,A neural probabilistic language model,"A neural probabilistic language model

A goal of statistical language modeling is to learn the joint probability function sequences words in a language. This intrinsically difficult because curse dimensionality: wor...

computer science, linguistics, large language model, language model, natural language processing, language, computational linguistics, language processing in the brain, language learning",2003,2805,computer science|linguistics|large language model|language model|natural language processing|language|computational linguistics|language processing in the brain|language learning,https://openalex.org/W2964110616|https://openalex.org/W2884001105|https://openalex.org/W2962965405|https://openalex.org/W2963918774|https://openalex.org/W2171928131|https://openalex.org/W2964236337|https://openalex.org/W2970476646|https://openalex.org/W1938755728|https://openalex.org/W2251803266|https://openalex.org/W3019166713|https://openalex.org/W2963758027|https://openalex.org/W2963042536
https://openalex.org/W2146574666,Minimum error rate training in statistical machine translation,"Minimum error rate training in statistical machine translation

Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria. A general problem of this approach that there only a loose relation to final quality unseen text. In paper, we analyze various criteria which directly optimize quality. These make use recently proposed automatic evaluation metrics. We describe new algorithm efficient an unsmoothed error count. show significantly better results can often be obtained if criterion taken into account as part procedure.

computer science, statistical machine translation, natural language processing, machine translation",2003,2804,computer science|statistical machine translation|natural language processing|machine translation,https://openalex.org/W2962965405|https://openalex.org/W2963206148|https://openalex.org/W2158108973|https://openalex.org/W2152263452
https://openalex.org/W1552847225,"DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia","DBpedia – A large-scale, multilingual knowledge base extracted from Wikipedia

The DBpedia community project extracts structured, multilingual knowledge from Wikipedia and makes it freely available on the Web using Semantic Linked Data technologies. 111 different language editions of

language resource, computer science, knowledge representation and reasoning, knowledge graph, natural language processing, knowledge management, online information, data science, knowledge discovery, knowledge base, content similarity detection, knowledge extraction, nlp task, semantic web, information retrieval, multilingual knowledge base, multilingualism, relationship extraction",2015,2791,language resource|computer science|knowledge representation and reasoning|knowledge graph|natural language processing|knowledge management|online information|data science|knowledge discovery|knowledge base|content similarity detection|knowledge extraction|nlp task|semantic web|information retrieval|multilingual knowledge base|multilingualism|relationship extraction,https://openalex.org/W2963012544
https://openalex.org/W1996430422,Feature-rich part-of-speech tagging with a cyclic dependency network,"Feature-rich part-of-speech tagging with a cyclic dependency network

We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and tag contexts via dependency network representation, (ii) broad lexical features, including jointly conditioning on multiple consecutive words, (iii) effective priors in conditional loglinear models, (iv) fine-grained modeling unknown word features. Using these ideas together, resulting gives 97.24% accuracy Penn Treebank WSJ, an error reduction 4.4% best previous single automatically learned tagging result.

computer science, linguistics, speech recognition, feature-rich part-of-speech tagging, computational linguistics, speech corpus, information fusion, communication, semantic evaluation, nlp task, cyclic dependency network, language model, dependency linguistics, part-of-speech tagging, language learning, phonology, annotation tool, natural language processing, knowledge discovery, spoken language technology",2003,2766,computer science|linguistics|speech recognition|feature-rich part-of-speech tagging|computational linguistics|speech corpus|information fusion|communication|semantic evaluation|nlp task|cyclic dependency network|language model|dependency linguistics|part-of-speech tagging|language learning|phonology|annotation tool|natural language processing|knowledge discovery|spoken language technology,https://openalex.org/W2123442489|https://openalex.org/W2158899491|https://openalex.org/W2250861254|https://openalex.org/W2158108973|https://openalex.org/W2153848201|https://openalex.org/W2113772582
https://openalex.org/W2141599568,Linguistic Regularities in Continuous Space Word Representations,"Linguistic Regularities in Continuous Space Word Representations

Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by input-layer weights. We find these surprisingly good at capturing syntactic and semantic regularities in language, each relationship is characterized relation-specific vector offset. This allows vector-oriented reasoning based on offsets between words. For example, male/female automatically learned, with induced representations, “King Man + Woman” very close to “Queen.” demonstrate vectors capture means analogy questions (provided paper), able correctly answer almost 40% questions. using offset method SemEval-2012 Task 2 Remarkably, outperforms best previous systems.

applied linguistics, semantic representation, computer science, linguistics, word embeddings, language model, natural language processing, language, general linguistics, syntax, linguistic regularities, computational linguistics, semantics, linguistic typology, distributional semantics, cross-lingual representation",2013,2757,applied linguistics|semantic representation|computer science|linguistics|word embeddings|language model|natural language processing|language|general linguistics|syntax|linguistic regularities|computational linguistics|semantics|linguistic typology|distributional semantics|cross-lingual representation,https://openalex.org/W2250539671|https://openalex.org/W2153579005|https://openalex.org/W2950577311|https://openalex.org/W2949547296|https://openalex.org/W2131744502|https://openalex.org/W2265846598|https://openalex.org/W2125031621|https://openalex.org/W658020064|https://openalex.org/W2126725946|https://openalex.org/W1527575280|https://openalex.org/W2251803266
https://openalex.org/W2107598941,Distant supervision for relation extraction without labeled data,"Distant supervision for relation extraction without labeled data

Modern models of relation extraction for tasks like ACE are based on supervised learning relations from small hand-labeled corpora. We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence ACE-style algorithms, and allowing use corpora any size. Our experiments Freebase, a large semantic database several thousand relations, to provide distant supervision. For each pair entities appears in some Freebase relation, we find all sentences containing those unlabeled corpus extract textual features train classifier. algorithm combines advantages IE (combining 400,000 noisy pattern probabilistic classifier) unsupervised (extracting numbers domain). model is able 10,000 instances 102 at precision 67.6%. also analyze feature performance, showing syntactic parse particularly helpful ambiguous or lexically their expression.

computer science, relation extraction, natural language processing, distant supervision, relationship extraction",2009,2749,computer science|relation extraction|natural language processing|distant supervision|relationship extraction,https://openalex.org/W2517194566|https://openalex.org/W1663984431|https://openalex.org/W2153848201|https://openalex.org/W2251135946|https://openalex.org/W2515462165
https://openalex.org/W3166396011,Learning Transferable Visual Models From Natural Language Supervision,"Learning Transferable Visual Models From Natural Language Supervision

State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form supervision limits their generality and usability since additional labeled data is needed specify any other visual concept. Learning directly from raw text about images promising alternative which leverages much broader source supervision. We demonstrate that the simple pre-training task predicting caption goes with image an efficient scalable way learn SOTA representations scratch on dataset 400 million (image, text) pairs collected internet. After pre-training, natural language used reference learned concepts (or describe new ones) enabling zero-shot transfer model downstream tasks. study performance this approach by benchmarking over 30 different existing datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, many types fine-grained classification. The transfers non-trivially most often competitive fully supervised baseline without need for specific training. For instance, we match accuracy original ResNet-50 ImageNet needing use 1.28 training examples it was on. release our code pre-trained weights at https://github.com/OpenAI/CLIP.

transferable visual models, transfer learning, natural language processing, vision language model, scene interpretation, computer science, language model, machine learning, natural language supervision, deep learning",2021,2746,transferable visual models|transfer learning|natural language processing|vision language model|scene interpretation|computer science|language model|machine learning|natural language supervision|deep learning,
https://openalex.org/W2923014074,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding

Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task struggle with out-of-domain data. If we aspire develop understanding beyond detection of superficial correspondences between inputs outputs, then it critical unified model that can execute range linguistic tasks across different domains. To facilitate research in this direction, present General Language Understanding Evaluation (GLUE, gluebenchmark.com): benchmark nine diverse tasks, an auxiliary dataset probing phenomena, online platform evaluating comparing models. For some training data plentiful, but others limited or does not match genre test set. GLUE thus favors represent knowledge way facilitates sample-efficient learning effective knowledge-transfer tasks. While none datasets were created from scratch benchmark, four them feature privately-held data, which used ensure fairly. We evaluate baselines use ELMo (Peters et al., 2018), powerful transfer technique, as well state-of-the-art sentence representation The best still achieve fairly low absolute scores. Analysis our diagnostic yields similarly weak performance over all phenomena tested, exceptions.

natural language understanding, nlp task, computer science, language model, deep learning, multi-task benchmark, information fusion, natural language processing, machine translation, language testing, multi-task learning, language learning, machine learning, language engineering, data science, computational linguistics, analysis platform, question answering, cognitive science, semantic evaluation, large language model",2018,2727,natural language understanding|nlp task|computer science|language model|deep learning|multi-task benchmark|information fusion|natural language processing|machine translation|language testing|multi-task learning|language learning|machine learning|language engineering|data science|computational linguistics|analysis platform|question answering|cognitive science|semantic evaluation|large language model,https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W3035390927|https://openalex.org/W2970476646|https://openalex.org/W3019166713|https://openalex.org/W3105966348|https://openalex.org/W2953356739|https://openalex.org/W2948947170
https://openalex.org/W2005422315,A re-examination of text categorization methods,"A re-examination of text categorization methods

Article Free Access Share on A re-examination of text categorization methods Authors: Yiming Yang School Computer Science, Carnegie Mellon University, Pittsburgh, PA PAView Profile , Xin Liu Authors Info & Claims SIGIR '99: Proceedings the 22nd annual international ACM conference Research and development in information retrievalAugust 1999Pages 42–49https://doi.org/10.1145/312624.312647Published:01 August 1999Publication History 1,646citation8,975DownloadsMetricsTotal Citations1,646Total Downloads8,975Last 12 Months668Last 6 weeks66 Get Citation AlertsNew Alert added!This alert has been successfully added will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

computer science, text mining, natural language processing, text categorization methods, document classification",1999,2620,computer science|text mining|natural language processing|text categorization methods|document classification,https://openalex.org/W2118020653|https://openalex.org/W2150102617|https://openalex.org/W2127480961
https://openalex.org/W4243041937,Analyzing Linguistic Data,"Analyzing Linguistic Data

Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides straightforward introduction statistical language. Designed with non-mathematical background, it clearly introduces basic principles methods analysis, using 'R', leading computational statistics programme. The reader guided step-by-step through range real data sets, analyse acoustic data, construct grammatical trees variety languages, quantify register variation in corpus linguistics, measure experimental state-of-the-art models. visualization plays key role, both initial stages exploration later on when encouraged criticize various Containing over 40 exercises model answers, this book will be welcomed by all wishing learn more about working presenting

linguistics, computational linguistics, language, speech analysis, semantic evaluation, discourse analysis, data science, data extraction, keyword extraction, semantic analysis (linguistics), general linguistics, applied linguistics, text mining, natural language processing, information extraction, linguistic data, knowledge discovery, speech communication, corpus linguistics",2008,2615,linguistics|computational linguistics|language|speech analysis|semantic evaluation|discourse analysis|data science|data extraction|keyword extraction|semantic analysis (linguistics)|general linguistics|applied linguistics|text mining|natural language processing|information extraction|linguistic data|knowledge discovery|speech communication|corpus linguistics,
https://openalex.org/W2980282514,HuggingFace's Transformers: State-of-the-art Natural Language Processing,"HuggingFace's Transformers: State-of-the-art Natural Language Processing

Recent progress in natural language processing has been driven by advances both model architecture and pretraining. Transformer architectures have facilitated building higher-capacity models pretraining made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal opening up these wider machine learning community. The consists carefully engineered state-of-the art under unified API. Backing curated collection pretrained available designed be extensible researchers, simple practitioners, fast robust industrial deployments. at \url{https://github.com/huggingface/transformers}.

computer science, artificial intelligence, language model, natural language processing, machine learning, deep learning, natural language, nlp task",2019,2605,computer science|artificial intelligence|language model|natural language processing|machine learning|deep learning|natural language|nlp task,https://openalex.org/W3011411500
https://openalex.org/W2161070585,Understanding normal and impaired word reading: Computational principles in quasi-regular domains.,"Understanding normal and impaired word reading: Computational principles in quasi-regular domains.

A connectionist approach to processing in quasi-regular domains, as exemplified by English word reading, is developed. Networks using appropriately structured orthographic and phonological representations were trained read both regular exception words, yet also able pronounceable nonwords well skilled readers. mathematical analysis of a simplified system clarifies the close relationship frequency spelling-sound consistency influencing naming latencies. These insights verified subsequent simulations, including an attractor network that accounted for latency data directly its time settle on response. Further analyses ability networks reproduce acquired surface dyslexia support view reading incorporates graded division labor between semantic processes, contrasts important ways with standard dual-route account.

psycholinguistics, neurolinguistics, language science, cognitive impairment, natural language processing, cognitive science, language comprehension, specific learning disorder, impaired word reading, quasi-regular domains, language processing in the brain, linguistics, language, reading research, computational linguistics, computational principles",1996,2604,psycholinguistics|neurolinguistics|language science|cognitive impairment|natural language processing|cognitive science|language comprehension|specific learning disorder|impaired word reading|quasi-regular domains|language processing in the brain|linguistics|language|reading research|computational linguistics|computational principles,https://openalex.org/W2022963108|https://openalex.org/W2011035448|https://openalex.org/W2016534914
https://openalex.org/W2124479173,Three models for the description of language,"Three models for the description of language

We investigate several conceptions of linguistic structure to determine whether or not they can provide simple and ""revealing"" grammars that generate all the sentences English only these. find no finite-state Markov process produces symbols with transition from state serve as an grammar. Furthermore, particular subclass such processes produce <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n</tex> -order statistical approximations do come closer, increasing , matching output formalize-the notions ""phrase structure"" show this gives us a method for describing language which is essentially more powerful, though still representable rather elementary type process. Nevertheless, it successful when limited small subset sentences. study formal properties set grammatical transformations carry phrase into new derived structure, showing transformational are same phrase-structure grammars; grammar materially simplified if description kernel other constructed by repeated transformations; view certain insight use understanding language.

applied linguistics, morphology (linguistics), computer science, linguistics, structural linguistics, cognitive linguistics, language model, natural language processing, language, general linguistics, theoretical linguistics, semantic evaluation, computational linguistics, semantics, machine translation, linguistic theory, language science, language learning",1956,2603,applied linguistics|morphology (linguistics)|computer science|linguistics|structural linguistics|cognitive linguistics|language model|natural language processing|language|general linguistics|theoretical linguistics|semantic evaluation|computational linguistics|semantics|machine translation|linguistic theory|language science|language learning,https://openalex.org/W2158899491|https://openalex.org/W2169818249|https://openalex.org/W2061271742|https://openalex.org/W2054125330
https://openalex.org/W2144578941,Introduction to the CoNLL-2003 shared task,"Introduction to the CoNLL-2003 shared task

We describe the CoNLL-2003 shared task: language-independent named entity recognition. give background information on data sets (English and German) evaluation method, present a general overview of systems that have taken part in task discuss their performance.

distributed system, computer science, inference, distributed problem solving, natural language processing, clustering, cognitive science, automated reasoning, data science, formal verification, coreference resolution, question answering",2003,2558,distributed system|computer science|inference|distributed problem solving|natural language processing|clustering|cognitive science|automated reasoning|data science|formal verification|coreference resolution|question answering,https://openalex.org/W2250539671|https://openalex.org/W2896457183|https://openalex.org/W2962739339|https://openalex.org/W2296283641|https://openalex.org/W2020278455|https://openalex.org/W2963625095|https://openalex.org/W3019166713|https://openalex.org/W86887328|https://openalex.org/W2953356739|https://openalex.org/W3037109418
https://openalex.org/W2150102617,RCV1: A New Benchmark Collection for Text Categorization Research,"RCV1: A New Benchmark Collection for Text Categorization Research

Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use this data on text categorization requires a detailed understanding the real world constraints under which was produced. Drawing interviews with personnel and access to documentation, we describe coding policy quality control procedures used in producing RCV1 data, intended semantics hierarchical category taxonomies, corrections necessary remove errorful data. We refer original as RCV1-v1, corrected RCV1-v2. benchmark several widely supervised learning methods RCV1-v2, illustrating collection's properties, suggesting new directions research, providing baseline results future studies. make detailed, per-category experimental results, well versions assignments taxonomy structures, via online appendices.

pattern recognition, computer science, topic model, text mining, natural language processing, support vector machine, text categorization research, machine learning, document classification, data science, knowledge discovery, deep learning, statistics, machine learning research, automatic classification, classification method",2004,2558,pattern recognition|computer science|topic model|text mining|natural language processing|support vector machine|text categorization research|machine learning|document classification|data science|knowledge discovery|deep learning|statistics|machine learning research|automatic classification|classification method,https://openalex.org/W2158899491|https://openalex.org/W2963012544|https://openalex.org/W2170738476|https://openalex.org/W2161793142
https://openalex.org/W2079145130,Prediction and Entropy of Printed English,"Prediction and Entropy of Printed English

A new method of estimating the entropy and redundancy a language is described. This exploits knowledge statistics possessed by those who speak language, depends on experimental results in prediction next letter when preceding text known. Results experiments are given, some properties an ideal predictor developed.

linguistics, english, text mining, natural language processing, language, entropy, english language, language science, english study",1951,2493,linguistics|english|text mining|natural language processing|language|entropy|english language|language science|english study,https://openalex.org/W2158899491
https://openalex.org/W2115792525,The Berkeley FrameNet Project,"The Berkeley FrameNet Project

FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now its second year (NSF IRI-9618838, ""Tools for Lexicon Building""). The project's key features are (a) commitment to corpus evidence semantic and syntactic generalizations, (b) the representation of valences target words (mostly nouns, adjectives, verbs) which portion makes use frame semantics. resulting database will contain descriptions frames underlying meanings described, valence (semantic syntactic) several thousand phrases, each accompanied by (c) representative collection annotated attestations, jointly exemplify observed linkings between ""frame elements"" their realizations (e.g. grammatical function, phrase type, other traits). This report present goals workflow, information about tools that have been adapted or created in-house this work.

computer science, knowledge discovery, nlp task, text mining, online information, language resource, computational linguistics, linguistics, machine learning, language engineering, semantic framework, language, berkeley framenet project, natural language processing, data-driven learning, natural language generation, computational semantics, language model, semantic web",1998,2481,computer science|knowledge discovery|nlp task|text mining|online information|language resource|computational linguistics|linguistics|machine learning|language engineering|semantic framework|language|berkeley framenet project|natural language processing|data-driven learning|natural language generation|computational semantics|language model|semantic web,https://openalex.org/W2277195237|https://openalex.org/W2252123671|https://openalex.org/W2088622183
https://openalex.org/W2117827905,A 2-tuple fuzzy linguistic representation model for computing with words,"A 2-tuple fuzzy linguistic representation model for computing with words

The fuzzy linguistic approach has been applied successfully to many problems. However, there is a limitation of this imposed by its information representation model and the computation methods used when fusion processes are performed on values. This loss information; implies lack precision in final results from information. In paper, we present tools for overcoming limitation. expressed means 2-tuples, which composed term numeric value assessed (-0.5, 0.5). allows continuous domain, therefore, it can represent any counting obtained aggregation process. We then develop computational technique computing with words without Finally, different classical operators extended deal 2-tuple model.

computer science, artificial intelligence, linguistics, fuzzy logic, language model, natural language processing, language, soft computing, fuzzy modeling, fuzzy pattern recognition, symbolic linguistic representation, fuzzy optimization, computational linguistics, fuzzy computing, deep learning, machine learning research, fuzzy system, fuzzy set",2000,2478,computer science|artificial intelligence|linguistics|fuzzy logic|language model|natural language processing|language|soft computing|fuzzy modeling|fuzzy pattern recognition|symbolic linguistic representation|fuzzy optimization|computational linguistics|fuzzy computing|deep learning|machine learning research|fuzzy system|fuzzy set,https://openalex.org/W1978062397
https://openalex.org/W2161466446,ELIZA — a computer program for the study of natural language communication between man and machine,"ELIZA — a computer program for the study of natural language communication between man and machine

ELIZA is a program operating within the MAC time-sharing system of MIT which makes certain kinds natural language conversation between man and computer possible. Input sentences are analyzed on basis decomposition rules triggered by key words appearing in input text. Responses generated reassembly associated with selected rules. The fundamental technical problems concerned are: (1) identification words, (2) discovery minimal context, (3) choice appropriate transformations, (4) generation responses absence (5) provision an editing capability for “scripts”. A discussion some psychological issues relevant to approach as well future developments concludes paper.

computer science, artificial intelligence, human-computer interaction, human-ai interaction, language model, natural language processing, language, language technology, communication, human-machine interaction, computer program, computational linguistics, machine translation, spoken language technology, natural language communication",1983,2477,computer science|artificial intelligence|human-computer interaction|human-ai interaction|language model|natural language processing|language|language technology|communication|human-machine interaction|computer program|computational linguistics|machine translation|spoken language technology|natural language communication,https://openalex.org/W1591706642|https://openalex.org/W1663984431|https://openalex.org/W3019166713
https://openalex.org/W2019759670,Sentiment analysis algorithms and applications: A survey,"Sentiment analysis algorithms and applications: A survey

Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA the computational treatment opinions, sentiments and subjectivity text. This survey paper tackles a comprehensive overview last update this Many recently proposed algorithms' enhancements various applications are investigated presented briefly survey. These articles categorized according to their contributions techniques. The related fields (transfer learning, emotion detection, building resources) that attracted researchers discussed. main target give nearly full image techniques with brief details. include sophisticated categorizations large number recent illustration trend sentiment analysis its areas.

computer science, linguistics, opinion aggregation, content analysis, disinformation detection, natural language generation, semantic evaluation, data science, content similarity detection, fact checking, data mining, multimodal sentiment analysis, sentiment analysis algorithms, machine learning research, text mining, natural language processing, affective computing, knowledge discovery, information retrieval",2014,2456,computer science|linguistics|opinion aggregation|content analysis|disinformation detection|natural language generation|semantic evaluation|data science|content similarity detection|fact checking|data mining|multimodal sentiment analysis|sentiment analysis algorithms|machine learning research|text mining|natural language processing|affective computing|knowledge discovery|information retrieval,
https://openalex.org/W2001496424,A guided tour to approximate string matching,"A guided tour to approximate string matching

We survey the current techniques to cope with problem of string matching that allows errors. This is becoming a more and relevant issue for many fast growing areas such as information retrieval computational biology. focus on online searching mostly edit distance, explaining its relevance, statistical behavior, history developments, central ideas algorithms their complexities. present number experiments compare performance different show which are best choices. conclude some directions future work open problems.

pattern recognition, computer science, approximation theory, natural language processing, pattern matching, matching technique, machine learning, applied mathematics, similarity search, data science, approximate reasoning, string-searching algorithm, machine learning research, guided tour, data mining, string processing",2001,2436,pattern recognition|computer science|approximation theory|natural language processing|pattern matching|matching technique|machine learning|applied mathematics|similarity search|data science|approximate reasoning|string-searching algorithm|machine learning research|guided tour|data mining|string processing,
https://openalex.org/W182831726,Speech and language processing,"Speech and language processing

is one of the most recognizablecharacters in 20th century cinema. HAL an artiﬁcial agent capable such advancedlanguage behavior as speaking and understanding English, at a crucial moment inthe plot, even reading lips. It now clear that HAL’s creator, Arthur C. Clarke, wasa little optimistic predicting when would be avail-able. But just how far off was he? What it take to create least language-relatedpartsofHAL?WecallprogramslikeHALthatconversewithhumansinnatural

linguistics, psycholinguistics, speech recognition, natural language processing, speech processing, language, speech analysis, speech perception, language processing, spoken language processing, cognitive science, language processing in the brain, speech communication, speech science, speech technology, spoken language technology, language science",2010,2414,linguistics|psycholinguistics|speech recognition|natural language processing|speech processing|language|speech analysis|speech perception|language processing|spoken language processing|cognitive science|language processing in the brain|speech communication|speech science|speech technology|spoken language technology|language science,https://openalex.org/W1574901103|https://openalex.org/W2436001372|https://openalex.org/W1591706642|https://openalex.org/W2169818249
https://openalex.org/W2095655043,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts

Politics and political conflict often occur in the written spoken word. Scholars have long recognized this, but massive costs of analyzing even moderately sized collections texts hindered their use science research. Here lies promise automated text analysis: it substantially reduces large text. We provide a guide to this exciting new area research show how, many instances, methods already obtained part promise. But there are pitfalls using methods—they no substitute for careful thought close reading require extensive problem-specific validation. survey wide range methods, guidance on how validate output models, clarify misconceptions errors literature. To conclude, we argue that become standard tool scientists, methodologists must contribute

political texts, content analysis, political communication, political methodology, natural language processing, text mining, information extraction, political science, communication, data privacy, data science, knowledge discovery, political analysis, document analysis",2013,2382,political texts|content analysis|political communication|political methodology|natural language processing|text mining|information extraction|political science|communication|data privacy|data science|knowledge discovery|political analysis|document analysis,
https://openalex.org/W2101210369,Unsupervised word sense disambiguation rivaling supervised methods,"Unsupervised word sense disambiguation rivaling supervised methods

This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The is based two powerful constraints---that words tend to have one per discourse and collocation---exploited in iterative bootstrapping procedure. Tested accuracy exceeds 96%.

computer science, linguistics, word-sense disambiguation, text mining, natural language processing, language, unsupervised machine learning, machine learning, semantic evaluation, computational linguistics, knowledge discovery, computational intelligence, deep learning, semantic processing, machine learning research, entity disambiguation, distributional semantics",1995,2367,computer science|linguistics|word-sense disambiguation|text mining|natural language processing|language|unsupervised machine learning|machine learning|semantic evaluation|computational linguistics|knowledge discovery|computational intelligence|deep learning|semantic processing|machine learning research|entity disambiguation|distributional semantics,https://openalex.org/W2436001372|https://openalex.org/W3101913037|https://openalex.org/W1916559533|https://openalex.org/W2153848201|https://openalex.org/W2130337399|https://openalex.org/W2340954483
https://openalex.org/W2964110616,Transformer-XL: Attentive Language Models beyond a Fixed-Length Context,"Transformer-XL: Attentive Language Models beyond a Fixed-Length Context

Transformers have a potential of learning longer-term dependency, but are limited by fixed-length context in the setting language modeling. We propose novel neural architecture Transformer-XL that enables dependency beyond fixed length without disrupting temporal coherence. It consists segment-level recurrence mechanism and positional encoding scheme. Our method not only capturing also resolves fragmentation problem. As result, learns is 80% longer than RNNs 450% vanilla Transformers, achieves better performance on both short long sequences, up to 1,800+ times faster during evaluation. Notably, we improve state-of-the-art results bpc/perplexity 0.99 enwiki8, 1.08 text8, 18.3 WikiText-103, 21.8 One Billion Word, 54.5 Penn Treebank (without finetuning). When trained manages generate reasonably coherent, text articles with thousands tokens. code, pretrained models, hyperparameters available Tensorflow PyTorch.

attentive language models, language, natural language processing, fixed-length context, language model",2019,2365,attentive language models|language|natural language processing|fixed-length context|language model,https://openalex.org/W2979826702|https://openalex.org/W3170841864|https://openalex.org/W2970476646
https://openalex.org/W2963226019,InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets,"InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets

This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able learn disentangled representations in a completely unsupervised manner. InfoGAN generative adversarial network also maximizes mutual information between small subset of latent variables and observation. We derive lower bound objective can be optimized efficiently, show our training procedure interpreted as variation Wake-Sleep algorithm. Specifically, successfully disentangles writing styles from digit shapes on MNIST dataset, pose lighting 3D rendered images, background digits central SVHN dataset. It discovers visual concepts include hair styles, presence/absence eyeglasses, emotions CelebA face Experiments learns interpretable are competitive with learned by existing fully supervised methods.

generative ai, information fusion, knowledge representation and reasoning, natural language processing, neural computation, generative model, interpretable representation learning, cognitive science, computational intelligence, computer science, feature learning, scene interpretation, machine learning, generative adversarial nets, machine learning research, generative adversarial network, deep learning, data science",2016,2340,generative ai|information fusion|knowledge representation and reasoning|natural language processing|neural computation|generative model|interpretable representation learning|cognitive science|computational intelligence|computer science|feature learning|scene interpretation|machine learning|generative adversarial nets|machine learning research|generative adversarial network|deep learning|data science,https://openalex.org/W2962770929|https://openalex.org/W2963847595
https://openalex.org/W1981307601,The Cambridge Encyclopedia of Language,"The Cambridge Encyclopedia of Language

1 Popular ideas about language 2 Language and identity 3 The structure of 4 medium language: speaking listening 5 reading writing 6 signing seeing 7 Child acquisition 8 Language, brain handicap 9 languages the world 10 in 11 communication Appendices Glossary Special symbols abbreviations used Encyclopedia Table world's Further References Index langauges, families, dialects scripts authors personalities topics

poetics, linguistics, english language, language, syntax, lexical semantics, cambridge encyclopedia, lexicography, lexicon, linguistic theory, language science, multilingualism, philosophy of language, general linguistics, language learning, applied linguistics, english, natural language processing, semantics",1992,2302,poetics|linguistics|english language|language|syntax|lexical semantics|cambridge encyclopedia|lexicography|lexicon|linguistic theory|language science|multilingualism|philosophy of language|general linguistics|language learning|applied linguistics|english|natural language processing|semantics,
https://openalex.org/W2963250244,SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing

This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ Python implementations units. While existing segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train models directly from raw sentences, which allows us to make purely end-to-end language independent system. We perform validation experiment of NMT on English-Japanese machine translation, find it possible achieve comparable accuracy direct training sentences. also compare performance with various configurations. available under Apache 2 license at https://github.com/google/sentencepiece.

spoken language processing, nlp task, natural language processing, machine translation, neural text processing, text processing, text recognition, computer science, language engineering, language model, machine learning, text mining, linguistics, language, computational linguistics",2018,2297,spoken language processing|nlp task|natural language processing|machine translation|neural text processing|text processing|text recognition|computer science|language engineering|language model|machine learning|text mining|linguistics|language|computational linguistics,https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W3169483174
https://openalex.org/W2884001105,Recent Trends in Deep Learning Based Natural Language Processing [Review Article],"Recent Trends in Deep Learning Based Natural Language Processing [Review Article]

Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety model designs blossomed the context natural language (NLP). In this paper, we review significant deep related models that been employed for numerous NLP tasks provide walk-through their evolution. We also summarize, compare contrast various put forward detailed understanding past, present future NLP.

computer science, artificial intelligence, linguistics, language model, natural language processing, language, machine learning, deep learning, nlp task",2018,2297,computer science|artificial intelligence|linguistics|language model|natural language processing|language|machine learning|deep learning|nlp task,
https://openalex.org/W2963216553,Improving Neural Machine Translation Models with Monolingual Data,"Improving Neural Machine Translation Models with Monolingual Data

Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data training.Targetside monolingual plays an important role in boosting fluency phrasebased statistical machine translation, and we investigate the use of NMT.In contrast to previous work, which combines NMT models with separately trained models, note that encoder-decoder architectures already have capacity learn same information as a model, explore strategies train without changing neural network architecture.By pairing training automatic backtranslation, can treat it additional data, obtain substantial improvements on WMT 15 task English↔German (+2.8-3.7 BLEU), low-resourced IWSLT 14 Turkish→English (+2.1-3.4BLEU), obtaining new state-of-the-art results.We also show fine-tuning in-domain gives English→German.

computer science, linguistics, language model, natural language processing, language, neural machine translation, monolingual data, computational linguistics, nlp task, machine translation, translation studies",2016,2291,computer science|linguistics|language model|natural language processing|language|neural machine translation|monolingual data|computational linguistics|nlp task|machine translation|translation studies,https://openalex.org/W2963026768|https://openalex.org/W3030163527|https://openalex.org/W3185341429
https://openalex.org/W2162471372,Natural language and natural selection,"Natural language and natural selection

Abstract Many people have argued that the evolution of human language faculty cannot be explained by Darwinian natural selection. Chomsky and Gould suggested may evolved as by-product selection for other abilities or a consequence as-yet unknown laws growth form. Others biological specialization grammar is incompatible with every tenet theory – it shows no genetic variation, could not exist in any intermediate forms, confers selective advantage, would require more evolutionary time genomic space than available. We examine these arguments show they depend on inaccurate assumptions about biology both. Evolutionary offers clear criteria when trait should attributed to selection: complex design some function, absence alternative processes capable explaining such complexity. Human meets criteria: Grammar mechanism tailored transmission propositional structures through serial interface. Autonomous arbitrary grammatical phenomena been offered counterexamples position an adaptation, but this reasoning unsound: Communication protocols conventions are adaptive long shared. Consequently, acquisition child systematically differ from species, attempts analogize them misleading. Reviewing data, we conclude there reason believe conventional neo-Darwinian process.

linguistics, psycholinguistics, natural language processing, language, natural language generation, evolutionary biology, evolutionary intelligence, language evolution, computational linguistics, semantics, language symbiosis, natural language, language science, natural selection, multilingualism",1990,2283,linguistics|psycholinguistics|natural language processing|language|natural language generation|evolutionary biology|evolutionary intelligence|language evolution|computational linguistics|semantics|language symbiosis|natural language|language science|natural selection|multilingualism,
https://openalex.org/W3042011474,CSPNet: A New Backbone that can Enhance Learning Capability of CNN,"CSPNet: A New Backbone that can Enhance Learning Capability of CNN

Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, success greatly relies costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) mitigate problem that previous works require heavy inference computations network architecture perspective. We attribute duplicate gradient information within optimization. The proposed respect variability of gradients by integrating feature maps beginning and end a stage, which, in our experiments, reduces 20% equivalent or even superior accuracy ImageNet dataset, significantly outperforms terms AP <inf xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">50</inf> MS COCO detection dataset. CSPNet is easy implement general enough cope architectures based ResNet, ResNeXt, DenseNet.

adversarial machine learning, computer science, convolutional neural network, online learning, natural language processing, learning capability, machine learning, sparse neural network, cognitive science, digital learning, data science, neural computation, deep learning, computational intelligence, machine learning research, neural network (machine learning)",2020,2276,adversarial machine learning|computer science|convolutional neural network|online learning|natural language processing|learning capability|machine learning|sparse neural network|cognitive science|digital learning|data science|neural computation|deep learning|computational intelligence|machine learning research|neural network (machine learning),
https://openalex.org/W1544827683,Teaching Machines to Read and Comprehend,"Teaching Machines to Read and Comprehend

Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability answer questions posed the contents of that they have seen, but until now large scale training and test datasets been missing for this type evaluation. In work we define a new methodology resolves bottleneck provides supervised comprehension data. This allows us develop class attention based deep neural networks learn real complex with minimal prior knowledge structure.

language comprehension, language acquisition, computer science, machine learning, machine-readable representation, machine translation, artificial intelligence, language, vision language model, natural language processing, neural machine translation, reading research, deep learning, machine vision, cognitive science, language learning",2015,2244,language comprehension|language acquisition|computer science|machine learning|machine-readable representation|machine translation|artificial intelligence|language|vision language model|natural language processing|neural machine translation|reading research|deep learning|machine vision|cognitive science|language learning,https://openalex.org/W2963748441|https://openalex.org/W3034999214|https://openalex.org/W2606974598|https://openalex.org/W2963929190|https://openalex.org/W2562607067|https://openalex.org/W2517194566|https://openalex.org/W2970419734|https://openalex.org/W2963339397|https://openalex.org/W2888482885
https://openalex.org/W2020278455,A survey of named entity recognition and classification,"A survey of named entity recognition and classification

This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains textual genres studied literature. From start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These surveyed along with other critical aspects such as features evaluation methods. Features word-level, dictionary-level corpus-level representations words a document. Evaluation techniques, ranging intuitive exact match very complex matching adjustable cost errors, an indisputable key progress.

pattern recognition, computer science, text mining, natural language processing, entity recognition, machine learning, cognitive science, document classification, data science, knowledge discovery, deep learning, entity summarization, machine learning research, entity disambiguation, knowledge extraction, named-entity recognition",2007,2230,pattern recognition|computer science|text mining|natural language processing|entity recognition|machine learning|cognitive science|document classification|data science|knowledge discovery|deep learning|entity summarization|machine learning research|entity disambiguation|knowledge extraction|named-entity recognition,https://openalex.org/W2964167098
https://openalex.org/W2489487449,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)

Welcome to EMNLP 2020!Due the unprecedented situation with Covid-19 pandemic, 2020 will be held completely online this year.We decided move a virtual format early on, when pandemic just started, and postponed paper submission deadlines by three weeks such that authors affected could have more time for their submissions.This resulted in much tighter schedule review decisions, as well publications, workshop programs, infrastructure, etc.However, thanks everyone's hard work, we made it.We received record number of 3,677 is significant increase 26% over 2019, making it largest NLP conference date terms submissions.After removing withdrawals desk rejecting papers which violated our formatting requirements, anonymity policy, or double 3,359 submissions were sent out review.Despite sharp submissions, kept acceptance rates at similar level past years.

empirical methods, computer science, natural language processing, machine learning, nlp task",2020,2230,empirical methods|computer science|natural language processing|machine learning|nlp task,https://openalex.org/W2436001372
https://openalex.org/W2962965405,A Neural Attention Model for Abstractive Sentence Summarization,"A Neural Attention Model for Abstractive Sentence Summarization

Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build.In this work, we propose a fully data-driven approach sentence summarization.Our method utilizes local attention-based model that generates each word of the summary conditioned input sentence.While structurally simple, it can easily be trained end-to-end and scales large amount training data.The shows significant performance gains DUC-2004 shared task compared with several strong baselines.

machine learning, computer science, semantic processing, natural language processing, abstractive sentence summarization, neural attention model, cognitive science, neural computation",2015,2222,machine learning|computer science|semantic processing|natural language processing|abstractive sentence summarization|neural attention model|cognitive science|neural computation,https://openalex.org/W2606974598|https://openalex.org/W2963250244|https://openalex.org/W2884001105|https://openalex.org/W2963929190|https://openalex.org/W2562607067|https://openalex.org/W2970419734|https://openalex.org/W3019166713|https://openalex.org/W3098605233|https://openalex.org/W2888482885|https://openalex.org/W2963096510
https://openalex.org/W1990689692,Constructing inferences during narrative text comprehension.,"Constructing inferences during narrative text comprehension.

The authors describe a constructionist theory that accounts for the knowledge-based inferences are constructed when readers comprehend narrative text. Readers potentially generate rich variety of they construct referential situation model what text is about. proposed specifies some, but not all, this information under most conditions comprehension. distinctive assumptions embrace principle search (or effort) after meaning. According to principle, attempt meaning representation addresses reader's goals, coherent at both local and global levels, explains why actions, events, states mentioned in This study reviews empirical evidence contrasts it with alternative theoretical frameworks.

discourse structure, psycholinguistics, narrative extraction, narrative text comprehension, natural language processing, narrative representation, discourse analysis, narrative, inference, cognitive science, interpretation, language comprehension, semantic interpretation, linguistics, language, reading research",1994,2214,discourse structure|psycholinguistics|narrative extraction|narrative text comprehension|natural language processing|narrative representation|discourse analysis|narrative|inference|cognitive science|interpretation|language comprehension|semantic interpretation|linguistics|language|reading research,
https://openalex.org/W2072644219,Dynamic topic models,"Dynamic topic models

A family of probabilistic time series models is developed to analyze the evolution topics in large document collections. The approach use state space on natural parameters multinomial distributions that represent topics. Variational approximations based Kalman filters and nonparametric wavelet regression are carry out approximate posterior inference over latent In addition giving quantitative, predictive a sequential corpus, dynamic topic provide qualitative window into contents collection. demonstrated by analyzing OCR'ed archives journal Science from 1880 through 2000.

computer science, disinformation detection, topic model, text mining, natural language processing, dynamic topic models, machine learning, data science, knowledge discovery, machine learning research, information retrieval",2006,2205,computer science|disinformation detection|topic model|text mining|natural language processing|dynamic topic models|machine learning|data science|knowledge discovery|machine learning research|information retrieval,https://openalex.org/W2174706414
https://openalex.org/W2737258237,Scene Parsing through ADE20K Dataset,"Scene Parsing through ADE20K Dataset

Scene parsing, or recognizing and segmenting objects stuff in an image, is one of the key problems computer vision. Despite communitys efforts data collection, there are still few image datasets covering a wide range scenes object categories with dense detailed annotations for scene parsing. In this paper, we introduce analyze ADE20K dataset, spanning diverse scenes, objects, parts some cases even parts. A parsing benchmark built upon 150 classes included. Several segmentation baseline models evaluated on benchmark. novel network design called Cascade Segmentation Module proposed to parse into stuff, cascade improve over baselines. We further show that trained networks can lead applications such as content removal synthesis <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> .

image analysis, pattern recognition, computer science, computational imaging, information fusion, natural language processing, scene interpretation, scene understanding, video understanding, computer vision, scene analysis, multimedia retrieval, data science, systems engineering, deep learning, localization, large-scale datasets, machine vision",2017,2161,image analysis|pattern recognition|computer science|computational imaging|information fusion|natural language processing|scene interpretation|scene understanding|video understanding|computer vision|scene analysis|multimedia retrieval|data science|systems engineering|deep learning|localization|large-scale datasets|machine vision,
https://openalex.org/W2112420033,A contextual-bandit approach to personalized news article recommendation,"A contextual-bandit approach to personalized news article recommendation

Personalized web services strive to adapt their (advertisements, news articles, etc) individual users by making use of both content and user information. Despite a few recent advances, this problem remains challenging for at least two reasons. First, service is featured with dynamically changing pools content, rendering traditional collaborative filtering methods inapplicable. Second, the scale most practical interest calls solutions that are fast in learning computation. In work, we model personalized recommendation articles as contextual bandit problem, principled approach which algorithm sequentially selects serve based on information about while simultaneously adapting its article-selection strategy user-click feedback maximize total clicks. The contributions work three-fold. propose new, general computationally efficient well motivated from theory. argue any can be reliably evaluated offline using previously recorded random traffic. Finally, evaluation method, successfully applied our new Yahoo! Front Page Today Module dataset containing over 33 million events. Results showed 12.5% click lift compared standard context-free algorithm, advantage becomes even greater when data gets more scarce.

exploration v exploitation, text mining, natural language processing, personalized search, news analytics, machine learning, recommender system, predictive analytics, cognitive science, relevance feedback, data science, knowledge discovery, journalism, interactive information retrieval, information retrieval, online information",2010,2152,exploration v exploitation|text mining|natural language processing|personalized search|news analytics|machine learning|recommender system|predictive analytics|cognitive science|relevance feedback|data science|knowledge discovery|journalism|interactive information retrieval|information retrieval|online information,
https://openalex.org/W1559870885,The B-book: assigning programs to meanings,"The B-book: assigning programs to meanings

The B Method is a means for specifying, designing and coding software systems. long-awaited B-Book the standard reference everything concerning this method. It contains mathematical basis on which it founded, precise definitions of notations used, large number examples illustrating its use in practice. J.-R. Abrial, inventor B, has written book such way that can be used self-study or reference. four parts, first dealing with foundations definition various structures are needed to formalise systems; special emphasis placed notion proof. second part presentation Generalised Substitution Language Abstract Machine Notation; given show how specifications constructed systematically. next introduces two basic programming features sequencing loop. last covers very important refinement. shows construct systems by layered architectures modules. With appearance B-Book, formal methods practitioners, computer scientists, developers at will have access definitive account what become one approaches construction

affective computing, automated reasoning, programming language, natural language processing, assigning programs, program derivation, semantics, semantic web, language, program comprehension, computational semantics, linguistics, program management, semantic representation, program analysis, program evaluation, interpretation, semantic evaluation, communication",1997,2146,affective computing|automated reasoning|programming language|natural language processing|assigning programs|program derivation|semantics|semantic web|language|program comprehension|computational semantics|linguistics|program management|semantic representation|program analysis|program evaluation|interpretation|semantic evaluation|communication,
https://openalex.org/W2963310665,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding

Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task struggle with out-of-domain data. If we aspire develop understanding beyond detection of superficial correspondences between inputs outputs, then it critical unified model that can execute range linguistic tasks across different domains. To facilitate research in this direction, present General Language Understanding Evaluation (GLUE, gluebenchmark.com): benchmark nine diverse tasks, an auxiliary dataset probing phenomena, online platform evaluating comparing models. For some training data plentiful, but others limited or does not match genre test set. GLUE thus favors represent knowledge way facilitates sample-efficient learning effective knowledge-transfer tasks. While none datasets were created from scratch benchmark, four them feature privately-held data, which used ensure fairly. We evaluate baselines use ELMo (Peters et al., 2018), powerful transfer technique, as well state-of-the-art sentence representation The best still achieve fairly low absolute scores. Analysis our diagnostic yields similarly weak performance over all phenomena tested, exceptions.

natural language understanding, nlp task, computer science, language model, deep learning, multi-task benchmark, information fusion, natural language processing, machine translation, language testing, multi-task learning, language learning, machine learning, language engineering, data science, computational linguistics, analysis platform, question answering, cognitive science, semantic evaluation, large language model",2018,2102,natural language understanding|nlp task|computer science|language model|deep learning|multi-task benchmark|information fusion|natural language processing|machine translation|language testing|multi-task learning|language learning|machine learning|language engineering|data science|computational linguistics|analysis platform|question answering|cognitive science|semantic evaluation|large language model,https://openalex.org/W2896457183|https://openalex.org/W3034999214|https://openalex.org/W2996428491|https://openalex.org/W3035390927|https://openalex.org/W2980282514|https://openalex.org/W2914120296|https://openalex.org/W3034238904|https://openalex.org/W3098824823|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2948947170
https://openalex.org/W2346578806,"<scp>BORIS</scp>: a free, versatile open‐source event‐logging software for video/audio coding and live observations","<scp>BORIS</scp>: a free, versatile open‐source event‐logging software for video/audio coding and live observations

Summary Quantitative aspects of the study animal and human behaviour are increasingly relevant to test hypotheses find empirical support for them. At same time, photo video cameras can store a large number recordings often used monitor subjects remotely. Researchers frequently face need code considerable quantities with relatively flexible software, constrained by species‐specific options or exact settings. BORIS is free, open‐source multiplatform standalone program that allows user‐specific coding environment be set computer‐based review previously recorded videos live observations. Being open settings, project‐based ethogram defined then shared collaborators, imported modified. Projects created in include list observations, each observation may one two (e.g. simultaneous screening visual stimuli subject being tested; from different sides an aquarium). Once user has ethogram, including state point events both, performed using assigned keys on computer keyboard. definition unlimited (states/point events) subjects. process completed, extract time‐budget single grouped observations automatically present at‐a‐glance summary main behavioural features. The data analysis exported many common formats ( TSV , CSV ODF XLS SQL JSON ). observed plotted various graphic SVG PNG JPG TIFF EPS PDF

computer science, machine learning, mobile computing, information fusion, event-driven monitoring, digital signal processing, complex event processing, live observations, communication, data science, event camera, event processing, multimedia information processing, video observation, multimedia computing, natural language processing, computer vision, clustering, multimedia retrieval",2016,2098,computer science|machine learning|mobile computing|information fusion|event-driven monitoring|digital signal processing|complex event processing|live observations|communication|data science|event camera|event processing|multimedia information processing|video observation|multimedia computing|natural language processing|computer vision|clustering|multimedia retrieval,
https://openalex.org/W2893425640,Semantics derived automatically from language corpora contain human-like biases,"Semantics derived automatically from language corpora contain human-like biases

Artificial intelligence and machine learning are in a period of astounding growth. However, there concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice unfairness unfortunately characterizes many human institutions. Here we show for first time human-like semantic biases result from application standard ordinary language---the same sort language humans exposed every day. We replicate spectrum as by Implicit Association Test other well-known psychological studies. using widely purely statistical machine-learning model---namely, GloVe word embedding---trained on corpus text Web. Our results indicate itself contains recoverable accurate imprints our historic biases, whether morally neutral towards insects flowers, problematic race gender, even simply veridical, reflecting {\em status quo} distribution gender respect careers names. These regularities captured along rest semantics. In addition empirical findings concerning language, also contribute new methods evaluating bias text, Word Embedding (WEAT) Factual (WEFAT). have implications not only AI learning, but fields psychology, sociology, ethics, since they raise possibility mere exposure everyday can account here.

linguistics, natural language processing, semantic analysis (linguistics), language, human-like biases, computational linguistics, semantics, computational semantics, corpus linguistics",2017,2093,linguistics|natural language processing|semantic analysis (linguistics)|language|human-like biases|computational linguistics|semantics|computational semantics|corpus linguistics,https://openalex.org/W2963847595|https://openalex.org/W2794557536|https://openalex.org/W2891177506
https://openalex.org/W1978062397,Hesitant Fuzzy Linguistic Term Sets for Decision Making,"Hesitant Fuzzy Linguistic Term Sets for Decision Making

Dealing with uncertainty is always a challenging problem, and different tools have been proposed to deal it. Recently, new model that based on hesitant fuzzy sets has presented manage situations in which experts hesitate between several values assess an indicator, alternative, variable, etc. Hesitant suit the modeling of quantitative settings; however, similar may occur qualitative settings so think possible linguistic or richer expressions than single term for In this paper, concept set introduced provide computational basis increase richness elicitation approach use context-free grammars by using comparative terms. Then, multicriteria decision-making their assessments eliciting expressions. This decision manages such means its representation sets.

linguistics, psycholinguistics, fuzzy logic, decision science, natural language processing, decision making, fuzzy multi-criteria decision-making, fuzzy optimization, cognitive science, fuzzy computing, fuzzy expert system, fuzzy mathematics, human decision process, fuzzy system, fuzzy set",2012,2078,linguistics|psycholinguistics|fuzzy logic|decision science|natural language processing|decision making|fuzzy multi-criteria decision-making|fuzzy optimization|cognitive science|fuzzy computing|fuzzy expert system|fuzzy mathematics|human decision process|fuzzy system|fuzzy set,
https://openalex.org/W1659833910,Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language,"Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language

This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion shared information content. Experimental evaluation against benchmark set human judgments demonstrates that performs better than traditional edge-counting approach. The algorithms take advantage taxonomic resolving syntactic and ambiguity, along with experimental results demonstrating their effectiveness.

linguistics, natural language processing, semantic similarity, semantics, information-based measure",1999,2066,linguistics|natural language processing|semantic similarity|semantics|information-based measure,https://openalex.org/W2120779048
https://openalex.org/W2005876975,Online dictionary learning for sparse coding,"Online dictionary learning for sparse coding

Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the set, also called dictionary, to adapt it specific data, an approach that has recently proven be very effective for reconstruction classification audio image processing domains. proposes a new online optimization algorithm dictionary based stochastic approximations, which scales up gracefully large datasets with millions training samples. A proof convergence is presented, along experiments natural images demonstrating leads faster performance better dictionaries than classical batch algorithms both small datasets.

computer science, sparse coding, online dictionary learning, natural language processing",2009,2040,computer science|sparse coding|online dictionary learning|natural language processing,
https://openalex.org/W2040467972,CROWDSOURCING A WORD–EMOTION ASSOCIATION LEXICON,"CROWDSOURCING A WORD–EMOTION ASSOCIATION LEXICON

Even though considerable attention has been given to the polarity of words (positive and negative) creation large lexicons, research in emotion analysis had rely on limited small lexicons. In this paper, we show how combined strength wisdom crowds can be used generate a large, high‐quality, word–emotion word–polarity association lexicon quickly inexpensively. We enumerate challenges annotation crowdsourcing scenario propose solutions address them. Most notably, addition questions about emotions associated with terms, inclusion word choice question discourage malicious data entry, help identify instances where annotator may not familiar target term (allowing us reject such annotations), obtain annotations at sense level (rather than level). conducted experiments formulate emotion‐annotation questions, that asking if is an leads markedly higher interannotator agreement obtained by evokes emotion.

computer science, linguistics, emotion recognition, text mining, natural language processing, affective computing, data science, knowledge discovery, crowdsourcing, lexicon, machine learning research, social media, emotion, data mining",2012,2022,computer science|linguistics|emotion recognition|text mining|natural language processing|affective computing|data science|knowledge discovery|crowdsourcing|lexicon|machine learning research|social media|emotion|data mining,
https://openalex.org/W2100659887,A database for handwritten text recognition research,"A database for handwritten text recognition research

An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, state 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each was scanned from mail in a working post office at 300 pixels/in 8-bit gray scale on high-quality flat bed digitizer. The data were unconstrained the writer, style, method preparation. These characteristics help overcome limitations earlier databases that contained only isolated or prepared laboratory setting under prescribed circumstances. Also, divided into explicit training testing sets to facilitate sharing results among researchers as well performance comparisons.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

pattern recognition, computer science, text mining, language model, natural language processing, character recognition, text recognition, reading research, cognitive science, data science, knowledge discovery, image representation, machine learning research, database, document processing, information retrieval, writer identification, handwriting",1994,2001,pattern recognition|computer science|text mining|language model|natural language processing|character recognition|text recognition|reading research|cognitive science|data science|knowledge discovery|image representation|machine learning research|database|document processing|information retrieval|writer identification|handwriting,
https://openalex.org/W2120779048,Computing semantic relatedness using Wikipedia-based explicit semantic analysis,"Computing semantic relatedness using Wikipedia-based explicit semantic analysis

Computing semantic relatedness of natural language texts requires access to vast amounts common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning in high-dimensional space concepts derived from Wikipedia. use machine learning techniques explicitly represent any text as weighted vector Wikipedia-based concepts. Assessing this comparing corresponding vectors using conventional metrics (e.g., cosine). Compared with previous state art, ESA results substantial improvements correlation computed scores human judgments: r = 0.56 0.75 for individual words 0.60 0.72 texts. Importantly, due concepts, model is easy explain users.

semantic representation, computer science, knowledge representation and reasoning, knowledge graph, semantic web data, natural language processing, semantic relatedness, semantic approach, semantics, data science, knowledge discovery, semantic interpretation, semantic web, information retrieval, semantic wiki",2007,1991,semantic representation|computer science|knowledge representation and reasoning|knowledge graph|semantic web data|natural language processing|semantic relatedness|semantic approach|semantics|data science|knowledge discovery|semantic interpretation|semantic web|information retrieval|semantic wiki,https://openalex.org/W2185175083|https://openalex.org/W86887328
https://openalex.org/W2327501763,"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition","Listen, attend and spell: A neural network for large vocabulary conversational speech recognition

We present Listen, Attend and Spell (LAS), a neural speech recognizer that transcribes utterances directly to characters without pronunciation models, HMMs or other components of traditional recognizers. In LAS, the network architecture subsumes acoustic, language models making it not only an end-to-end trained system but model. contrast DNN-HMM, CTC most LAS makes no independence assumptions about probability distribution output character sequences given acoustic sequence. Our has two components: listener speller. The is pyramidal recurrent encoder accepts filter bank spectra as inputs. speller attention-based decoder emits each conditioned on all previous characters, entire On Google voice search task, achieves WER 14.1% dictionary external model 10.3% with rescoring over top 32 beams. comparison, state-of-the-art CLDNN-HMM 8.0% same set.

linguistics, speech recognition, large language model, natural language processing, speech processing, language, spoken dialog system, voice recognition, spoken language processing, machine learning, communication, spoken language technology, conversation analysis, neural network, speech communication, neural network (machine learning), language learning, speech corpus",2016,1989,linguistics|speech recognition|large language model|natural language processing|speech processing|language|spoken dialog system|voice recognition|spoken language processing|machine learning|communication|spoken language technology|conversation analysis|neural network|speech communication|neural network (machine learning)|language learning|speech corpus,
https://openalex.org/W2964268978,SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,"SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient

As a new way of training generative models, Generative Adversarial Net (GAN) that uses discriminative model to guide the has enjoyed considerable success in generating real-valued data. However, it limitations when goal is for sequences discrete tokens. A major reason lies outputs from make difficult pass gradient update model. Also, can only assess complete sequence, while partially generated non-trivial balance its current score and future one once entire sequence been generated. In this paper, we propose generation framework, called SeqGAN, solve problems. Modeling data generator as stochastic policy reinforcement learning (RL), SeqGAN bypasses differentiation problem by directly performing update. The RL reward signal comes GAN discriminator judged on passed back intermediate state-action steps using Monte Carlo search. Extensive experiments synthetic real-world tasks demonstrate significant improvements over strong baselines.

adversarial machine learning, computer science, natural language processing, policy gradient, generative adversarial network, machine learning, generative model, generative ai, data science, reinforcement learning, deep learning, knowledge discovery, machine learning research, sequence modelling, neural network (machine learning)",2017,1977,adversarial machine learning|computer science|natural language processing|policy gradient|generative adversarial network|machine learning|generative model|generative ai|data science|reinforcement learning|deep learning|knowledge discovery|machine learning research|sequence modelling|neural network (machine learning),https://openalex.org/W2884001105|https://openalex.org/W2742947407|https://openalex.org/W2938704169
https://openalex.org/W2163302275,"Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification","Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification

Automatic sentiment classification has been extensively studied and applied in recent years. However, is expressed differently different domains, annotating corpora for every possible domain of interest impractical. We investigate adaptation classifiers, focusing on online reviews types products. First, we extend to the recently-proposed structural correspondence learning (SCL) algorithm, reducing relative error due between domains by an average 30% over original SCL algorithm 46% a supervised baseline. Second, identify measure similarity that correlates well with potential classifier from one another. This could instance be used select small set annotate whose trained classifiers would transfer many other domains.

image analysis, computer science, linguistics, machine vision, text mining, natural language processing, disinformation detection, domain adaptation, supervised learning, transfer learning, machine learning, recommender system, film, knowledge discovery, content similarity detection, machine learning research, information retrieval, sentiment classification",2007,1955,image analysis|computer science|linguistics|machine vision|text mining|natural language processing|disinformation detection|domain adaptation|supervised learning|transfer learning|machine learning|recommender system|film|knowledge discovery|content similarity detection|machine learning research|information retrieval|sentiment classification,https://openalex.org/W2108646579|https://openalex.org/W2963026768|https://openalex.org/W2084046180|https://openalex.org/W2126581182|https://openalex.org/W22861983|https://openalex.org/W2250879510|https://openalex.org/W2108420397
https://openalex.org/W3082274269,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

Transfer learning, where a model is first pre-trained on data-rich task before being fine-tuned downstream task, has emerged as powerful technique in natural language processing (NLP). The effectiveness of transfer learning given rise to diversity approaches, methodology, and practice. In this paper, we explore the landscape techniques for NLP by introducing unified framework that converts all text-based problems into text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, other factors dozens understanding tasks. By combining insights from our exploration with scale new ``Colossal Clean Crawled Corpus'', achieve state-of-the-art results many benchmarks covering summarization, question answering, text classification, more. To facilitate future work NLP, release set, models, code.

computer science, transfer learning, unified text-to-text transformer, natural language processing, machine learning, text processing",2020,1953,computer science|transfer learning|unified text-to-text transformer|natural language processing|machine learning|text processing,https://openalex.org/W3034999214|https://openalex.org/W3035390927|https://openalex.org/W3030163527|https://openalex.org/W3099700870|https://openalex.org/W3034238904|https://openalex.org/W3098605233|https://openalex.org/W3169483174
https://openalex.org/W2158139315,Word Representations: A Simple and General Method for Semi-Supervised Learning,"Word Representations: A Simple and General Method for Semi-Supervised Learning

If we take an existing supervised NLP system, a simple and general way to improve accuracy is use unsupervised word representations as extra features. We evaluate Brown clusters, Collobert Weston (2008) embeddings, HLBL (Mnih & Hinton, 2009) embeddings of words on both NER chunking. near state-of-the-art baselines, find that each the three improves these baselines. further improvements by combining different representations. You can download our features, for off-the-shelf in systems, well code, here: http://metaoptimize.com/projects/wordreprs/

semantic representation, computer science, linguistics, text mining, natural language processing, language, machine learning, computational linguistics, deep learning, nlp task, machine learning research, semi-supervised learning, distributional semantics, word representations, general method, language learning",2010,1948,semantic representation|computer science|linguistics|text mining|natural language processing|language|machine learning|computational linguistics|deep learning|nlp task|machine learning research|semi-supervised learning|distributional semantics|word representations|general method|language learning,https://openalex.org/W2250539671|https://openalex.org/W2896457183|https://openalex.org/W2153579005|https://openalex.org/W2950577311|https://openalex.org/W2962739339|https://openalex.org/W2950133940|https://openalex.org/W2158899491|https://openalex.org/W2949547296|https://openalex.org/W2296283641|https://openalex.org/W2113459411|https://openalex.org/W2120615054|https://openalex.org/W2131744502|https://openalex.org/W2141599568|https://openalex.org/W2265846598|https://openalex.org/W2963625095|https://openalex.org/W2787560479|https://openalex.org/W2517194566|https://openalex.org/W2125031621|https://openalex.org/W658020064|https://openalex.org/W2126725946|https://openalex.org/W2251803266|https://openalex.org/W2153848201|https://openalex.org/W71795751|https://openalex.org/W2250879510|https://openalex.org/W2953356739
https://openalex.org/W2963929190,Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond,"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond

In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora.We propose several novel models address critical problems in are not adequately modeled by the basic architecture, such as modeling key-words, capturing hierarchy of sentence-toword structure, emitting words rare or unseen at training time.Our work shows many our proposed contribute to further improvement performance.We also a new dataset consisting multi-sentence summaries, establish benchmarks for research.

abstractive text summarization, automatic summarization, knowledge discovery, automatic classification, natural language processing, sequence-to-sequence rnns, computer science, recurrent neural network, language model, machine learning, machine learning research, text mining, deep learning, data science, sequence modelling",2016,1940,abstractive text summarization|automatic summarization|knowledge discovery|automatic classification|natural language processing|sequence-to-sequence rnns|computer science|recurrent neural network|language model|machine learning|machine learning research|text mining|deep learning|data science|sequence modelling,https://openalex.org/W2606974598|https://openalex.org/W2613904329|https://openalex.org/W2970419734|https://openalex.org/W2938704169|https://openalex.org/W2888482885
https://openalex.org/W2963918774,Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data

Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such sentences, have however not been so successful. Several attempts at learning representations sentences reached satisfactory enough performance be widely adopted. In this paper, we show how universal sentence using the supervised data Stanford Natural Language Inference datasets can consistently outperform methods like SkipThought vectors a wide range transfer tasks. Much computer vision uses ImageNet features, which then transferred other tasks, our work tends indicate suitability natural language inference Our encoder is publicly available.

natural language, knowledge discovery, nlp task, natural language processing, universal sentence representations, computer science, language model, machine learning, linguistics, machine learning research, deep learning, data science, semantic representation, computational linguistics",2017,1939,natural language|knowledge discovery|nlp task|natural language processing|universal sentence representations|computer science|language model|machine learning|linguistics|machine learning research|deep learning|data science|semantic representation|computational linguistics,https://openalex.org/W2896457183|https://openalex.org/W2970641574|https://openalex.org/W2963026768|https://openalex.org/W2923014074|https://openalex.org/W2794557536|https://openalex.org/W2891177506
https://openalex.org/W3170841864,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers

Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts larger receptive fields. Since context modeling is critical for segmentation, latest efforts have been focused on increasing field, through either dilated/atrous convolutions or inserting attention modules. However, based FCN architecture remains unchanged. In this paper, we aim to provide alternative perspective by treating as sequence-to-sequence prediction task. Specifically, deploy pure transformer (i.e., without convolution reduction) encode image sequence of patches. With global modeled in every layer transformer, can be combined simple decoder powerful model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state art ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) competitive results Cityscapes. Particularly, achieve first position highly test server leaderboard day submission.

computer science, natural language processing, sequence-to-sequence perspective, computer vision, machine learning, deep learning, image segmentation, semantic segmentation",2021,1934,computer science|natural language processing|sequence-to-sequence perspective|computer vision|machine learning|deep learning|image segmentation|semantic segmentation,
https://openalex.org/W2193413348,Deep Speech 2: End-to-End Speech Recognition in English and Mandarin,"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin

We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, allows us handle a diverse variety speech including noisy environments, accents and Key our is application HPC techniques, resulting in 7x speedup over previous system. this efficiency, experiments previously took weeks now run days. This enables iterate more quickly identify superior architectures algorithms. As result, several cases, system competitive the transcription human workers when benchmarked on standard datasets. Finally, using technique called Batch Dispatch GPUs data center, we inexpensively deployed online setting, delivering low latency serving users at scale.

language learning, language recognition, multi-speaker speech recognition, speech technology, speech translation, spoken language technology, natural language processing, speech processing, computer science, language model, machine learning, speech recognition, linguistics, language, deep learning, speech communication, voice recognition, end-to-end speech recognition",2015,1925,language learning|language recognition|multi-speaker speech recognition|speech technology|speech translation|spoken language technology|natural language processing|speech processing|computer science|language model|machine learning|speech recognition|linguistics|language|deep learning|speech communication|voice recognition|end-to-end speech recognition,
https://openalex.org/W2890964092,X-Vectors: Robust DNN Embeddings for Speaker Recognition,"X-Vectors: Robust DNN Embeddings for Speaker Recognition

In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained discriminate between speakers, maps variable-length utterances fixed-dimensional that call x-vectors. Prior studies have found leverage large-scale training datasets better than i-vectors. However, it can be challenging collect substantial quantities labeled training. We augmentation, consisting added noise and reverberation, as an inexpensive method multiply the amount robustness. x-vectors are compared with i-vector baselines on Speakers in Wild NIST SRE 2016 Cantonese. find while beneficial PLDA classifier, not helpful extractor. x-vector DNN effectively exploits due its supervised As a result, achieve superior evaluation datasets.

image analysis, adversarial machine learning, computer science, speech recognition, natural language processing, speech processing, machine learning, data science, speaker recognition, deep learning, image representation, autonomous learning, spoken language technology, robust speech recognition, robust dnn embeddings",2018,1917,image analysis|adversarial machine learning|computer science|speech recognition|natural language processing|speech processing|machine learning|data science|speaker recognition|deep learning|image representation|autonomous learning|spoken language technology|robust speech recognition|robust dnn embeddings,
https://openalex.org/W1981425990,Coding In-depth Semistructured Interviews,"Coding In-depth Semistructured Interviews

Many social science studies are based on coded in-depth semistructured interview transcripts. But researchers rarely report or discuss coding reliability in this work. Nor is there much literature the subject for type of data. This article presents a procedure developing schemes such It involves standardizing units text which coders work and then improving scheme’s discriminant capability (i.e., reducing errors) to an acceptable point as indicated by measures either intercoder agreement. approach especially useful situations where single knowledgeable coder will code all transcripts once scheme has been established. can also be used with other types qualitative data circumstances.

unstructured data, linguistics, information fusion, text mining, natural language processing, data coding, natural language generation, communication, conversation analysis, in-depth semistructured interviews, knowledge discovery, semi-structured data, deep learning, journalism, narrative, corpus linguistics, code representation, question answering",2013,1913,unstructured data|linguistics|information fusion|text mining|natural language processing|data coding|natural language generation|communication|conversation analysis|in-depth semistructured interviews|knowledge discovery|semi-structured data|deep learning|journalism|narrative|corpus linguistics|code representation|question answering,
https://openalex.org/W2562607067,Attention-based LSTM for Aspect-level Sentiment Classification,"Attention-based LSTM for Aspect-level Sentiment Classification

Aspect-level sentiment classification is a finegrained task in analysis.Since it provides more complete and in-depth results, aspect-level analysis has received much attention these years.In this paper, we reveal that the polarity of sentence not only determined by content but also highly related to concerned aspect.For instance, ""The appetizers are ok, service slow."",for aspect taste, positive while for service, negative.Therefore, worthwhile explore connection between an sentence.To end, propose Attention-based Long Short-Term Memory Network classification.The mechanism can concentrate on different parts when aspects taken as input.We experiment SemEval 2014 dataset results show our model achieves state-ofthe-art performance classification.

computer science, disinformation detection, topic model, text mining, natural language processing, large language model, aspect-level sentiment classification, attention-based lstm, machine learning, cognitive science, data science, computational intelligence, deep learning, knowledge discovery, machine learning research, content similarity detection, nlp task, attention",2016,1898,computer science|disinformation detection|topic model|text mining|natural language processing|large language model|aspect-level sentiment classification|attention-based lstm|machine learning|cognitive science|data science|computational intelligence|deep learning|knowledge discovery|machine learning research|content similarity detection|nlp task|attention,https://openalex.org/W2884001105|https://openalex.org/W2964236337
https://openalex.org/W2265846598,Recurrent Convolutional Neural Networks for Text Classification,"Recurrent Convolutional Neural Networks for Text Classification

Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce recurrent convolutional neural network for without features. our model, apply structure capture contextual information far possible when learning word representations, which may considerably less noise compared window-based networks. We also employ max-pooling layer that automatically judges words play key roles the components texts. conduct experiments four commonly used datasets. The experimental results show proposed method outperforms state-of-the-art methods several datasets, particularly document-level

computer science, topic model, text recognition, machine learning, text processing, text classification, convolutional neural network, natural language generation, recurrent neural network, cognitive science, data science, computational intelligence, language model, deep learning, machine learning research, machine vision, text mining, natural language processing, knowledge discovery",2015,1897,computer science|topic model|text recognition|machine learning|text processing|text classification|convolutional neural network|natural language generation|recurrent neural network|cognitive science|data science|computational intelligence|language model|deep learning|machine learning research|machine vision|text mining|natural language processing|knowledge discovery,https://openalex.org/W3105625590
https://openalex.org/W2970771982,SciBERT: A Pretrained Language Model for Scientific Text,"SciBERT: A Pretrained Language Model for Scientific Text

Iz Beltagy, Kyle Lo, Arman Cohan. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint (EMNLP-IJCNLP). 2019.

natural language processing, language model, language, scientific text, pretrained language model",2019,1897,natural language processing|language model|language|scientific text|pretrained language model,https://openalex.org/W2979826702|https://openalex.org/W2980282514|https://openalex.org/W2936695845|https://openalex.org/W3034238904|https://openalex.org/W3098824823
https://openalex.org/W2185175083,From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions

We propose to use the visual denotations of linguistic expressions (i.e. set images they describe) define novel denotational similarity metrics, which we show be at least as beneficial distributional similarities for two tasks that require semantic inference. To compute these similarities, construct a denotation graph, i.e. subsumption hierarchy over constituents and their denotations, based on large corpus 30K 150K descriptive captions.

computer science, data and information visualization, image analysis, information visualization, semantic similarity, visualization, semantic evaluation, cognitive science, data science, image representation, semantic interpretation, distributional semantics, machine vision, image descriptions, event descriptions, visual denotations, natural language processing, scene interpretation, semantic inference, knowledge discovery, image similarity, information retrieval",2014,1884,computer science|data and information visualization|image analysis|information visualization|semantic similarity|visualization|semantic evaluation|cognitive science|data science|image representation|semantic interpretation|distributional semantics|machine vision|image descriptions|event descriptions|visual denotations|natural language processing|scene interpretation|semantic inference|knowledge discovery|image similarity|information retrieval,https://openalex.org/W1514535095|https://openalex.org/W2950178297|https://openalex.org/W2277195237|https://openalex.org/W1840435438|https://openalex.org/W2550553598|https://openalex.org/W2561715562|https://openalex.org/W1527575280|https://openalex.org/W2139501017|https://openalex.org/W2425121537|https://openalex.org/W2966715458|https://openalex.org/W2886641317|https://openalex.org/W2963758027|https://openalex.org/W2962706528
https://openalex.org/W2092654472,Head-Driven Statistical Models for Natural Language Parsing,"Head-Driven Statistical Models for Natural Language Parsing

This article describes three statistical models for natural language parsing. The extend methods from probabilistic context-free grammars to lexicalized grammars, leading approaches in which a parse tree is represented as the sequence of decisions corresponding head-centered, top-down derivation tree. Independence assumptions then lead parameters that encode X-bar schema, subcategorization, ordering complements, placement adjuncts, bigram lexical dependencies, wh-movement, and preferences close attachment. All these are expressed by probabilities conditioned on heads. evaluated Penn Wall Street Journal Treebank, showing their accuracy competitive with other literature. To gain better understanding models, we also give results different constituent types, well breakdown precision/recall recovering various types dependencies. We analyze characteristics through experiments parsing accuracy, collecting frequencies structures treebank, linguistically motivated examples. Finally, compare others have been applied aiming some explanation difference performance models.

linguistics, natural language processing, language model, machine learning, computational linguistics, natural language parsing, nlp task, head-driven statistical models",2003,1865,linguistics|natural language processing|language model|machine learning|computational linguistics|natural language parsing|nlp task|head-driven statistical models,https://openalex.org/W2158899491|https://openalex.org/W2097606805|https://openalex.org/W1535015163|https://openalex.org/W2027979924
https://openalex.org/W3142508279,Semantic priming effects in visual word recognition : A selective review of current findings and theories,"Semantic priming effects in visual word recognition : A selective review of current findings and theories

Nearly two decades have elapsed since Meyer and Schvaneveldt (1971) 
reported their seminal experiment on semantic priming. In experiment, 
subjects were either to press one key if both of simultaneously presented 
visual letter strings English words or another otherwise. 
Unsurprisingly, subjects faster more accurate in responding displays 
containing semantically/associatively related (e.g., bread butter) 
than with unrelated doctor butter). Despite its 
intuitive simplicity, Schvaneveldt's priming effect spawned 
an exponential growth the number published papers that examined 
general issue how word recognition is influenced by context. Given 
the extensive research literature now exists this topic, it seems reasonable 
eighteen years later pause review empirical theoretical productsthat has yielded. My contribution volume attempts provide 
such a review.

semantic representation, psycholinguistics, natural language processing, selective review, semantic priming effects, computer vision, semantic evaluation, visual word recognition, cognitive science, current findings, data science, semantic processing, visual perception, language processing in the brain, psychology, image representation",1991,1851,semantic representation|psycholinguistics|natural language processing|selective review|semantic priming effects|computer vision|semantic evaluation|visual word recognition|cognitive science|current findings|data science|semantic processing|visual perception|language processing in the brain|psychology|image representation,
https://openalex.org/W2963809228,Energy and Policy Considerations for Deep Learning in NLP,"Energy and Policy Considerations for Deep Learning in NLP

Recent progress in hardware and methodology for training neural networks has ushered a new generation of large trained on abundant data. These models have obtained notable gains accuracy across many NLP tasks. However, these improvements depend the availability exceptionally computational resources that necessitate similarly substantial energy consumption. As result are costly to train develop, both financially, due cost electricity or cloud compute time, environmentally, carbon footprint required fuel modern tensor processing hardware. In this paper we bring issue attention researchers by quantifying approximate financial environmental costs variety recently successful network NLP. Based findings, propose actionable recommendations reduce improve equity research practice.

energy policy, public policy, policy considerations, natural language processing, deep learning",2019,1851,energy policy|public policy|policy considerations|natural language processing|deep learning,
https://openalex.org/W3030163527,Language Models are Few-Shot Learners,"Language Models are Few-Shot Learners

Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training a large corpus of text followed fine-tuning specific task. While typically task-agnostic in architecture, this method still requires task-specific datasets thousands or tens examples. By contrast, humans can generally perform new language task from only few examples simple instructions - something which current systems largely struggle to do. Here we show that scaling up models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art approaches. Specifically, train GPT-3, an autoregressive model 175 billion parameters, 10x more than any previous non-sparse model, test its performance the setting. For all tasks, GPT-3 is applied without gradient updates fine-tuning, demonstrations specified purely via interaction model. achieves strong datasets, including translation, question-answering, cloze as well several require on-the-fly reasoning domain adaptation, such unscrambling words, using novel word sentence, performing 3-digit arithmetic. At same time, also identify some where GPT-3's learning struggles, faces methodological issues related training web corpora. Finally, find generate samples news articles human evaluators have difficulty distinguishing written humans. We discuss broader societal impacts finding general.

language learning, few-shot learners, language, natural language processing, few-shot learning, language model",2020,1826,language learning|few-shot learners|language|natural language processing|few-shot learning|language model,https://openalex.org/W3094502228
https://openalex.org/W2953320089,NLTK: The Natural Language Toolkit,"NLTK: The Natural Language Toolkit

NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic statistical natural language processing, interfaced to annotated corpora. Students augment replace existing components, learn structured programming by example, manipulate sophisticated models from outset.

natural language interface, natural language processing, language, natural language toolkit, computational linguistics, natural language, nlp task, annotation tool",2002,1822,natural language interface|natural language processing|language|natural language toolkit|computational linguistics|natural language|nlp task|annotation tool,https://openalex.org/W2979826702|https://openalex.org/W2980282514|https://openalex.org/W3030163527|https://openalex.org/W3098824823
https://openalex.org/W2951548327,Fast R-CNN,"Fast R-CNN

This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. R-CNN builds on previous work to efficiently classify proposals using deep convolutional networks. Compared work, employs several innovations improve training and testing speed while also increasing detection accuracy. trains the very VGG16 network 9x faster than R-CNN, is 213x at test-time, achieves higher mAP PASCAL VOC 2012. SPPnet, 3x faster, tests 10x more accurate. implemented in Python C++ (using Caffe) available under open-source MIT License this https URL.

image analysis, computer science, machine learning, deep reinforcement learning, object recognition, computational imaging, object detection, neural network (machine learning), machine learning research, natural language processing, convolutional neural network, data science, deep learning, recurrent neural network, machine vision, cognitive science",2015,1822,image analysis|computer science|machine learning|deep reinforcement learning|object recognition|computational imaging|object detection|neural network (machine learning)|machine learning research|natural language processing|convolutional neural network|data science|deep learning|recurrent neural network|machine vision|cognitive science,
https://openalex.org/W4226278401,Training language models to follow instructions with human feedback,"Training language models to follow instructions with human feedback

Making language models bigger does not inherently make them better at following a user's intent. For example, large can generate outputs that are untruthful, toxic, or simply helpful to the user. In other words, these aligned with their users. this paper, we show an avenue for aligning user intent on wide range of tasks by fine-tuning human feedback. Starting set labeler-written prompts and submitted through OpenAI API, collect dataset labeler demonstrations desired model behavior, which use fine-tune GPT-3 using supervised learning. We then rankings outputs, further reinforcement learning from call resulting InstructGPT. evaluations our prompt distribution, 1.3B parameter InstructGPT preferred 175B GPT-3, despite having 100x fewer parameters. Moreover, improvements in truthfulness reductions toxic output generation while minimal performance regressions public NLP datasets. Even though still makes simple mistakes, results feedback is promising direction

natural language processing, language model, language, training language models, human feedback",2022,1817,natural language processing|language model|language|training language models|human feedback,
https://openalex.org/W2613904329,Convolutional Sequence to Sequence Learning,"Convolutional Sequence to Sequence Learning

The prevalent approach to sequence learning maps an input a variable length output via recurrent neural networks. We introduce architecture based entirely on convolutional Compared models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities fixed independent length. Our use gated linear units eases gradient propagation we equip each decoder layer with separate attention module. outperform accuracy deep LSTM setup Wu et al. (2016) both WMT'14 English-German English-French translation at order magnitude faster speed, GPU CPU.

convolutional sequence, pattern recognition, automatic classification, natural language processing, sequential learning, cognitive science, computer science, convolutional neural network, machine learning, deep learning, data science, sequence modelling",2017,1813,convolutional sequence|pattern recognition|automatic classification|natural language processing|sequential learning|cognitive science|computer science|convolutional neural network|machine learning|deep learning|data science|sequence modelling,https://openalex.org/W2884001105|https://openalex.org/W2742947407|https://openalex.org/W2888482885|https://openalex.org/W2963096510
https://openalex.org/W2250861254,A Fast and Accurate Dependency Parser using Neural Networks,"A Fast and Accurate Dependency Parser using Neural Networks

Almost all current dependency parsers classify based on millions of sparse indicator features.Not only do these features generalize poorly, but the cost feature computation restricts parsing speed significantly.In this work, we propose a novel way learning neural network classifier for use in greedy, transition-based parser.Because learns and uses just small number dense features, it can work very fast, while achieving an about 2% improvement unlabeled labeled attachment scores both English Chinese datasets.Concretely, our parser is able to parse more than 1000 sentences per second at 92.2% score Penn Treebank.

computer science, natural language processing, machine learning, neural networks, accurate dependency parser",2014,1812,computer science|natural language processing|machine learning|neural networks|accurate dependency parser,https://openalex.org/W2884001105|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2964167098|https://openalex.org/W2963042536
https://openalex.org/W2610179052,"Algorithms on strings, trees, and sequences","Algorithms on strings, trees, and sequences

Linear-Time Construction of Suffix Trees We will present two methods for constructing suffix trees in detail, Ukkonen’s method and Weiner’s method. Weiner was the first to show that can be built linear time, his is presented both its historical importance some different technical ideas it contains. However, lJkkonen’s equally fast uses far less space (i.e., memory) practice than Hence Ukkonen choice most problems requiring construction a tree. also believe easier understand. Therefore, A reader who wishes study only one advised concentrate on it. our development does not depend understanding algorithm, algorithms read independently (with small shared section noted description method).

pattern recognition, computer science, fuzzy logic, tree language, natural language processing, homological algebra, sequence analysis, treebanks, theory of computation, combinatorial theory, machine learning, applied mathematics, numerical algorithm, automated reasoning, evolutionary computation, data science, string-searching algorithm, string processing",1997,1802,pattern recognition|computer science|fuzzy logic|tree language|natural language processing|homological algebra|sequence analysis|treebanks|theory of computation|combinatorial theory|machine learning|applied mathematics|numerical algorithm|automated reasoning|evolutionary computation|data science|string-searching algorithm|string processing,https://openalex.org/W2001496424
https://openalex.org/W2402268235,LSTM neural networks for language modeling,"LSTM neural networks for language modeling

Neural networks have become increasingly popular for the task of language modeling.Whereas feed-forward only exploit a fixed context length to predict next word sequence, conceptually, standard recurrent neural can take into account all predecessor words.On other hand, it is well known that are difficult train and therefore unlikely show full potential models.These problems addressed by Long Short-Term Memory network architecture.In this work, we analyze type on an English large French modeling task.Experiments improvements about 8 % relative in perplexity over LMs.In addition, gain considerable WER top state-of-the-art speech recognition system.

computer science, linguistics, convolutional neural network, large language model, language model, natural language processing, language, language modeling, machine learning, recurrent neural network, computational intelligence, deep learning, sequence modelling, machine learning research, neural network (machine learning), principal component analysis, language learning, lstm neural networks",2012,1800,computer science|linguistics|convolutional neural network|large language model|language model|natural language processing|language|language modeling|machine learning|recurrent neural network|computational intelligence|deep learning|sequence modelling|machine learning research|neural network (machine learning)|principal component analysis|language learning|lstm neural networks,https://openalex.org/W2130942839|https://openalex.org/W2949888546|https://openalex.org/W1938755728|https://openalex.org/W2938704169|https://openalex.org/W2963042536
https://openalex.org/W2963444790,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,"DualGAN: Unsupervised Dual Learning for Image-to-Image Translation

Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently [7, 8, 21, 12, 4, 18]. Depending on the task complexity, thousands to millions of labeled image pairs are needed train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities data may not always be available. Inspired by dual learning from natural language [23], we develop novel dual-GAN mechanism, which enables translators trained two sets unlabeled images domains. In our architecture, primal GAN learns translate domain U those in V, while invert task. The closed loop tasks allows either translated then reconstructed. Hence loss function that accounts reconstruction error can used translators. Experiments multiple with show considerable performance gain DualGAN over single For some tasks, achieve comparable or slightly better results than fully data.

unsupervised dual learning, digital image processing, computer science, feature learning, deep learning, feature fusion, machine vision, natural language processing, machine translation, image-to-image translation, feature extraction, image representation, computational imaging, style transfer, linguistics, data science, computer vision, computational intelligence, unsupervised machine learning, image analysis",2017,1800,unsupervised dual learning|digital image processing|computer science|feature learning|deep learning|feature fusion|machine vision|natural language processing|machine translation|image-to-image translation|feature extraction|image representation|computational imaging|style transfer|linguistics|data science|computer vision|computational intelligence|unsupervised machine learning|image analysis,
https://openalex.org/W2963084599,Self-Critical Sequence Training for Image Captioning,"Self-Critical Sequence Training for Image Captioning

Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics the task at hand. In this paper we consider problem of optimizing image captioning using learning, and show by carefully our test MSCOCO task, significant gains in performance realized. Our are built a new optimization approach call self-critical sequence training (SCST). SCST is form popular REINFORCE algorithm that, rather than estimating baseline normalize rewards reduce variance, utilizes output its own test-time inference experiences. Using approach, reward signal (as actor-critic must do) normalization algorithms typically avoided, while same time harmonizing model with respect procedure. Empirically find CIDEr metric greedy decoding highly effective. results evaluation sever establish state-of-the-art improving best result terms from 104.9 114.7.

computer science, self-critical sequence training, image captioning, natural language processing, computer vision, machine learning, self-supervised learning, sequential learning, deep learning",2017,1799,computer science|self-critical sequence training|image captioning|natural language processing|computer vision|machine learning|self-supervised learning|sequential learning|deep learning,https://openalex.org/W2745461083
https://openalex.org/W2026992087,Recognizing Spoken Words: The Neighborhood Activation Model,"Recognizing Spoken Words: The Neighborhood Activation Model

Objective: A fundamental problem in the study of human spoken word recognition concerns structural relations among sound patterns words memory and effects these have on recognition. In present investigation, computational experimental methods were employed to address a number issues related representation organization mental lexicon lay groundwork for model Design: Using computerized consisting transcriptions 20,000 words, similarity neighborhoods each computed. Among variables interest computation were: 1) occurring neighborhood, 2) degree phonetic 3) frequencies occurrence language. The auditory examined series behavioral experiments employing three paradigms: perceptual identification noise, lexical decision, naming. Results: results demonstrated that nature neighborhood affect speed accuracy probability rule was developed adequately predicted performance. This rule, based onLuce's (1959) choice combines stimulus intelligibility, confusability, frequency into single expression. Based this recognition, activation model, proposed. describes structure process discriminating acoustic-phonetic representations memory. important implications current conceptions normal hearing impaired populations children adults.

pattern recognition, speech recognition, voice recognition, natural language processing, speech processing, language, speech analysis, speech perception, neighborhood activation model, spoken language processing, communication, cognitive science, language recognition, spoken words, language processing in the brain, speech communication, speech science, spoken language technology",1998,1794,pattern recognition|speech recognition|voice recognition|natural language processing|speech processing|language|speech analysis|speech perception|neighborhood activation model|spoken language processing|communication|cognitive science|language recognition|spoken words|language processing in the brain|speech communication|speech science|spoken language technology,
https://openalex.org/W2146089916,"Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications","Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications

We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text. describe our system, the Text Analysis Knowledge Extraction System (cTAKES), released at http://www.ohnlp.org. The cTAKES builds on existing technologies—the Unstructured Information Management Architecture framework OpenNLP toolkit. Its components, specifically trained domain, create rich linguistic semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer system-level evaluation F-score=0.715 exact 0.824 overlapping spans, accuracy concept mapping, negation, status attributes spans 0.957, 0.943, 0.859, 0.580, 0.939, 0.839, respectively. Overall performance is discussed against five applications. annotations are foundation methods modules higher-level

computer science, clinical database, principal component analysis, clinical text analysis, digital medicine, semantic evaluation, digital health, data science, biomedical analysis, health informatics, clinical decision support system, component evaluation, medical image computing, text mining, natural language processing, medical ontology, knowledge extraction, diagnosis, biomedical informatics, public health",2010,1794,computer science|clinical database|principal component analysis|clinical text analysis|digital medicine|semantic evaluation|digital health|data science|biomedical analysis|health informatics|clinical decision support system|component evaluation|medical image computing|text mining|natural language processing|medical ontology|knowledge extraction|diagnosis|biomedical informatics|public health,https://openalex.org/W2169818249
https://openalex.org/W2436001372,Word sense disambiguation,"Word sense disambiguation

Word sense disambiguation (WSD) is the ability to identify meaning of words in context a computational manner. WSD considered an AI-complete problem, that is, task whose solution at least as hard most difficult problems artificial intelligence. We introduce reader motivations for solving ambiguity and provide description task. overview supervised, unsupervised, knowledge-based approaches. The assessment systems discussed Senseval/Semeval campaigns, aiming objective evaluation participating several different tasks. Finally, applications, open problems, future directions are discussed.

applied linguistics, linguistics, psycholinguistics, word-sense disambiguation, natural language processing, semantic analysis (linguistics), language, general linguistics, semantic evaluation, lexical semantics, cognitive science, computational linguistics, semantics, semantic processing, semantic interpretation",2009,1786,applied linguistics|linguistics|psycholinguistics|word-sense disambiguation|natural language processing|semantic analysis (linguistics)|language|general linguistics|semantic evaluation|lexical semantics|cognitive science|computational linguistics|semantics|semantic processing|semantic interpretation,
https://openalex.org/W3101913037,LexRank: Graph-based Lexical Centrality as Salience in Text Summarization,"LexRank: Graph-based Lexical Centrality as Salience in Text Summarization

We introduce a stochastic graph-based method for computing relative importance of textual units Natural Language Processing. test the technique on problem Text Summarization (TS). Extractive TS relies concept sentence salience to identify most important sentences in document or set documents. Salience is typically defined terms presence particular words similarity centroid pseudo-sentence. consider new approach, LexRank, based eigenvector centrality graph representation sentences. In this model, connectivity matrix intra-sentence cosine used as adjacency Our system, LexRank ranked first place more than one task recent DUC 2004 evaluation. paper we present detailed analysis our approach and apply it larger data including from earlier evaluations. discuss several methods compute using graph. The results show that degree-based (including LexRank) outperform both centroid-based other systems participating cases. Furthermore, with threshold outperforms techniques continuous LexRank. also quite insensitive noise may result an imperfect topical clustering

linguistics, natural language processing, semantic similarity, graph-based lexical centrality, computational linguistics, text summarization",2004,1786,linguistics|natural language processing|semantic similarity|graph-based lexical centrality|computational linguistics|text summarization,https://openalex.org/W2963929190
https://openalex.org/W2963206148,A Diversity-Promoting Objective Function for Neural Conversation Models,"A Diversity-Promoting Objective Function for Neural Conversation Models

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan. Proceedings of the 2016 Conference North American Chapter Association for Computational Linguistics: Human Language Technologies. 2016.

computer science, narrative, information fusion, natural language generation, neuroscience, communication, conversation analysis, dialogue management, nlp task, conversational recommender system, context model, human-computer interaction, language model, spoken dialog system, deep learning, diversity-promoting objective function, neural conversation models, natural language processing, speech communication, neural network (machine learning)",2016,1783,computer science|narrative|information fusion|natural language generation|neuroscience|communication|conversation analysis|dialogue management|nlp task|conversational recommender system|context model|human-computer interaction|language model|spoken dialog system|deep learning|diversity-promoting objective function|neural conversation models|natural language processing|speech communication|neural network (machine learning),https://openalex.org/W2884001105|https://openalex.org/W2742947407|https://openalex.org/W2938704169|https://openalex.org/W2963167310|https://openalex.org/W2963096510
https://openalex.org/W2102113734,Towards End-To-End Speech Recognition with Recurrent Neural Networks,"Towards End-To-End Speech Recognition with Recurrent Neural Networks

This paper presents a speech recognition system that directly transcribes audio data with text, without requiring an intermediate phonetic representation. The is based on combination of the deep bidirectional LSTM recurrent neural network architecture and Connectionist Temporal Classification objective function. A modification to function introduced trains minimise expectation arbitrary transcription loss allows direct optimisation word error rate, even in absence lexicon or language model. achieves rate 27.3% Wall Street Journal corpus no prior linguistic information, 21.9% only allowed words, 8.2% trigram Combining baseline further reduces 6.7%.

computer science, speech recognition, natural language processing, machine learning, spoken language technology, recurrent neural network, deep learning, neural network (machine learning)",2014,1775,computer science|speech recognition|natural language processing|machine learning|spoken language technology|recurrent neural network|deep learning|neural network (machine learning),https://openalex.org/W2884001105|https://openalex.org/W2327501763|https://openalex.org/W2193413348|https://openalex.org/W1922655562|https://openalex.org/W2139501017|https://openalex.org/W2742947407|https://openalex.org/W2962826786
https://openalex.org/W1971220772,Automatic sense disambiguation using machine readable dictionaries,"Automatic sense disambiguation using machine readable dictionaries

Article Free Access Share on Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream Author: Michael Lesk Bell Communications Research, Morristown, NJ NJView Profile Authors Info & Claims SIGDOC '86: Proceedings of the 5th annual international conference Systems documentationJune 1986 Pages 24–26https://doi.org/10.1145/318723.318728Online:01 June 1986Publication History 861citation3,775DownloadsMetricsTotal Citations861Total Downloads3,775Last 12 Months247Last 6 weeks34 Get Citation AlertsNew Alert added!This alert has been successfully added and will be sent to:You notified whenever record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log in account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

linguistics, word-sense disambiguation, natural language processing, machine-readable dictionary, computational linguistics, automatic sense disambiguation",1986,1760,linguistics|word-sense disambiguation|natural language processing|machine-readable dictionary|computational linguistics|automatic sense disambiguation,https://openalex.org/W2101210369|https://openalex.org/W1659833910|https://openalex.org/W2436001372|https://openalex.org/W2130337399
https://openalex.org/W3171007011,Exploring Simple Siamese Representation Learning,"Exploring Simple Siamese Representation Learning

Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These maximize the similarity between two augmentations of one image, subject to certain conditions avoiding collapsing solutions. In this paper, we report surprising empirical results that simple can learn meaningful representations even using none following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show solutions do exist loss and structure, but stop-gradient operation plays an essential role preventing collapsing. We provide hypothesis on implication stop-gradient, further proof-of-concept verifying it. ""SimSiam"" method achieves competitive ImageNet downstream tasks. hope baseline will motivate people rethink roles architectures Code is made available. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>

image analysis, computational imaging, computer science, linguistics, information fusion, language model, natural language processing, machine learning, multilingual pretraining, geometric learning, data science, knowledge discovery, deep learning, image representation, similarity search, machine vision, representation analysis, manifold learning",2021,1739,image analysis|computational imaging|computer science|linguistics|information fusion|language model|natural language processing|machine learning|multilingual pretraining|geometric learning|data science|knowledge discovery|deep learning|image representation|similarity search|machine vision|representation analysis|manifold learning,
https://openalex.org/W2526050071,CNN architectures for large-scale audio classification,"CNN architectures for large-scale audio classification

Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset 70M training videos (5.24 million hours) with 30,871 video-level labels. examine fully connected Deep (DNNs), AlexNet [1], VGG [2], Inception [3], ResNet [4]. investigate varying size both set label vocabulary, finding that analogs CNNs used do well on our audio task, larger sets help up point. A model using embeddings from these classifiers does much better than raw features Audio Set [5] Acoustic Event Detection (AED) task.

pattern recognition, computer science, machine learning, automatic classification, large-scale audio classification, information fusion, convolutional neural network, cognitive science, data science, large-scale datasets, neural architecture search, cnn architectures, audio retrieval, deep learning, machine learning research, natural language processing, speech processing, knowledge discovery, neural network (machine learning), music information retrieval",2017,1735,pattern recognition|computer science|machine learning|automatic classification|large-scale audio classification|information fusion|convolutional neural network|cognitive science|data science|large-scale datasets|neural architecture search|cnn architectures|audio retrieval|deep learning|machine learning research|natural language processing|speech processing|knowledge discovery|neural network (machine learning)|music information retrieval,
https://openalex.org/W2963625095,Named Entity Recognition with Bidirectional LSTM-CNNs,"Named Entity Recognition with Bidirectional LSTM-CNNs

Named entity recognition is a challenging task that has traditionally required large amounts of knowledge in the form feature engineering and lexicons to achieve high performance. In this paper, we present novel neural network architecture automatically detects word- character-level features using hybrid bidirectional LSTM CNN architecture, eliminating need for most engineering. We also propose method encoding partial lexicon matches networks compare it existing approaches. Extensive evaluation shows that, given only tokenized text publicly available word embeddings, our system competitive on CoNLL-2003 dataset surpasses previously reported state art performance OntoNotes 5.0 by 2.13 F1 points. By two constructed from publicly-available sources, establish new with an score 91.62 86.28 OntoNotes, surpassing systems employ heavy engineering, proprietary lexicons, rich linking information.

computer science, machine learning, automatic classification, information fusion, convolutional neural network, natural language generation, bidirectional lstm-cnns, entity recognition, semantic evaluation, cognitive science, data science, language model, entity summarization, deep learning, machine learning research, text mining, natural language processing, knowledge discovery, named-entity recognition, entity disambiguation",2016,1727,computer science|machine learning|automatic classification|information fusion|convolutional neural network|natural language generation|bidirectional lstm-cnns|entity recognition|semantic evaluation|cognitive science|data science|language model|entity summarization|deep learning|machine learning research|text mining|natural language processing|knowledge discovery|named-entity recognition|entity disambiguation,https://openalex.org/W2962739339|https://openalex.org/W2296283641|https://openalex.org/W2884001105|https://openalex.org/W2787560479|https://openalex.org/W3019166713
https://openalex.org/W2053677366,On cluster validity for the fuzzy c-means model,"On cluster validity for the fuzzy c-means model

Many functionals have been proposed for validation of partitions object data produced by the fuzzy c-means (FCM) clustering algorithm. We examine role a subtle but important parameter-the weighting exponent m FCM model-plays in determining validity partitions. The considered are partition coefficient and entropy indexes Bezdek, Xie-Beni (1991), extended indexes, Fukuyama-Sugeno index (1989). Limit analysis indicates, numerical experiments confirm, that is sensitive to both high low values may be unreliable because this. Of tested, provided best response over wide range choices number clusters, (2-10), from 1.01-7. Finally, our calculations suggest choice probably interval [1.5, 2.5], whose mean midpoint, m=2, often preferred many users FCM.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

computer science, fuzzy logic, soft computing, machine learning, fuzzy clustering, cluster validity, fuzzy modeling, fuzzy pattern recognition, fuzzy c-means model, cognitive science, fuzzy mathematics, fuzzy system, approximation theory, biostatistics, deep learning, machine learning research, natural language processing, clustering, fuzzy computing, cluster computing",1995,1726,computer science|fuzzy logic|soft computing|machine learning|fuzzy clustering|cluster validity|fuzzy modeling|fuzzy pattern recognition|fuzzy c-means model|cognitive science|fuzzy mathematics|fuzzy system|approximation theory|biostatistics|deep learning|machine learning research|natural language processing|clustering|fuzzy computing|cluster computing,
https://openalex.org/W2949541494,Convolutional Neural Networks for Sentence Classification,"Convolutional Neural Networks for Sentence Classification

We report on a series of experiments with convolutional neural networks (CNN) trained top pre-trained word vectors for sentence-level classification tasks. show that simple CNN little hyperparameter tuning and static achieves excellent results multiple benchmarks. Learning task-specific through fine-tuning offers further gains in performance. additionally propose modification to the architecture allow use both vectors. The models discussed herein improve upon state art 4 out 7 tasks, which include sentiment analysis question classification.

computer science, knowledge discovery, nlp task, semantic evaluation, convolutional neural network, cognitive science, neural computation, sequential learning, neuroscience, sentence classification, text processing, deep learning, neural network (machine learning), semantic interpretation, machine learning, machine learning research, natural language processing, computational intelligence, machine translation",2014,1702,computer science|knowledge discovery|nlp task|semantic evaluation|convolutional neural network|cognitive science|neural computation|sequential learning|neuroscience|sentence classification|text processing|deep learning|neural network (machine learning)|semantic interpretation|machine learning|machine learning research|natural language processing|computational intelligence|machine translation,
https://openalex.org/W2097333193,A statistical approach to machine translation,"A statistical approach to machine translation

In this paper, we present a statistical approach to machine translation. We describe the application of our translation from French English and give preliminary results.

computer science, statistical methodology, language model, natural language processing, applied statistics, statistical inference, neural machine translation, data science, statistics, machine translation, language learning, statistical model, computer-assisted translation",1990,1688,computer science|statistical methodology|language model|natural language processing|applied statistics|statistical inference|neural machine translation|data science|statistics|machine translation|language learning|statistical model|computer-assisted translation,https://openalex.org/W2096175520|https://openalex.org/W2121227244|https://openalex.org/W2963216553|https://openalex.org/W2117400858|https://openalex.org/W1916559533|https://openalex.org/W2010595692
https://openalex.org/W2550553598,SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning,"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning

Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing models are generally spatial, i.e., the is modeled spatial probabilities that re-weight last conv-layer feature map of a CNN encoding an input image. However, we argue does not necessarily conform to mechanism - dynamic extractor combines contextual fixations over time, features naturally channel-wise multi-layer. In this paper, introduce novel convolutional neural network dubbed SCA-CNN incorporates Spatial Channel-wise Attentions CNN. task image captioning, dynamically modulates sentence generation context multi-layer maps, where (i.e., attentive locations at multiple layers) what channels) is. We evaluate proposed architecture on three benchmark datasets: Flickr8K, Flickr30K, MSCOCO. It consistently observed significantly outperforms state-of-the-art attention-based methods.

computer science, image captioning, channel-wise attention, machine learning, image analysis, convolutional neural network, convolutional networks, feature detection, cognitive science, data science, image representation, computational imaging, image communication, deep learning, machine learning research, machine vision, natural language processing, feature extraction, computer vision, feature fusion, image classification",2017,1683,computer science|image captioning|channel-wise attention|machine learning|image analysis|convolutional neural network|convolutional networks|feature detection|cognitive science|data science|image representation|computational imaging|image communication|deep learning|machine learning research|machine vision|natural language processing|feature extraction|computer vision|feature fusion|image classification,
https://openalex.org/W2787560479,Deep contextualized word representations,"Deep contextualized word representations

We introduce a new type of deep contextualized word representation that models both (1) complex characteristics use (e.g., syntax and semantics), (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our vectors are learned functions the internal states bidirectional language (biLM), which is pre-trained on large text corpus. show representations can be easily added existing significantly improve state art six challenging NLP problems, including question answering, textual entailment sentiment analysis. also present an analysis showing exposing internals network crucial, allowing downstream mix different types semi-supervision signals.

word representations, linguistics, knowledge discovery, language learning, computational semantics, semantic representation, semantic similarity, language, language science, natural language processing, text mining, computational linguistics, word embeddings, deep learning, cognitive science, language model",2018,1677,word representations|linguistics|knowledge discovery|language learning|computational semantics|semantic representation|semantic similarity|language|language science|natural language processing|text mining|computational linguistics|word embeddings|deep learning|cognitive science|language model,https://openalex.org/W2911489562|https://openalex.org/W2980282514|https://openalex.org/W2936695845|https://openalex.org/W3011411500|https://openalex.org/W3098824823|https://openalex.org/W2742947407|https://openalex.org/W2966715458|https://openalex.org/W3105625590|https://openalex.org/W2968124245
https://openalex.org/W2133576408,Co-clustering documents and words using bipartite spectral graph partitioning,"Co-clustering documents and words using bipartite spectral graph partitioning

Both document clustering and word are well studied problems. Most existing algorithms cluster documents words separately but not simultaneously. In this paper we present the novel idea of modeling collection as a bipartite graph between words, using which simultaneous problem can be posed partitioning problem. To solve problem, use new spectral co-clustering algorithm that uses second left right singular vectors an appropriately scaled word-document matrix to yield good bipartitionings. The enjoys some optimality properties; it shown real relaxation NP-complete bipartitioning We experimental results verify resulting works in practice.

computer science, topic model, document classification, co-clustering documents, information fusion, information visualization, language, data science, content similarity detection, data mining, document clustering, document analysis, text mining, natural language processing, clustering, knowledge discovery, semantic web, information retrieval, cluster computing",2001,1672,computer science|topic model|document classification|co-clustering documents|information fusion|information visualization|language|data science|content similarity detection|data mining|document clustering|document analysis|text mining|natural language processing|clustering|knowledge discovery|semantic web|information retrieval|cluster computing,
https://openalex.org/W2963223306,Generating Sentences from a Continuous Space,"Generating Sentences from a Continuous Space

The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation.In this work, we introduce study rnn-based variational autoencoder generative that incorporates distributed latent representations of entire sentences.This factorization allows it to explicitly holistic properties such as style, topic, high-level syntactic features.Samples the prior over these remarkably produce diverse well-formed through simple deterministic decoding.By examining paths space, are able generate coherent novel interpolate between known sentences.We present techniques for solving difficult learning problem presented by model, demonstrate its effectiveness in imputing missing words, explore many interesting model's negative results on use modeling.but now , they parked out front owen stepped car he could see True: transition was complete .RNNLM: "" i said .VAE: driver 's door .you kill him his men .

continuous delivery, computer science, linguistics, continuous space, language model, natural language processing, natural language generation, language engineering, syntax, generative linguistics, machine learning, computational linguistics, nlp task, machine translation, language generation",2016,1662,continuous delivery|computer science|linguistics|continuous space|language model|natural language processing|natural language generation|language engineering|syntax|generative linguistics|machine learning|computational linguistics|nlp task|machine translation|language generation,https://openalex.org/W2884001105|https://openalex.org/W2963969878
https://openalex.org/W2096943734,Transfer Feature Learning with Joint Distribution Adaptation,"Transfer Feature Learning with Joint Distribution Adaptation

Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data the source domain to build accurate classifier target domain. However, most prior methods have not simultaneously reduced difference both marginal distribution and conditional between domains. In this paper, we put forward a novel transfer approach, referred Joint Distribution Adaptation (JDA). Specifically, JDA aims jointly adapt principled dimensionality reduction procedure, construct new feature representation that robust substantial difference. Extensive experiments verify can significantly outperform several state-of-the-art on four types of cross-domain image classification problems.

pattern recognition, computer science, feature learning, transfer feature, machine learning, joint distribution adaptation, image analysis, information fusion, robot learning, feature transformation, data science, transfer learning, systems engineering, deep learning, machine learning research, natural language processing, knowledge transfer, domain adaptation, knowledge discovery",2013,1654,pattern recognition|computer science|feature learning|transfer feature|machine learning|joint distribution adaptation|image analysis|information fusion|robot learning|feature transformation|data science|transfer learning|systems engineering|deep learning|machine learning research|natural language processing|knowledge transfer|domain adaptation|knowledge discovery,
https://openalex.org/W2517194566,Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification,"Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification

Relation classification is an important semantic processing task in the field of natural language (NLP). State-ofthe-art systems still rely on lexical resources such as WordNet or NLP like dependency parser and named entity recognizers (NER) to get high-level features. Another challenge that information can appear at any position sentence. To tackle these problems, we propose Attention-Based Bidirectional Long Short-Term Memory Networks(AttBLSTM) capture most a The experimental results SemEval-2010 relation show our method outperforms existing methods, with only word vectors.

natural language processing, machine learning, cognitive science, relation classification, deep learning, neural network (machine learning)",2016,1630,natural language processing|machine learning|cognitive science|relation classification|deep learning|neural network (machine learning),
https://openalex.org/W2962883855,Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models,"Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models

We investigate the task of building open domain, conversational dialogue systems based on large corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up possibility for realistic, flexible interactions. In support this goal, we extend recently proposed hierarchical recurrent encoder-decoder neural network to and demonstrate model is competitive with state-of-the-art language back-off n-gram limitations similar approaches, show how its performance can be improved by bootstrapping learning from a larger question-answer pair corpus pretrained word embeddings.

computer science, human-computer interaction, information fusion, language model, natural language processing, natural language generation, spoken dialog system, end-to-end dialogue systems, communication, generative ai, cognitive science, computational linguistics, dialogue management, computational intelligence, deep learning, generative system, nlp task, neural network (machine learning)",2016,1627,computer science|human-computer interaction|information fusion|language model|natural language processing|natural language generation|spoken dialog system|end-to-end dialogue systems|communication|generative ai|cognitive science|computational linguistics|dialogue management|computational intelligence|deep learning|generative system|nlp task|neural network (machine learning),https://openalex.org/W2884001105|https://openalex.org/W2963206148|https://openalex.org/W2742947407|https://openalex.org/W2963167310
https://openalex.org/W2134237567,Estimation of probabilities from sparse data for the language model component of a speech recognizer,"Estimation of probabilities from sparse data for the language model component of a speech recognizer

The description of a novel type m-gram language model is given. offers, via nonlinear recursive procedure, computation and space efficient solution to the problem estimating probabilities from sparse data. This compares favorably other proposed methods. While method has been developed for successfully implemented in IBM Real Time Speech Recognizers, its generality makes it applicable areas where data arises.

language model component, computer science, linguistics, speech recognition, language model, natural language processing, language, speech processing, sparse data, spoken language processing, machine learning, sparse representation, language recognition, statistical signal processing, speech communication, noisy data, spoken language technology, speech recognizer",1987,1626,language model component|computer science|linguistics|speech recognition|language model|natural language processing|language|speech processing|sparse data|spoken language processing|machine learning|sparse representation|language recognition|statistical signal processing|speech communication|noisy data|spoken language technology|speech recognizer,https://openalex.org/W1659833910|https://openalex.org/W1934041838
https://openalex.org/W2936695845,BERTScore: Evaluating Text Generation with BERT,"BERTScore: Evaluating Text Generation with BERT

We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score each token in the candidate sentence with reference sentence. However, instead of exact matches, we compute using contextual embeddings. evaluate outputs 363 machine translation and image captioning systems. correlates better human judgments provides stronger model selection performance than existing metrics. Finally, use adversarial paraphrase detection task show that is more robust challenging examples when compared

computer science, natural language processing, text generation, machine learning, text processing",2019,1625,computer science|natural language processing|text generation|machine learning|text processing,https://openalex.org/W2970641574
https://openalex.org/W2159525276,An overview of the KL-ONE Knowledge Representation System,"An overview of the KL-ONE Knowledge Representation System

KL-ONE is a system for representing knowledge in Artificial Intelligence programs. It has been developed and refined over long period used both basic research implemented knowledge-based systems number of places the AI community. Here we present kernel ideas KL-ONE, emphasizing its ability to form complex structured descriptions. In addition detailing all KL-ONE's description-forming structures, discuss bit philosophy underlying system, highlight notions taxonomy classification that are central it, include an extended example use classifier recognition task.

computer science, knowledge representation and reasoning, knowledge-based system, natural language processing, knowledge integration, knowledge management",1985,1604,computer science|knowledge representation and reasoning|knowledge-based system|natural language processing|knowledge integration|knowledge management,
https://openalex.org/W1574968832,Using Corpora in Discourse Analysis,"Using Corpora in Discourse Analysis

This book examines approaches to carrying out discourse analysis (DA) using techniques that are grounded in corpus linguistics. Assuming no prior knowledge of corpora, the and evaluates a variety corpus-based methodologies including:

applied linguistics, textual practice, linguistics, natural language processing, semantic analysis (linguistics), language, syntactic parsing, communication, spoken language technology, discourse analysis, conversation analysis, discourse structure, interactional linguistics, narrative, rhetoric, corpus linguistics, pragmatic analysis",2006,1603,applied linguistics|textual practice|linguistics|natural language processing|semantic analysis (linguistics)|language|syntactic parsing|communication|spoken language technology|discourse analysis|conversation analysis|discourse structure|interactional linguistics|narrative|rhetoric|corpus linguistics|pragmatic analysis,
https://openalex.org/W2078861931,Automatic evaluation of machine translation quality using n-gram co-occurrence statistics,"Automatic evaluation of machine translation quality using n-gram co-occurrence statistics

Evaluation is recognized as an extremely helpful forcing function in Human Language Technology R&D. Unfortunately, evaluation has not been a very powerful tool machine translation (MT) research because it requires human judgments and thus expensive time-consuming easily factored into the MT agenda. However, at July 2001 TIDES PI meeting Philadelphia, IBM described automatic technique that can provide immediate feedback guidance research. Their idea, which they call ""evaluation understudy"", compares output with expert reference translations terms of statistics short sequences words (word N-grams). The more these N-grams shares translations, better judged to be. idea elegant its simplicity. But far important, showed strong correlation between automatically generated scores quality. As result, DARPA commissioned NIST develop facility based on work. This utility now available from serves primary measure for

machine translation quality, computer science, linguistics, language model, natural language processing, machine learning, n-gram co-occurrence statistics, computational linguistics, data science, machine translation, automatic classification, computer-assisted translation, automatic evaluation",2002,1601,machine translation quality|computer science|linguistics|language model|natural language processing|machine learning|n-gram co-occurrence statistics|computational linguistics|data science|machine translation|automatic classification|computer-assisted translation|automatic evaluation,https://openalex.org/W2146574666|https://openalex.org/W2936695845|https://openalex.org/W2150824314
https://openalex.org/W2550821151,Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation,"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation

We propose a simple solution to use single Neural Machine Translation (NMT) model translate between multiple languages. Our requires no changes the architecture from standard NMT system but instead introduces an artificial token at beginning of input sentence specify required target language. Using shared wordpiece vocabulary, our approach enables Multilingual systems using model. On WMT’14 benchmarks, multilingual achieves comparable performance for English→French and surpasses state-of-theart results English→German. Similarly, state-of-the-art French→English German→English on WMT’15 respectively. production corpora, models up twelve language pairs allow better translation many individual pairs. can also learn perform implicit bridging never seen explicitly during training, showing that transfer learning zero-shot is possible neural translation. Finally, we show analyses hints universal interlingua representation in some interesting examples when mixing

computer science, linguistics, natural language processing, language, neural machine translation, machine learning, multilingual pretraining, zero-shot learning, multimodal translation, zero-shot translation, deep learning, machine translation, translation studies, multilingualism, computer-assisted translation",2017,1586,computer science|linguistics|natural language processing|language|neural machine translation|machine learning|multilingual pretraining|zero-shot learning|multimodal translation|zero-shot translation|deep learning|machine translation|translation studies|multilingualism|computer-assisted translation,https://openalex.org/W3035390927|https://openalex.org/W2963250244|https://openalex.org/W3019166713
https://openalex.org/W2158108973,Domain adaptation with structural correspondence learning,"Domain adaptation with structural correspondence learning

Discriminative learning methods are widely used in natural language processing. These work best when their training and test data drawn from the same distribution. For many NLP tasks, however, we confronted with new domains which labeled is scarce or non-existent. In such cases, seek to adapt existing models a resource-rich source domain resource-poor target domain. We introduce structural correspondence automatically induce correspondences among features different domains. our technique on part of speech tagging show performance gains for varying amounts data, as well improvements parsing accuracy using improved tagger.

computer science, transfer learning, structural correspondence learning, natural language processing, domain adaptation, clustering, machine learning, geometric learning, data science, systems engineering, deep learning, machine learning research, semantic learning, algorithmic learning",2006,1570,computer science|transfer learning|structural correspondence learning|natural language processing|domain adaptation|clustering|machine learning|geometric learning|data science|systems engineering|deep learning|machine learning research|semantic learning|algorithmic learning,https://openalex.org/W2896457183|https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2163302275|https://openalex.org/W22861983|https://openalex.org/W658020064
https://openalex.org/W2914120296,Cross-lingual Language Model Pretraining,"Cross-lingual Language Model Pretraining

Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend approach to multiple languages and show effectiveness cross-lingual pretraining. We propose two methods learn models (XLMs): one unsupervised that only relies on monolingual data, supervised leverages parallel data with a new model objective. obtain state-of-the-art results classification, machine translation. On XNLI, our pushes state art by an absolute gain 4.9% accuracy. translation, 34.3 BLEU WMT'16 German-English, improving previous more than 9 BLEU. 38.5 Romanian-English, outperforming best 4 Our code pretrained will be made publicly available.

cross-lingual representation, cross-lingual language model, natural language processing, machine translation, retrieval augmented generation, computer science, large language model, language model, cross-language retrieval, multilingual pretraining, linguistics, language, computational linguistics",2019,1568,cross-lingual representation|cross-lingual language model|natural language processing|machine translation|retrieval augmented generation|computer science|large language model|language model|cross-language retrieval|multilingual pretraining|linguistics|language|computational linguistics,https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W3035390927|https://openalex.org/W3030163527|https://openalex.org/W2936695845|https://openalex.org/W2966715458|https://openalex.org/W3019166713|https://openalex.org/W3169483174
https://openalex.org/W1578856370,"Spoken Language Processing: A Guide to Theory, Algorithm, and System Development","Spoken Language Processing: A Guide to Theory, Algorithm, and System Development

From the Publisher:

New advances in spoken language processing: theory and practice
In-depth coverage of speech processing, recognition, synthesis, understanding, interface design
Many case studies from state-of-the-art systems, including examples Microsoft's advanced research labs


Spoken Language Processing draws on latest techniques multiple fields: computer science, electrical engineering, acoustics, linguistics, mathematics, psychology, beyond. Starting with fundamentals, it presents all this more: 

Essential background production perception, probability information theory, pattern recognition
Extracting signal: useful representations practical compression solutions
Modern recognition techniques: hidden Markov models, acoustic modeling, improving resistance to environmental noises, search algorithms, large vocabulary recognition
Text-to-speech: analyzing documents, pitch duration controls; trainable more
Spoken understanding: dialog management, applications, multimodal interfaces 


To illustrate book's methods, authors present detailed based Whisper recognizer, Whistler text-to-speech system, Dr. Who MiPad handheld device. Whether you're planning, designing, building, or purchasing technology, is state artfromalgorithms through business productivity.

computer science, linguistics, speech recognition, process development, language model, natural language processing, language, speech processing, speech analysis, spoken language processing, computational linguistics, speech communication, speech technology, spoken language technology, system development",2001,1566,computer science|linguistics|speech recognition|process development|language model|natural language processing|language|speech processing|speech analysis|spoken language processing|computational linguistics|speech communication|speech technology|spoken language technology|system development,https://openalex.org/W2963206148
https://openalex.org/W2166776180,Automatic retrieval and clustering of similar words,"Automatic retrieval and clustering of similar words

Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on distributional pattern words. The allows us to construct thesaurus using parsed corpus. then present new evaluation methodology for automatically constructed thesaurus. results show that significantly closer WordNet than Roget Thesaurus is.

linguistics, computer science, document clustering, clustering, semantic similarity, automatic retrieval, natural language processing, corpus linguistics, computational linguistics, text mining, similar words, similarity search",1998,1564,linguistics|computer science|document clustering|clustering|semantic similarity|automatic retrieval|natural language processing|corpus linguistics|computational linguistics|text mining|similar words|similarity search,https://openalex.org/W2108646579|https://openalex.org/W2020278455|https://openalex.org/W2436001372
https://openalex.org/W2171928131,Extensions of recurrent neural network language model,"Extensions of recurrent neural network language model

We present several modifications of the original recurrent neural network language model (RNN LM).While this has been shown to significantly outperform many competitive modeling techniques in terms accuracy, remaining problem is computational complexity. In work, we show approaches that lead more than 15 times speedup for both training and testing phases. Next, importance using a backpropagation through time algorithm. An empirical comparison with feedforward networks also provided. end, discuss possibilities how reduce amount parameters model. The resulting RNN can thus be smaller, faster during testing, accurate basic one.

computer science, linguistics, large language model, language model, natural language processing, language, language network, neural machine translation, machine learning, recurrent neural network, computational intelligence, deep learning, machine learning research, neural network (machine learning), language learning",2011,1556,computer science|linguistics|large language model|language model|natural language processing|language|language network|neural machine translation|machine learning|recurrent neural network|computational intelligence|deep learning|machine learning research|neural network (machine learning)|language learning,https://openalex.org/W2153579005|https://openalex.org/W2950577311|https://openalex.org/W2120615054|https://openalex.org/W2884001105|https://openalex.org/W2402268235|https://openalex.org/W2963223306|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2963042536|https://openalex.org/W2962706528
https://openalex.org/W2561715562,CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning

When building artificial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress discover short-comings. Existing benchmarks for question answering help, but have strong biases models exploit correctly without reasoning. They also conflate multiple sources of error, making it hard pinpoint model weaknesses. We present a dataset range reasoning abilities. It contains minimal has detailed annotations describing the kind each requires. use this variety modern systems, providing novel insights into their abilities limitations.

computer science, vision language model, automated reasoning, visual science, principal component analysis, visual reasoning, diagnostic dataset, neuroscience, cognitive science, image representation, computer-aided diagnosis, language model, deep learning, machine vision, visual question answering, natural language processing, computer vision, knowledge discovery, compositional language, visual perception, elementary visual reasoning",2017,1551,computer science|vision language model|automated reasoning|visual science|principal component analysis|visual reasoning|diagnostic dataset|neuroscience|cognitive science|image representation|computer-aided diagnosis|language model|deep learning|machine vision|visual question answering|natural language processing|computer vision|knowledge discovery|compositional language|visual perception|elementary visual reasoning,
https://openalex.org/W3036601975,wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations

We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed can outperform best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks input in latent space and solves a contrastive task defined over quantization of which are jointly learned. Experiments using all labeled data Librispeech achieve 1.8/3.3 WER clean/other test sets. When lowering amount to one hour, outperforms previous state art 100 hour subset times less data. Using just ten minutes pre-training 53k hours unlabeled still achieves 4.8/8.2 WER. This demonstrates feasibility recognition with limited amounts

self-supervised learning, speech analysis, spoken language technology, speech representations, neuroscience, natural language processing, speech science, speech processing, cognitive science, auditory modeling, computer science, language model, machine learning, speech recognition, deep learning, speech communication, voice recognition",2020,1548,self-supervised learning|speech analysis|spoken language technology|speech representations|neuroscience|natural language processing|speech science|speech processing|cognitive science|auditory modeling|computer science|language model|machine learning|speech recognition|deep learning|speech communication|voice recognition,
https://openalex.org/W2117400858,Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging,"Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging

Recently, there has been a rebirth of empiricism in the field natural language processing. Manual encoding linguistic information is being challenged by automated corpus-based learning as method providing processing system with knowledge. Although approaches have successful many different areas processing, it often case that these methods capture they are modelling indirectly large opaque tables statistics. This can make difficult to analyze, understand and improve ability model underlying behavior. In this paper, we will describe simple rule-based approach shown for number tasks clearer more direct fashion without compromise performance. We present detailed study applied part-of-speech tagging.

language, part-of-speech tagging, transformation-based error-driven learning, natural language processing",1995,1546,language|part-of-speech tagging|transformation-based error-driven learning|natural language processing,https://openalex.org/W1996430422|https://openalex.org/W1916559533|https://openalex.org/W2161793142
https://openalex.org/W2013942038,Models of reading aloud: Dual-route and parallel-distributed-processing approaches.,"Models of reading aloud: Dual-route and parallel-distributed-processing approaches.

It has often been argued that various facts about skilled reading aloud cannot be explained by any model unless possesses a dual-route architecture (lexical and nonlexical routes from print to speech). This broad claim challenged Seidenberg McClelland (1989, 1990). Their but single route speech, yet, they contend, it can account for major have hitherto claimed require architecture. The authors identify 6 of these reading. 1-route proposed the first not remaining 5. Because models with architectures explain all basic reading, suggest this remains viable tenable learning read. cascaded model, computational version is described

spoken language technology, spoken language processing, natural language processing, parallel computing, speech processing, computer science, language model, speech recognition, speech communication, reading research",1993,1545,spoken language technology|spoken language processing|natural language processing|parallel computing|speech processing|computer science|language model|speech recognition|speech communication|reading research,https://openalex.org/W2022963108|https://openalex.org/W2161070585|https://openalex.org/W2016534914
https://openalex.org/W3099700870,Dense Passage Retrieval for Open-Domain Question Answering,"Dense Passage Retrieval for Open-Domain Question Answering

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.

retrieval technique, question answering, image analysis, computer science, computer vision, knowledge discovery, clustering, open-domain question, dense passage retrieval, multimedia retrieval, machine learning research, information retrieval, nlp task, natural language processing, data science, interactive information retrieval, machine vision, content similarity detection",2020,1545,retrieval technique|question answering|image analysis|computer science|computer vision|knowledge discovery|clustering|open-domain question|dense passage retrieval|multimedia retrieval|machine learning research|information retrieval|nlp task|natural language processing|data science|interactive information retrieval|machine vision|content similarity detection,
https://openalex.org/W1989484209,Bags of Binary Words for Fast Place Recognition in Image Sequences,"Bags of Binary Words for Fast Place Recognition in Image Sequences

We propose a novel method for visual place recognition using bag of words obtained from accelerated segment test (FAST)+BRIEF features. For the first time, we build vocabulary tree that discretizes binary descriptor space and use to speed up correspondences geometrical verification. present competitive results with no false positives in very different datasets, exactly same settings. The whole technique, including feature extraction, requires 22 ms/frame sequence 26 300 images is one order magnitude faster than previous approaches.

pattern recognition, computer science, fast place recognition, automatic classification, image analysis, image sequences, information fusion, image retrieval, object detection, data science, image representation, machine learning research, machine vision, digital image processing, natural language processing, object recognition, scene interpretation, computer vision, applied mathematics, multimedia retrieval, binary words",2012,1539,pattern recognition|computer science|fast place recognition|automatic classification|image analysis|image sequences|information fusion|image retrieval|object detection|data science|image representation|machine learning research|machine vision|digital image processing|natural language processing|object recognition|scene interpretation|computer vision|applied mathematics|multimedia retrieval|binary words,
https://openalex.org/W1535015163,A maximum-entropy-inspired parser,"A maximum-entropy-inspired parser

We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall sentences of length 40 and less, 89.5% 100 less when trained tested on the previously established [5, 9, 10, 15, 17] standard sections Wall Street Journal treebank. This represents 13% decrease in error rate over best single-parser results this corpus [9]. The major technical innovation is use maximum-entropy-inspired model conditioning smoothing let us successfully test combine many different events. also some partial showing effects information, including surprising 2% improvement due guessing lexical head's pre-terminal before head.

computer science, maximum-entropy-inspired parser, natural language processing, semantic parsing, syntactic parsing",2000,1535,computer science|maximum-entropy-inspired parser|natural language processing|semantic parsing|syntactic parsing,https://openalex.org/W2158899491|https://openalex.org/W2097606805|https://openalex.org/W2092654472
https://openalex.org/W2605376995,The Stroop Color and Word Test,"The Stroop Color and Word Test

The Stroop Colour and Word Test (SCWT) is a neuropsychological test extensively used to assess the ability inhibit cognitive interference that occurs when processing of specific stimulus feature impedes simultaneous second attribute, well-known as Effect. aim present work verify theoretical adequacy various scoring methods measure effect. We systematic review studies have provided normative data for SCWT. referred both electronic databases (i.e. PubMed, Scopus, Google Scholar) citations. Our findings show while several been reported in literature, none reviewed enables us fully Furthermore, we discuss from Italian panorama literature. claim an alternative method which takes into consideration speed accuracy response. Finally, underline importance assessing performance all conditions (word reading, colour naming, named colour-word).

stroop color, language grounding, linguistics, psycholinguistics, text mining, language model, natural language processing, language, language assessment, language testing, communication, semantic evaluation, colorimetry, word test, computational linguistics, direct observation, colorization, color constancy",2017,1528,stroop color|language grounding|linguistics|psycholinguistics|text mining|language model|natural language processing|language|language assessment|language testing|communication|semantic evaluation|colorimetry|word test|computational linguistics|direct observation|colorization|color constancy,
https://openalex.org/W1591706642,A Neural Conversational Model,"A Neural Conversational Model

Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking airline ticket) require hand-crafted rules. In this paper, we present a simple approach for which uses the recently proposed sequence framework. Our model converses by predicting next sentence given or sentences conversation. The strength of our that it can be trained end-to-end thus requires much fewer We find straightforward generate conversations large conversational training dataset. preliminary results suggest that, despite optimizing wrong objective function, able converse well. It extract knowledge from both domain dataset, large, noisy, general dataset movie subtitles. On domain-specific IT helpdesk solution technical problem via conversations. noisy open-domain transcript perform forms common sense reasoning. As expected, also lack consistency failure mode model.

computer science, human-computer interaction, language model, natural language processing, speech processing, spoken dialog system, communication, spoken language technology, cognitive science, conversation analysis, neural computation, dialogue management, nlp task, speech communication, verbal interaction, neural network (machine learning), neural conversational model",2015,1527,computer science|human-computer interaction|language model|natural language processing|speech processing|spoken dialog system|communication|spoken language technology|cognitive science|conversation analysis|neural computation|dialogue management|nlp task|speech communication|verbal interaction|neural network (machine learning)|neural conversational model,https://openalex.org/W2963250244|https://openalex.org/W2884001105|https://openalex.org/W2963206148|https://openalex.org/W2962883855|https://openalex.org/W2742947407|https://openalex.org/W2963167310
https://openalex.org/W1533946607,N-gram-based text categorization,"N-gram-based text categorization

Text categorization is a fundamental task in document processing, allowing the automated handling of enormous streams documents electronic form. One difficulty some classes presence different kinds textual errors, such as spelling and grammatical errors email, character recognition that come through OCR. must work reliably on all input, thus tolerate level these problems. We describe here an N-gram-based approach to text tolerant errors. The system small, fast robust. This worked very well for language classification, achieving one test 99.8% correct classification rate Usenet newsgroup articles written languages. also reasonably classifying from number computer-oriented newsgroups according subject, high 80% rate. There are several obvious directions improving system`s performance those cases where it did not do well. based calculating comparing profiles N-gram frequencies. First, we use compute training set data represent variousmore » categories, e.g., samples or content samples. Then computes profile particular be classified. Finally, distance measure between document`s each category profiles. selects whose has smallest profile. involved quite typically 10K bytes set, less than 4K individual document. Using frequency provides simple reliable way categorize wide range tasks.« less

computer science, linguistics, disinformation detection, text mining, natural language processing, keyword extraction, n-gram-based text categorization, machine learning, computational linguistics, data science, document classification, deep learning, nlp task, machine learning research, automatic classification",1994,1526,computer science|linguistics|disinformation detection|text mining|natural language processing|keyword extraction|n-gram-based text categorization|machine learning|computational linguistics|data science|document classification|deep learning|nlp task|machine learning research|automatic classification,https://openalex.org/W2118020653|https://openalex.org/W2161793142
https://openalex.org/W2126581182,Sentiment Analysis and Subjectivity,"Sentiment Analysis and Subjectivity

Textual information in the world can be broadly categorized into two main types: facts and opinions. Facts are objective expressions about entities, events their properties. Opinions usually subjective that describe people’s sentiments, appraisals or feelings toward The concept of opinion is very broad. In this chapter, we only focus on convey positive negative sentiments. Much existing research textual processing has been focused mining retrieval factual information, e.g., retrieval, Web search, text classification, clustering many other natural language tasks. Little work had done opinions until recently. Yet, so important whenever need to make a decision want hear others’ This not true for individuals but also organizations. One reasons lack study fact there was little opinionated available before World Wide Web. Before Web, when an individual needed decision, he/she typically asked from friends families. When organization wanted find sentiments general public its products services, it conducted polls, surveys, groups. However, with especially explosive growth usergenerated content past few years, transformed. dramatically changed way people express views They now post reviews at merchant sites almost anything Internet forums, discussion groups, blogs, which collectively called user-generated content. online wordof-mouth behavior represents new measurable sources practical applications. Now if one wants buy product, no longer limited asking his/her families because product give users product. For company, may necessary conduct organize groups employ external consultants order consumer those competitors already them such information.

linguistics, opinion aggregation, literary criticism, narrative, content analysis, affective neuroscience, disinformation detection, speech analysis, communication, semantic evaluation, discourse analysis, psychology, film criticism, semantic analysis (linguistics), multimodal sentiment analysis, text mining, natural language processing, affective computing, sentiment analysis",2010,1525,linguistics|opinion aggregation|literary criticism|narrative|content analysis|affective neuroscience|disinformation detection|speech analysis|communication|semantic evaluation|discourse analysis|psychology|film criticism|semantic analysis (linguistics)|multimodal sentiment analysis|text mining|natural language processing|affective computing|sentiment analysis,https://openalex.org/W2108646579|https://openalex.org/W2031998113
https://openalex.org/W2169818249,Natural language processing: an introduction,"Natural language processing: an introduction

Objectives To provide an overview and tutorial of natural language processing (NLP) modern NLP-system design.

computer science, linguistics, text mining, natural language processing, language engineering, information extraction, machine learning, computational linguistics, nlp task, machine learning research",2011,1520,computer science|linguistics|text mining|natural language processing|language engineering|information extraction|machine learning|computational linguistics|nlp task|machine learning research,https://openalex.org/W3019166713
https://openalex.org/W2912924812,Natural Questions: A Benchmark for Question Answering Research,"Natural Questions: A Benchmark for Question Answering Research

We present the Natural Questions corpus, a question answering data set. consist of real anonymized, aggregated queries issued to Google search engine. An annotator is presented with along Wikipedia page from top 5 results, and annotates long answer (typically paragraph) short (one or more entities) if on page, marks null no long/short present. The public release consists 307,373 training examples single annotations; 7,830 5-way annotations for development data; further 7,842 annotated sequestered as test data. experiments validating quality also describe analysis 25-way 302 examples, giving insights into human variability annotation task. introduce robust metrics purposes evaluating systems; demonstrate high upper bounds these metrics; establish baseline results using competitive methods drawn related literature.

natural questions, natural language processing, data science, nlp task, machine learning research, benchmark datasets, question answering",2019,1518,natural questions|natural language processing|data science|nlp task|machine learning research|benchmark datasets|question answering,https://openalex.org/W3030163527|https://openalex.org/W3099700870|https://openalex.org/W3011411500|https://openalex.org/W2970476646
https://openalex.org/W22861983,Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach,"Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach

The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic academic industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all them. Hence, this paper studies problem domain adaptation classifiers, hereby a system trained on labeled from one source but meant be deployed another. We propose deep learning approach which learns extract meaningful representation each review unsupervised fashion. Sentiment classifiers with high-level feature clearly outperform state-of-the-art methods benchmark composed 4 types Amazon products. Furthermore, method scales well allowed us successfully perform larger industrial-strength dataset 22 domains.

computer science, text mining, natural language processing, domain adaptation, machine learning, large-scale sentiment classification, data science, deep learning, content similarity detection, machine learning research, large-scale datasets",2011,1518,computer science|text mining|natural language processing|domain adaptation|machine learning|large-scale sentiment classification|data science|deep learning|content similarity detection|machine learning research|large-scale datasets,https://openalex.org/W2153579005|https://openalex.org/W2884001105|https://openalex.org/W658020064|https://openalex.org/W2250966211|https://openalex.org/W2742947407|https://openalex.org/W2250879510|https://openalex.org/W2306941105
https://openalex.org/W2125031621,Neural Word Embedding as Implicit Matrix Factorization,"Neural Word Embedding as Implicit Matrix Factorization

We analyze skip-gram with negative-sampling (SGNS), a word embedding method introduced by Mikolov et al., and show that it is implicitly factorizing word-context matrix, whose cells are the pointwise mutual information (PMI) of respective context pairs, shifted global constant. find another method, NCE, similar where each cell (shifted) log conditional probability given its context. using sparse Shifted Positive PMI matrix to represent words improves results on two similarity tasks one analogy tasks. When dense low-dimensional vectors preferred, exact factorization SVD can achieve solutions at least as good SGNS's for On questions SGNS remains superior SVD. conjecture this stems from weighted nature factorization.

word embeddings, computer science, natural language processing, neural word embedding, machine learning, cognitive science, neural computation, semantic processing, deep learning, matrix factorization, machine learning research, implicit matrix factorization, principal component analysis",2014,1517,word embeddings|computer science|natural language processing|neural word embedding|machine learning|cognitive science|neural computation|semantic processing|deep learning|matrix factorization|machine learning research|implicit matrix factorization|principal component analysis,https://openalex.org/W2963042536
https://openalex.org/W2150824314,Automatic evaluation of summaries using N-gram co-occurrence statistics,"Automatic evaluation of summaries using N-gram co-occurrence statistics

Following the recent adoption by machine translation community of automatic evaluation using BLEU/NIST scoring process, we conduct an in-depth study a similar idea for evaluating summaries. The results show that unigram co-occurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application BLEU procedure does not always give good results.

text mining, natural language processing, automatic summarization, n-gram co-occurrence statistics, statistics, machine learning research, automatic classification, automatic evaluation",2003,1511,text mining|natural language processing|automatic summarization|n-gram co-occurrence statistics|statistics|machine learning research|automatic classification|automatic evaluation,https://openalex.org/W2154652894|https://openalex.org/W1525595230|https://openalex.org/W3101913037|https://openalex.org/W2888482885
https://openalex.org/W3011411500,SpanBERT: Improving Pre-training by Representing and Predicting Spans,"SpanBERT: Improving Pre-training by Representing and Predicting Spans

We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than tokens, (2) training the span boundary representations entire content masked span, without relying on individual token within it. SpanBERT consistently outperforms our better-tuned baselines, with substantial gains selection tasks such as question answering coreference resolution. In particular, same data model size large , single obtains 94.6% 88.7% F1 SQuAD 1.1 2.0 respectively. also achieve new state art OntoNotes resolution task (79.6% F1), strong performance TACRED relation extraction benchmark, even GLUE. 1

image analysis, computer science, human-computer interaction, supervised learning, transfer learning, language model, natural language processing, natural language generation, predictive learning, machine learning, multilingual pretraining, cognitive science, data science, knowledge discovery, deep learning, multilinear subspace learning, machine learning research, machine vision",2020,1503,image analysis|computer science|human-computer interaction|supervised learning|transfer learning|language model|natural language processing|natural language generation|predictive learning|machine learning|multilingual pretraining|cognitive science|data science|knowledge discovery|deep learning|multilinear subspace learning|machine learning research|machine vision,https://openalex.org/W2979826702|https://openalex.org/W3034999214|https://openalex.org/W2980282514|https://openalex.org/W3098824823
https://openalex.org/W4393849379,The World Atlas of Language Structures Online,"The World Atlas of Language Structures Online

Cite the source of dataset as: Dryer, Matthew S. &amp; Haspelmath, Martin (eds.) 2013. The World Atlas Language Structures Online. Leipzig: Max Planck Institute for Evolutionary Anthropology. (Available online at https://wals.info)

lexical resource, linguistics, language documentation, discourse structure, language structures, language resource, world english, structural linguistics, reference data, structured vocabulary, world atlas, language, natural language processing, corpus linguistics, computational linguistics, syntactic structure",2022,1498,lexical resource|linguistics|language documentation|discourse structure|language structures|language resource|world english|structural linguistics|reference data|structured vocabulary|world atlas|language|natural language processing|corpus linguistics|computational linguistics|syntactic structure,
https://openalex.org/W2151322535,"Technologies, Texts and Affordances","Technologies, Texts and Affordances

In contrast to recent sociological emphases on the social shaping of technology, this article proposes and illustrates a way analysing technological sociality. Drawing concept affordances (Gibson 1979), argues for recognition constraining, as well enabling, materiality artefacts. The argument is set in theoretical context one most comprehensive statements anti-essentialism (Grint Woolgar 1997). position illustrated through reinterpretation some case studies used by proponents radical constructivist position.

technology, human-computer interaction, spoken language technology, textual practice, information technology, natural language processing, digital humanities, language technology, educational technology, science and technology studies, philosophy of technology, communication",2001,1495,technology|human-computer interaction|spoken language technology|textual practice|information technology|natural language processing|digital humanities|language technology|educational technology|science and technology studies|philosophy of technology|communication,
https://openalex.org/W2432004435,Improved Techniques for Training GANs,"Improved Techniques for Training GANs

We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. focus on two applications GANs: semi-supervised learning, generation images humans find visually realistic. Unlike most work models, our primary goal is not train model assigns high likelihood test data, nor do require be able learn well without using any labels. Using techniques, achieve state-of-the-art results in classification MNIST, CIFAR-10 SVHN. The generated are quality as confirmed by visual Turing test: generates MNIST samples cannot distinguish from real yield human error rate 21.3%. also ImageNet with unprecedented resolution show methods enable recognizable classes.

training gans, recommender system, machine learning, computer science, generative adversarial network, adversarial machine learning, generative ai, natural language processing, deep learning, neural network (machine learning), aural augmentation, neural computation",2016,1485,training gans|recommender system|machine learning|computer science|generative adversarial network|adversarial machine learning|generative ai|natural language processing|deep learning|neural network (machine learning)|aural augmentation|neural computation,https://openalex.org/W2962879692|https://openalex.org/W2605135824
https://openalex.org/W658020064,From Word Embeddings To Document Distances,"From Word Embeddings To Document Distances

We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences sentences. The WMD measures dissimilarity two documents as minimum amount of embedded one document need to travel reach another document. show this metric can be cast an instance Earth Distance, well studied transportation problem which several highly efficient solvers have been developed. has no hyperparameters and straight-forward implement. Further, we demonstrate eight real world classification data sets, comparison with seven state-of-the-art baselines, leads unprecedented low k-nearest neighbor error rates.

word embeddings, computer science, natural language processing",2015,1474,word embeddings|computer science|natural language processing,https://openalex.org/W2936695845
https://openalex.org/W2964265128,Convolutional Sequence to Sequence Learning,"Convolutional Sequence to Sequence Learning

The prevalent approach to sequence learning maps an input a variable length output via recurrent neural networks. We introduce architecture based entirely on convolutional Compared models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities fixed independent length. Our use gated linear units eases gradient propagation we equip each decoder layer with separate attention module. outperform accuracy deep LSTM setup Wu et al. (2016) both WMT'14 English-German English-French translation at order magnitude faster speed, GPU CPU.

automatic classification, machine learning, computer science, sequential learning, convolutional sequence, natural language processing, sequence modelling, convolutional neural network, data science, deep learning, pattern recognition, cognitive science",2017,1470,automatic classification|machine learning|computer science|sequential learning|convolutional sequence|natural language processing|sequence modelling|convolutional neural network|data science|deep learning|pattern recognition|cognitive science,https://openalex.org/W2884001105|https://openalex.org/W2936695845|https://openalex.org/W3019166713|https://openalex.org/W2888482885
https://openalex.org/W1934041838,Improved backing-off for M-gram language modeling,"Improved backing-off for M-gram language modeling

In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. case of unseen events this backs off less specific distribution. paper we propose use distributions which are especially optimized for task backing-off. Two different theoretical derivations lead quite from probability that usually Experiments show an improvement about 10% in terms perplexity and 5% word error rate.

computer science, large language model, language model, natural language processing, language, m-gram language modeling, machine learning, computational linguistics, computational semantics, deep learning, machine translation",2002,1470,computer science|large language model|language model|natural language processing|language|m-gram language modeling|machine learning|computational linguistics|computational semantics|deep learning|machine translation,https://openalex.org/W2402268235|https://openalex.org/W2963970792
https://openalex.org/W2169213601,Relevance-Based Language Models,"Relevance-Based Language Models

We explore the relation between classical probabilistic models of information retrieval and emerging language modeling approaches. It has long been recognized that primary obstacle to effective performance is need estimate a relevance model: probabilities words in relevant class. propose novel technique for estimating these using query alone. demonstrate our can produce highly accurate models, addressing important notions synonymy polysemy. Our experiments show outperforming baseline systems on TREC TDT tracking tasks. The main contribution this work an formal method model with no training data.

relevance logic, topic model, text mining, natural language processing, language model, language, large language model, machine learning, clustering, relevance feedback, relevance-based language models, nlp task, machine learning research",2017,1470,relevance logic|topic model|text mining|natural language processing|language model|language|large language model|machine learning|clustering|relevance feedback|relevance-based language models|nlp task|machine learning research,
https://openalex.org/W2963847595,Explaining Explanations: An Overview of Interpretability of Machine Learning,"Explaining Explanations: An Overview of Interpretability of Machine Learning

There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior thought processes. XAI allows users parts internal system to be more transparent, providing explanations decisions some level detail. These are ensure algorithmic fairness, identify potential bias/problems training data, perform as expected. However, produced by these systems is neither standardized nor systematically assessed. In an effort create best practices open challenges, we describe foundational concepts explainability show how they can used classify existing literature. We discuss why current approaches methods especially for deep neural networks insufficient. Finally, based on our survey, conclude with suggested future directions intelligence.

knowledge representation and reasoning, computer science, machine learning, interpretation, knowledge discovery, interpretability, computational learning theory, artificial intelligence, machine learning research, explanation-based learning, explainable ai, natural language processing, data science, model-based reasoning, automated reasoning",2018,1464,knowledge representation and reasoning|computer science|machine learning|interpretation|knowledge discovery|interpretability|computational learning theory|artificial intelligence|machine learning research|explanation-based learning|explainable ai|natural language processing|data science|model-based reasoning|automated reasoning,
https://openalex.org/W2142159465,Detecting text in natural scenes with stroke width transform,"Detecting text in natural scenes with stroke width transform

We present a novel image operator that seeks to find the value of stroke width for each pixel, and demonstrate its use on task text detection in natural images. The suggested is local data dependent, which makes it fast robust enough eliminate need multi-scale computation or scanning windows. Extensive testing shows scheme outperforms latest published algorithms. Its simplicity allows algorithm detect texts many fonts languages.

computer science, character recognition, text recognition, text processing, image analysis, text segmentation, data science, image representation, stroke width transform, computational imaging, multimedia information processing, machine vision, digital image processing, text mining, natural language processing, computer vision, multimedia retrieval, natural scenes, text-to-image retrieval",2010,1459,computer science|character recognition|text recognition|text processing|image analysis|text segmentation|data science|image representation|stroke width transform|computational imaging|multimedia information processing|machine vision|digital image processing|text mining|natural language processing|computer vision|multimedia retrieval|natural scenes|text-to-image retrieval,https://openalex.org/W2605982830|https://openalex.org/W1998042868
https://openalex.org/W3034238904,Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks,"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks

Language models pretrained on text from a wide variety of sources form the foundation today’s NLP. In light success these broad-coverage models, we investigate whether it is still helpful to tailor model domain target task. We present study across four domains (biomedical and computer science publications, news, reviews) eight classification tasks, showing that second phase pretraining in-domain (domain-adaptive pretraining) leads performance gains, under both high- low-resource settings. Moreover, adapting task’s unlabeled data (task-adaptive improves even after domain-adaptive pretraining. Finally, show task corpus augmented using simple selection strategies an effective alternative, especially when resources for might be unavailable. Overall, consistently find multi-phase adaptive offers large gains in performance.

domain model, linguistics, computer science, machine learning, multilingual pretraining, domain adaptation, language, nlp task, adapt language models, natural language processing, language adaptation, language model",2020,1459,domain model|linguistics|computer science|machine learning|multilingual pretraining|domain adaptation|language|nlp task|adapt language models|natural language processing|language adaptation|language model,https://openalex.org/W3169483174
https://openalex.org/W2007709031,Computational linguistics in 1990,"Computational linguistics in 1990

Article Free Access Share on Computational linguistics in 1990 Author: Hans Karlgren KVAL Research Institute for Information Science, Stockholm, Sweden SwedenView Profile Authors Info & Claims COLING '90: Proceedings of the 13th conference - Volume 1August 1990Pages 97–99https://doi.org/10.3115/992507.992531Published:20 August 1990Publication History 1citation215DownloadsMetricsTotal Citations1Total Downloads215Last 12 Months22Last 6 weeks6 Get Citation AlertsNew Alert added!This alert has been successfully added and will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log to account Publisher SiteeReaderPDF

computer science, linguistics, natural language processing, language model, computational linguistics, computational semantics, machine translation",1990,1448,computer science|linguistics|natural language processing|language model|computational linguistics|computational semantics|machine translation,https://openalex.org/W2185175083
https://openalex.org/W2912512851,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics

The year 2000 gives Asia its first opportunity to host the annual meeting of Association for Computational Linguistics. ACL Executive Committee, under leadership Wolfgang Wahlster, selected one Asia's most exciting cities, Hong Kong, as venue, and organized an international conference committee including many representatives Asian countries. ACL-2000 also signifies a change in tenor global network communication access WWW languages grows with development practical applications enabling multilingual information access.The technical program covers all areas field, from theory methodology. There are four theme sessions, each own theme.

linguistics, text mining, language model, natural language processing, language engineering, computational linguistics, data science, computational semantics, machine translation, corpus linguistics",2020,1435,linguistics|text mining|language model|natural language processing|language engineering|computational linguistics|data science|computational semantics|machine translation|corpus linguistics,
https://openalex.org/W2962946486,Graph Convolutional Networks for Text Classification,"Graph Convolutional Networks for Text Classification

Text classification is an important and classical problem in natural language processing. There have been a number of studies that applied convolutional neural networks (convolution on regular grid, e.g., sequence) to classification. However, only limited explored the more flexible graph non-grid, arbitrary graph) for task. In this work, we propose use text We build single corpus based word co-occurrence document relations, then learn Graph Convolutional Network (Text GCN) corpus. Our GCN initialized with one-hot representation document, it jointly learns embeddings both words documents, as supervised by known class labels documents. experimental results multiple benchmark datasets demonstrate vanilla without any external or knowledge outperforms state-of-the-art methods On other hand, also predictive embeddings. addition, show improvement over comparison become prominent lower percentage training data, suggesting robustness less data

computer science, text mining, natural language processing, machine learning, document classification, data science, deep learning, graph convolutional networks, machine learning research, large-scale datasets, neural network (machine learning), text classification, graph neural network, automatic classification, graph processing",2019,1434,computer science|text mining|natural language processing|machine learning|document classification|data science|deep learning|graph convolutional networks|machine learning research|large-scale datasets|neural network (machine learning)|text classification|graph neural network|automatic classification|graph processing,
https://openalex.org/W2171278097,Building Watson: An Overview of the DeepQA Project,"Building Watson: An Overview of the DeepQA Project

IBM Research undertook a challenge to build computer system that could compete at the human champion level in real time on American TV quiz show, Jeopardy. The extent of includes fielding real‐time automatic contestant not merely laboratory exercise. Jeopardy Challenge helped us address requirements led design DeepQA architecture and implementation Watson. After three years intense research development by core team about 20 researchers, Watson is performing expert levels terms precision, confidence, speed show. Our results strongly suggest an effective extensible can be used as foundation for combining, deploying, evaluating, advancing wide range algorithmic techniques rapidly advance field question answering (QA).

computer science, artificial intelligence, machine learning, automated reasoning, knowledge representation and reasoning, semantic computing, natural language generation, model-based reasoning, data science, computational intelligence, computer engineering, deep learning, deepqa project, machine learning research, intelligent computing, large language model, natural language processing, computer-assisted reasoning, question answering",2010,1432,computer science|artificial intelligence|machine learning|automated reasoning|knowledge representation and reasoning|semantic computing|natural language generation|model-based reasoning|data science|computational intelligence|computer engineering|deep learning|deepqa project|machine learning research|intelligent computing|large language model|natural language processing|computer-assisted reasoning|question answering,https://openalex.org/W2963748441|https://openalex.org/W2277195237|https://openalex.org/W1552847225|https://openalex.org/W2963339397
https://openalex.org/W2534712034,Combining Local Context and WordNet Similarity for Word Sense Identification,"Combining Local Context and WordNet Similarity for Word Sense Identification

This chapter contains sections titled: Introducfion, Training and Testing Data, Experiment 1: The Local Context Classifier, 2: Measuring Word Similarity In Wordnet, 3: Combining Wordnet Measures, Conclusions, References

word embeddings, pattern recognition, linguistics, information fusion, natural language processing, semantic similarity, machine learning, word sense identification, local context, semantic processing, deep learning, wordnet similarity",1998,1429,word embeddings|pattern recognition|linguistics|information fusion|natural language processing|semantic similarity|machine learning|word sense identification|local context|semantic processing|deep learning|wordnet similarity,https://openalex.org/W2436001372
https://openalex.org/W2252136820,Semantic Parsing on Freebase from Question-Answer Pairs,"Semantic Parsing on Freebase from Question-Answer Pairs

In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive obtain at large scale, learn from question-answer pairs. The main challenge in setting narrowing down the huge number possible predicates for given question. We tackle problem two ways: First, build coarse mapping phrases using knowledge base and text corpus. Second, use bridging operation generate additional based neighboring predicates. On dataset Cai Yates (2013), despite not having our system outperforms their state-of-the-art parser. Additionally, collected more realistic challenging pairs improves over natural baseline.

semantic parsing, linguistics, question-answer pairs, natural language processing",2013,1421,semantic parsing|linguistics|question-answer pairs|natural language processing,https://openalex.org/W3030163527|https://openalex.org/W3099700870|https://openalex.org/W2963339397
https://openalex.org/W2134495021,An efficient context-free parsing algorithm,"An efficient context-free parsing algorithm

A parsing algorithm which seems to be the most efficient general context-free known is described. It similar both Knuth's LR( k ) and familiar top-down algorithm. has a time bound proportional n 3 (where length of string being parsed) in general; it an 2 for unambiguous grammars; runs linear on large class grammars, include practical programming language grammars. In empirical comparison appears superior bottom-up algorithms studied by Griffiths Petrick.

computer science, natural language processing, automated reasoning, computational linguistics, semantic parsing, information retrieval",1983,1421,computer science|natural language processing|automated reasoning|computational linguistics|semantic parsing|information retrieval,https://openalex.org/W2035032881|https://openalex.org/W2054125330
https://openalex.org/W2011035448,The architecture of the bilingual word recognition system: From identification to decision,"The architecture of the bilingual word recognition system: From identification to decision

The paper opens with an evaluation of the BIA model bilingual word recognition in light recent empirical evidence. After pointing out problems and omissions, a new model, called BIA+, is proposed. Structurally, this extends old one by adding phonological semantic lexical representations to available orthographic ones, assigns different role so-called language nodes. Furthermore, it makes distinction between effects non-linguistic context (such as instruction stimulus list composition) linguistic syntactic sentence context), based on identification system itself task/decision that regulates control. At end paper, generalizability BIA+ tasks modalities discussed.

computer science, linguistics, speech recognition, language model, natural language processing, language, character recognition, text recognition, cross-language retrieval, bilingual education, language recognition, language processing in the brain, cross-lingual representation, language learning, multilingualism, second language acquisition",2002,1421,computer science|linguistics|speech recognition|language model|natural language processing|language|character recognition|text recognition|cross-language retrieval|bilingual education|language recognition|language processing in the brain|cross-lingual representation|language learning|multilingualism|second language acquisition,
https://openalex.org/W2605982830,EAST: An Efficient and Accurate Scene Text Detector,"EAST: An Efficient and Accurate Scene Text Detector

Previous approaches for scene text detection have already achieved promising performances across various benchmarks. However, they usually fall short when dealing with challenging scenarios, even equipped deep neural network models, because the overall performance is determined by interplay of multiple stages and components in pipelines. In this work, we propose a simple yet powerful pipeline that yields fast accurate natural scenes. The directly predicts words or lines arbitrary orientations quadrilateral shapes full images, eliminating unnecessary intermediate steps (e.g., candidate aggregation word partitioning), single network. simplicity our allows concentrating efforts on designing loss functions architecture. Experiments standard datasets including ICDAR 2015, COCO-Text MSRA-TD500 demonstrate proposed algorithm significantly outperforms state-of-the-art methods terms both accuracy efficiency. On 2015 dataset, achieves an F-score 0.7820 at 13.2fps 720p resolution.

text recognition, image analysis, computer science, computer vision, machine learning, knowledge discovery, information fusion, text segmentation, computational imaging, computer engineering, scene understanding, natural language processing, image representation, data science, text processing, pattern recognition, character recognition, machine vision",2017,1420,text recognition|image analysis|computer science|computer vision|machine learning|knowledge discovery|information fusion|text segmentation|computational imaging|computer engineering|scene understanding|natural language processing|image representation|data science|text processing|pattern recognition|character recognition|machine vision,
https://openalex.org/W1965498649,Linda in context,"Linda in context

How can a system that differs sharply from all currently fashionable approaches score any kind of success? Here's how.

context model, linguistics, human-computer interaction, natural language processing, contextual issue, user context, communication, user experience, entity disambiguation, context management, narrative, corpus linguistics, context (linguistics)",1989,1416,context model|linguistics|human-computer interaction|natural language processing|contextual issue|user context|communication|user experience|entity disambiguation|context management|narrative|corpus linguistics|context (linguistics),
https://openalex.org/W2126725946,Exploiting Similarities among Languages for Machine Translation,"Exploiting Similarities among Languages for Machine Translation

Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate process generating extending dictionaries tables. Our translate missing word entries by learning language structures based on large monolingual data mapping between languages from small bilingual data. It uses distributed representation words learns linear vector spaces languages. Despite its simplicity, our is surprisingly effective: we achieve almost 90% precision@5 for English Spanish. makes little assumption about languages, so it be used to extend refine any pairs.

language, machine translation, natural language processing, translation studies",2013,1414,language|machine translation|natural language processing|translation studies,https://openalex.org/W2949547296|https://openalex.org/W3035390927|https://openalex.org/W2131744502|https://openalex.org/W2914120296|https://openalex.org/W2251803266
https://openalex.org/W1922655562,Deep Speech: Scaling up end-to-end speech recognition,"Deep Speech: Scaling up end-to-end speech recognition

We present a state-of-the-art speech recognition system developed using end-to-end deep learning. Our architecture is significantly simpler than traditional systems, which rely on laboriously engineered processing pipelines; these systems also tend to perform poorly when used in noisy environments. In contrast, our does not need hand-designed components model background noise, reverberation, or speaker variation, but instead directly learns function that robust such effects. do phoneme dictionary, nor even the concept of ""phoneme."" Key approach well-optimized RNN training uses multiple GPUs, as well set novel data synthesis techniques allow us efficiently obtain large amount varied for training. system, called Deep Speech, outperforms previously published results widely studied Switchboard Hub5'00, achieving 16.0% error full test set. Speech handles challenging environments better used, commercial systems.

end-to-end speech recognition, computer science, speech recognition, intelligent computing, deep speech, natural language processing, speech processing, machine learning, deep learning, speech technology, spoken language technology, computer engineering",2014,1411,end-to-end speech recognition|computer science|speech recognition|intelligent computing|deep speech|natural language processing|speech processing|machine learning|deep learning|speech technology|spoken language technology|computer engineering,https://openalex.org/W2327501763|https://openalex.org/W2193413348|https://openalex.org/W2962826786
https://openalex.org/W2091349213,Mental Spaces: Aspects of Meaning Construction in Natural Language,"Mental Spaces: Aspects of Meaning Construction in Natural Language

This book offers a highly original, integrated treatment of issues that play central role in linguistic semantics, philosophy language, and cognitive approaches to meaning.It is based on the idea expressions language are not interpreted directly via truth conditions; rather, at certain level they help build up mental spaces, internally structured linked one another. Because construction spaces typically underdetermined by expressions, simple principles yield multiple possibilities apparently complex ambiguities.Focusing constructions can be associated with rather than merely themselves, Fauconnier reveals general, uniform, elegant organization responsible for superficially diverse phenomena. A finding challenges several traditional widespread views meaning natural far-reaching implications: adequate theories reference cannot bypass space-construction process, standard arguments hidden structural levels invalidated.Gilles director studies Ecole des Hautes Etudes en Sciences Sociales Professor Linguistics University Paris VIII.A Bradford Book.

semantic representation, linguistics, psycholinguistics, cognitive linguistics, natural language processing, language, semantic evaluation, abstract interpretation, cognitive science, semantics, semantic parsing, narrative, language science, mental spaces, context (linguistics)",1985,1410,semantic representation|linguistics|psycholinguistics|cognitive linguistics|natural language processing|language|semantic evaluation|abstract interpretation|cognitive science|semantics|semantic parsing|narrative|language science|mental spaces|context (linguistics),
https://openalex.org/W2038043464,Exploring the Space of Topic Coherence Measures,"Exploring the Space of Topic Coherence Measures

Quantifying the coherence of a set statements is long standing problem with many potential applications that has attracted researchers from different sciences. The special case measuring topics been recently studied to remedy topic models give no guaranty on interpretablity their output. Several benchmark datasets were produced record human judgements interpretability topics. We are first propose framework allows construct existing word based measures as well new ones by combining elementary components. conduct systematic search space using all publicly available relevance data for evaluation. Our results show combinations components outperform respect correlation ratings. nFinally, we outline how our can be transferred further in context text mining, information retrieval and world wide web.

concept drift, topic model, text mining, natural language processing, topic coherence measures, machine learning, communication, nlp task, content similarity detection, machine learning research, document clustering",2015,1408,concept drift|topic model|text mining|natural language processing|topic coherence measures|machine learning|communication|nlp task|content similarity detection|machine learning research|document clustering,
https://openalex.org/W2154362705,NCBI GEO: mining tens of millions of expression profiles--database and tools update,"NCBI GEO: mining tens of millions of expression profiles--database and tools update

The Gene Expression Omnibus (GEO) repository at the National Center for Biotechnology Information (NCBI) archives and freely disseminates microarray other forms of high-throughput data generated by scientific community. database has a minimum information about experiment (MIAME)-compliant infrastructure that captures fully annotated raw processed data. Several deposit options formats are supported, including web forms, spreadsheets, XML Simple Format in Text (SOFT). In addition to storage, collection user-friendly web-based interfaces applications available help users effectively explore, visualize download thousands experiments tens millions gene expression patterns stored GEO. This paper provides summary GEO structure user facilities, describes recent enhancements design, performance, submission format options, query retrieval utilities. is accessible http://www.ncbi.nlm.nih.gov/geo/

computer science, machine learning, big data search, mining tens, fuzzy set, bioinformatics, information fusion, ncbi geo, data privacy, data science, frequent pattern mining, gene expression profiling, early detection, data mining, machine learning research, expression profiles, pattern mining, text mining, natural language processing, geography, knowledge discovery",2007,1408,computer science|machine learning|big data search|mining tens|fuzzy set|bioinformatics|information fusion|ncbi geo|data privacy|data science|frequent pattern mining|gene expression profiling|early detection|data mining|machine learning research|expression profiles|pattern mining|text mining|natural language processing|geography|knowledge discovery,
https://openalex.org/W2141766660,Inter-Coder Agreement for Computational Linguistics,"Inter-Coder Agreement for Computational Linguistics

This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions coefficients, covering Krippendorff's alpha as well Scott's pi Cohen's kappa; discusses use coefficients in several annotation tasks; argues that weighted, alpha-like traditionally less used than kappa-like measures computational linguistics, may be more appropriate many tasks—but their makes interpretation value coefficient even harder.

inter-coder agreement, computational linguistics, linguistics, natural language processing",2008,1407,inter-coder agreement|computational linguistics|linguistics|natural language processing,https://openalex.org/W4239946314|https://openalex.org/W1987425720
https://openalex.org/W2723293840,Deep Interest Network for Click-Through Rate Prediction,"Deep Interest Network for Click-Through Rate Prediction

Click-through rate prediction is an essential task in industrial applications, such as online advertising. Recently deep learning based models have been proposed, which follow a similar Embedding&MLP paradigm. In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed fixed-length vectors group-wise manner, finally concatenated together to fed multilayer perceptron (MLP) learn the nonlinear relations among features. this way, user compressed representation vector, regardless of what candidate ads are. The use vector will be bottleneck, brings difficulty for capture user's diverse interests effectively from rich historical behaviors. paper, we propose novel model: Deep Interest Network (DIN) tackles challenge by designing local activation unit adaptively behaviors with respect certain ad. This varies over different ads, improving expressive ability model greatly. Besides, develop two techniques: mini-batch aware regularization data adaptive function can help training networks hundreds millions parameters. Experiments on public datasets well Alibaba real production dataset 2 billion samples demonstrate effectiveness proposed approaches, achieve superior performance compared state-of-the-art methods. DIN now has successfully deployed display advertising system Alibaba, serving main traffic.

pattern recognition, computer science, parameter identification, machine learning, supervised learning, click-through rate prediction, deep interest network, recommender system, data science, online information, data mining, link prediction, performance prediction, motion detection, deep learning, machine learning research, machine vision, natural language processing, knowledge discovery",2018,1402,pattern recognition|computer science|parameter identification|machine learning|supervised learning|click-through rate prediction|deep interest network|recommender system|data science|online information|data mining|link prediction|performance prediction|motion detection|deep learning|machine learning research|machine vision|natural language processing|knowledge discovery,
https://openalex.org/W2405756170,Generative Adversarial Text to Image Synthesis,"Generative Adversarial Text to Image Synthesis

Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far this goal. However, in recent years generic powerful recurrent neural network architectures have been developed to learn discriminative feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) begun generate highly compelling specific categories, such as faces, album covers, room interiors. In work, we develop a novel architecture GAN formulation effectively bridge these advances image model- ing, translating visual concepts characters pixels. We demonstrate the capability our model plausible birds flowers detailed descriptions.

digital image processing, computer vision, generative ai, computational imaging, natural language processing, computational intelligence, computer science, generative adversarial text, synthetic image generation, machine learning, image synthesis, generative adversarial network, deep learning, data science, digital imaging",2016,1402,digital image processing|computer vision|generative ai|computational imaging|natural language processing|computational intelligence|computer science|generative adversarial text|synthetic image generation|machine learning|image synthesis|generative adversarial network|deep learning|data science|digital imaging,https://openalex.org/W2963444790|https://openalex.org/W2963966654
https://openalex.org/W1526308488,Efficiency and Complexity in Grammars,"Efficiency and Complexity in Grammars

This book addresses a question fundamental to any discussion of grammatical theory and variation: what extent can principles grammar be explained through language use? The argues that there is profound correspondence between performance data the fixed conventions grammars. Preferences patterns found in one, shows, are reflected constraints variation other. theoretical consequences proposed ‘performance-grammar hypothesis’ far-reaching — for current formalisms, innateness hypothesis, psycholinguistic models learning. Drawing on empirical generalizations insights from typology, generative grammar, psycholinguistics, historical linguistics, this demonstrates assumption grammars immune false. It presents detailed case studies arguments an alternative which has shaped thus world’s languages. language, argues, resides primarily mechanisms human beings have processing learning it.

computational complexity, computer science, linguistics, syntactic structure, natural language processing, language, grammar, syntax, automated deduction, grammatical formalism, automated reasoning, computational linguistics, grammar induction, categorial grammar, syntactic parsing",2004,1398,computational complexity|computer science|linguistics|syntactic structure|natural language processing|language|grammar|syntax|automated deduction|grammatical formalism|automated reasoning|computational linguistics|grammar induction|categorial grammar|syntactic parsing,
https://openalex.org/W2597655663,A Structured Self-attentive Sentence Embedding,"A Structured Self-attentive Sentence Embedding

This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using vector, we use 2-D matrix to represent the embedding, with each row attending on different part sentence. We also propose self-attention mechanism and special regularization term model. As side effect, comes easy way visualizing what specific parts are encoded into embedding. evaluate our 3 tasks: author profiling, sentiment classification, textual entailment. Results show that yields significant performance gain compared other methods in all tasks.

computer science, structured self-attentive sentence, natural language processing, language model, nlp task",2017,1385,computer science|structured self-attentive sentence|natural language processing|language model|nlp task,https://openalex.org/W2963918774
https://openalex.org/W2963966654,AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks,"AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks

In this paper, we propose an Attentional Generative Adversarial Network (AttnGAN) that allows attention-driven, multi-stage refinement for fine-grained text-to-image generation. With a novel attentional generative network, the AttnGAN can synthesize details at different sub-regions of image by paying attentions to relevant words in natural language description. addition, deep multimodal similarity model is proposed compute image-text matching loss training generator. The significantly outperforms previous state art, boosting best reported inception score 14.14% on CUB dataset and 170.25% more challenging COCO dataset. A detailed analysis also performed visualizing attention layers AttnGAN. It first time shows layered GAN able automatically select condition word level generating parts image.

pattern recognition, computer science, generative adversarial network, machine learning, generative ai, fine-grained text, synthetic image generation, natural language generation, cognitive science, data science, computational intelligence, image representation, generative system, image generation, computational imaging, generative model, deep learning, machine learning research, digital image processing, natural language processing",2018,1383,pattern recognition|computer science|generative adversarial network|machine learning|generative ai|fine-grained text|synthetic image generation|natural language generation|cognitive science|data science|computational intelligence|image representation|generative system|image generation|computational imaging|generative model|deep learning|machine learning research|digital image processing|natural language processing,
https://openalex.org/W2964236337,Deep learning for sentiment analysis: A survey,"Deep learning for sentiment analysis: A survey

Deep learning has emerged as a powerful machine technique that learns multiple layers of representations or features the data and produces state‐of‐the‐art prediction results. Along with success deep in many application domains, is also used sentiment analysis recent years. This paper gives an overview then provides comprehensive survey its current applications analysis. article categorized under: Fundamental Concepts Data Knowledge &gt; Algorithmic Development Text Mining

computer science, disinformation detection, natural language processing, machine learning, survey methodology, deep learning, sentiment analysis",2018,1377,computer science|disinformation detection|natural language processing|machine learning|survey methodology|deep learning|sentiment analysis,
https://openalex.org/W1560147776,Context‐Aware Recommender Systems,"Context‐Aware Recommender Systems

Context‐aware recommender systems (CARS) generate more relevant recommendations by adapting them to the specific contextual situation of user. This article explores how information can be used create intelligent and useful systems. It provides an overview multifaceted notion context, discusses several approaches for incorporating in recommendation process, illustrates usage such application areas where different types contexts are exploited. The concludes discussing challenges future research directions context‐aware

intelligent user interface, computer science, human-computer interaction, context model, natural language processing, user context, context awareness, machine learning, recommender system, data science, user experience, knowledge discovery, context-aware pervasive system, ubiquitous computing, context-aware recommender systems, information retrieval, context management, knowledge management",2011,1371,intelligent user interface|computer science|human-computer interaction|context model|natural language processing|user context|context awareness|machine learning|recommender system|data science|user experience|knowledge discovery|context-aware pervasive system|ubiquitous computing|context-aware recommender systems|information retrieval|context management|knowledge management,
https://openalex.org/W2250966211,Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,"Document Modeling with Gated Recurrent Neural Network for Sentiment Classification

Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in semantic meaning of document. To address this, we introduce neural network model to learn vector-based document representation unified, bottom-up fashion. The first learns sentence with convolutional or long short-term memory. Afterwards, semantics and their are adaptively encoded gated recurrent network. We conduct on four large-scale review datasets from IMDB Yelp Dataset Challenge. Experimental results show that: (1) our shows superior performances over several state-of-the-art algorithms; (2) dramatically outperforms standard modeling for classification. 1

computer science, machine learning, information fusion, supervised learning, recurrent neural network, cognitive science, data science, computational intelligence, content similarity detection, language model, deep learning, machine learning research, machine vision, text mining, natural language processing, large language model, knowledge discovery, sequence modelling, sentiment classification",2015,1368,computer science|machine learning|information fusion|supervised learning|recurrent neural network|cognitive science|data science|computational intelligence|content similarity detection|language model|deep learning|machine learning research|machine vision|text mining|natural language processing|large language model|knowledge discovery|sequence modelling|sentiment classification,https://openalex.org/W2963626623|https://openalex.org/W2884001105|https://openalex.org/W2562607067|https://openalex.org/W2964236337
https://openalex.org/W1743243001,Sentiment Analysis of Twitter Data,"Sentiment Analysis of Twitter Data

We examine sentiment analysis on Twitter data. The contributions of this paper are: (1) introduce POS-specific prior polarity features. (2) explore the use a tree kernel to obviate need for tedious feature engineering. new features (in conjunction with previously proposed features) and perform approximately at same level, both outperforming state-of-the-art baseline.

linguistics, social medium mining, twitter data, content analysis, disinformation detection, social network analysis, communication, semantic evaluation, data science, social data analysis, online information, machine learning research, text mining, natural language processing, affective computing, social medium monitoring, knowledge discovery, social medium data, social media, sentiment analysis",2011,1368,linguistics|social medium mining|twitter data|content analysis|disinformation detection|social network analysis|communication|semantic evaluation|data science|social data analysis|online information|machine learning research|text mining|natural language processing|affective computing|social medium monitoring|knowledge discovery|social medium data|social media|sentiment analysis,
https://openalex.org/W2605135824,Improved Training of Wasserstein GANs,"Improved Training of Wasserstein GANs

Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable of GANs, sometimes can still generate only low-quality samples or fail to converge. We find that these problems often due the use weight clipping in WGAN enforce a Lipschitz constraint on critic, which lead undesired behavior. propose an alternative weights: penalize norm gradient critic with respect its input. Our method performs better than standard and enables wide variety architectures almost no hyperparameter tuning, including 101-layer ResNets language models over discrete data. also achieve high quality generations CIFAR-10 LSUN bedrooms.

machine learning, computer science, gan power device, improved training, generative adversarial network, wasserstein gans, adversarial machine learning, natural language processing, deep learning, scene interpretation",2017,1356,machine learning|computer science|gan power device|improved training|generative adversarial network|wasserstein gans|adversarial machine learning|natural language processing|deep learning|scene interpretation,
https://openalex.org/W1987556956,WAYS WITH WORDS,"WAYS WITH WORDS

I was delighted to hear that the BMJ (British Medical Journal) had accepted an advertisement for (Builders Merchants Journal). It confirmed a long held belief in abomination of abbreviations and arrogance those who use them, expecting others know what they are talking about.

Over years have collected many examples with similar potential confusion. A simple case might be BP: …

applied linguistics, morphology (linguistics), linguistics, keyword extraction, natural language processing, language, general linguistics, language-based approach, syntax, communication, lexical semantics, sociolinguistics, semantics, social semiotics, linguistic theory, narrative, language learning, context (linguistics)",1995,1354,applied linguistics|morphology (linguistics)|linguistics|keyword extraction|natural language processing|language|general linguistics|language-based approach|syntax|communication|lexical semantics|sociolinguistics|semantics|social semiotics|linguistic theory|narrative|language learning|context (linguistics),
https://openalex.org/W1646278814,A comparison of string distance metrics for name-matching tasks,"A comparison of string distance metrics for name-matching tasks

Using an open-source, Java toolkit of name-matching methods, we experimentally compare string distance metrics on the task matching entity names. We investigate a number different proposed by communities, including edit-distance metrics, fast heuristic comparators, token-based and hybrid methods. Overall, best-performing method is scheme combining TFIDF weighting scheme, which widely used in information retrieval, with Jaro-Winkler string-distance was developed probabilistic record linkage community.

pattern recognition, computer science, distance metrics, text mining, natural language processing, pattern matching, semantic evaluation, similarity search, string-searching algorithm, named-entity recognition, deep learning, nlp task, machine learning research, name-matching tasks, information retrieval",2003,1353,pattern recognition|computer science|distance metrics|text mining|natural language processing|pattern matching|semantic evaluation|similarity search|string-searching algorithm|named-entity recognition|deep learning|nlp task|machine learning research|name-matching tasks|information retrieval,
https://openalex.org/W2048658075,Yahoo! for Amazon: Sentiment Extraction from Small Talk on the Web,"Yahoo! for Amazon: Sentiment Extraction from Small Talk on the Web

Extracting sentiment from text is a hard semantic problem. We develop methodology for extracting small investor stock message boards. The algorithm comprises different classifier algorithms coupled together by voting scheme. Accuracy levels are similar to widely used Bayes classifiers, but false positives lower and accuracy higher. Time series cross-sectional aggregation of information improves the quality resultant index, particularly in presence slang ambiguity. Empirical applications evidence relationship with values—tech-sector postings related index levels, volumes volatility. may be assess impact on opinion management announcements, press releases, third-party news, regulatory changes.

conversational recommender system, computer science, content analysis, keyword extraction, text mining, natural language processing, world wide web, opinion aggregation, affective computing, sentiment extraction, social computing, small talk, communication, data science, social media, online information",2007,1340,conversational recommender system|computer science|content analysis|keyword extraction|text mining|natural language processing|world wide web|opinion aggregation|affective computing|sentiment extraction|social computing|small talk|communication|data science|social media|online information,https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2126581182
https://openalex.org/W1964613733,A holistic lexicon-based approach to opinion mining,"A holistic lexicon-based approach to opinion mining

One of the important types information on Web is opinions expressed in user generated content, e.g., customer reviews products, forum posts, and blogs. In this paper, we focus products. particular, study problem determining semantic orientations (positive, negative or neutral) product features reviews. This has many applications, opinion mining, summarization search. Most existing techniques utilize a list (bearing) words (also called lexicon) for purpose. Opinion are that express desirable (e.g., great, amazing, etc.) undesirable bad, poor, etc) states. These approaches, however, all have some major shortcomings. propose holistic lexicon-based approach to solving by exploiting external evidences linguistic conventions natural language expressions. allows system handle context dependent, which cause difficulties algorithms. It also deals with special words, phrases constructs impacts based their patterns. an effective function aggregating multiple conflicting sentence. A system, Observer, proposed technique been implemented. Experimental results using benchmark review data set additional show highly effective. outperforms methods significantly

opinion mining, linguistics, natural language processing, knowledge discovery, lexicon, argument mining, data mining",2008,1338,opinion mining|linguistics|natural language processing|knowledge discovery|lexicon|argument mining|data mining,https://openalex.org/W2108646579|https://openalex.org/W4211186029|https://openalex.org/W2126581182|https://openalex.org/W2031998113|https://openalex.org/W2612769033|https://openalex.org/W71795751|https://openalex.org/W2250879510
https://openalex.org/W2952087486,Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition,"Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition

We describe the CoNLL-2002 shared task: language-independent named entity recognition. give background information on data sets and evaluation method, present a general overview of systems that have taken part in task discuss their performance.

computer science, linguistics, machine learning, computational linguistics, document classification, language, natural language generation, entity recognition, semantic evaluation, nlp task, keyword extraction, general linguistics, deep learning, machine learning research, text mining, natural language processing, knowledge discovery, named-entity recognition, entity disambiguation",2002,1337,computer science|linguistics|machine learning|computational linguistics|document classification|language|natural language generation|entity recognition|semantic evaluation|nlp task|keyword extraction|general linguistics|deep learning|machine learning research|text mining|natural language processing|knowledge discovery|named-entity recognition|entity disambiguation,https://openalex.org/W2962739339|https://openalex.org/W2296283641|https://openalex.org/W3035390927|https://openalex.org/W3019166713|https://openalex.org/W2953356739|https://openalex.org/W3037109418
https://openalex.org/W4287824654,ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators,"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators

Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct original tokens. While they produce good results when transferred downstream NLP tasks, generally require large amounts of compute be effective. As an alternative, we propose more sample-efficient task called replaced token detection. Instead masking input, our approach corrupts it plausible alternatives sampled from small generator network. Then, instead training that predicts identities corrupted tokens, discriminative whether each in was sample or not. Thorough experiments demonstrate this new is efficient than MLM because defined over all rather just subset masked out. result, contextual representations learned substantially outperform ones given same size, data, compute. The gains are particularly strong for models; example, on one GPU 4 days outperforms GPT (trained using 30x compute) GLUE natural understanding benchmark. Our also works well at scale, where performs comparably RoBERTa XLNet while less 1/4 their them amount

natural language processing, pre-training text encoders, text processing, text recognition, computer science, language model, machine learning, machine learning research, text mining, document classification",2020,1336,natural language processing|pre-training text encoders|text processing|text recognition|computer science|language model|machine learning|machine learning research|text mining|document classification,https://openalex.org/W2979826702|https://openalex.org/W3105966348|https://openalex.org/W3098605233
https://openalex.org/W2970476646,Language Models as Knowledge Bases?,"Language Models as Knowledge Bases?

Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint (EMNLP-IJCNLP). 2019.

language, natural language processing, knowledge base, language model",2019,1333,language|natural language processing|knowledge base|language model,https://openalex.org/W3185341429|https://openalex.org/W3098605233
https://openalex.org/W2101390659,A trainable document summarizer,"A trainable document summarizer

Article Free Access Share on A trainable document summarizer Authors: Julian Kupiec Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Alto, CA CAView Profile , Jan Pedersen Francine Chen Authors Info & Claims SIGIR '95: Proceedings of the 18th annual international ACM conference and development in information retrievalJuly 1995Pages 68–73https://doi.org/10.1145/215206.215333Published:01 July 1995Publication History 660citation4,106DownloadsMetricsTotal Citations660Total Downloads4,106Last 12 Months681Last 6 weeks101 Get Citation AlertsNew Alert added!This alert has been successfully added will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log to account Publisher SiteeReaderPDF

trainable document summarizer, computer science, natural language processing, documentation, automatic summarization, annotation tool",1995,1332,trainable document summarizer|computer science|natural language processing|documentation|automatic summarization|annotation tool,https://openalex.org/W2606974598|https://openalex.org/W3101913037|https://openalex.org/W1828401780
https://openalex.org/W3098824823,Transformers: State-of-the-Art Natural Language Processing,"Transformers: State-of-the-Art Natural Language Processing

Recent progress in natural language processing has been driven by advances both model architecture and pretraining. Transformer architectures have facilitated building higher-capacity models pretraining made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal opening up these wider machine learning community. The consists carefully engineered state-of-the art under unified API. Backing curated collection pretrained available designed be extensible researchers, simple practitioners, fast robust industrial deployments. at https://github.com/huggingface/transformers.

machine learning, computer science, natural language, artificial intelligence, nlp task, natural language processing, computational linguistics, language model",2020,1330,machine learning|computer science|natural language|artificial intelligence|nlp task|natural language processing|computational linguistics|language model,
https://openalex.org/W2098062695,Supervised Topic Models,"Supervised Topic Models

We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The accommodates variety response types. derive maximum-likelihood procedure for parameter estimation, which relies on variational approximations to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted predict values new test sLDA two real-world problems: movie ratings predicted from reviews, and web page popularity text descriptions. illustrate benefits versus modern regularized regression, as well an unsupervised LDA analysis followed by separate regression.

computer science, supervised learning, topic model, text mining, natural language processing, unsupervised machine learning, machine learning, data science, computational intelligence, deep learning, statistics, knowledge discovery, machine learning research, semi-supervised learning, data mining",2007,1327,computer science|supervised learning|topic model|text mining|natural language processing|unsupervised machine learning|machine learning|data science|computational intelligence|deep learning|statistics|knowledge discovery|machine learning research|semi-supervised learning|data mining,https://openalex.org/W2174706414|https://openalex.org/W2108646579|https://openalex.org/W1714665356
https://openalex.org/W2035032881,Transition network grammars for natural language analysis,"Transition network grammars for natural language analysis

The use of augmented transition network grammars for the analysis natural language sentences is described. Structure-building actions associated with arcs grammar allow reordering, restructuring, and copying constituents necessary to produce deep-structure representations type normally obtained from a transformational analysis, conditions on powerful selectivity which can rule out meaningless analyses take advantage semantic information guide parsing. advantages this model are discussed in detail illustrated by examples. An implementation an experimental parsing system briefly

computer science, linguistics, text mining, natural language processing, computational linguistics, natural language, machine learning research, syntactic parsing, natural language analysis, transition network grammars",1970,1326,computer science|linguistics|text mining|natural language processing|computational linguistics|natural language|machine learning research|syntactic parsing|natural language analysis|transition network grammars,
https://openalex.org/W2140124448,The author-topic model for authors and documents,"The author-topic model for authors and documents

We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include authorship information. Each author is associated with multinomial distribution over topics and each topic words. A document multiple authors modeled as mixture of distributions authors. apply collection 1,700 NIPS conference papers 160,000 CiteSeer abstracts. Exact inference intractable these datasets we use Gibbs sampling estimate distributions. compare performance two other models documents, which are special cases model: LDA (a model) simple in words rather than topics. show recovered by demonstrate applications computing similarity between entropy output.

computer science, content analysis, topic model, text mining, digital humanities, natural language processing, author-topic model, author profiling, bibliometrics, knowledge discovery, document clustering, information retrieval, writer identification",2004,1324,computer science|content analysis|topic model|text mining|digital humanities|natural language processing|author-topic model|author profiling|bibliometrics|knowledge discovery|document clustering|information retrieval|writer identification,https://openalex.org/W2174706414|https://openalex.org/W2072644219|https://openalex.org/W2098062695|https://openalex.org/W2334889010|https://openalex.org/W1714665356
https://openalex.org/W1549026077,Natural Language Understanding,"Natural Language Understanding

From the Publisher:
In addition, this title offers coverage of two entirely new subject areas. First, text features a chapter on statistically-based methods using large corpora. Second, it includes an appendix speech recognition and spoken language understanding. Also, information semantics that was covered in first edition has been largely expanded to include emphasis compositional interpretation.

natural language understanding, semantic analysis (linguistics), language model, semantic parsing, text mining, automated reasoning, spoken language technology, natural language processing, semantic similarity, semantics, language, language learning, semantic interpretation, linguistics, computational linguistics, natural language, general linguistics, semantic evaluation, communication",2021,1324,natural language understanding|semantic analysis (linguistics)|language model|semantic parsing|text mining|automated reasoning|spoken language technology|natural language processing|semantic similarity|semantics|language|language learning|semantic interpretation|linguistics|computational linguistics|natural language|general linguistics|semantic evaluation|communication,https://openalex.org/W1574901103|https://openalex.org/W1990689692|https://openalex.org/W2163107094
https://openalex.org/W2068882115,Message Understanding Conference-6,"Message Understanding Conference-6

We have recently completed the sixth in a series of ""Message Understanding Conferences"" which are designed to promote and evaluate research information extraction. MUC-6 introduced several innovations over prior MUCs, most notably range different tasks for evaluations were conducted. describe some motivations new format briefly discuss results evaluations.

computer science, human-computer interaction, message passing, natural language processing, communication theory, computer-mediated communication, communication, semantic evaluation, group communication, cognitive science, conversation analysis, dialogue management, scientific communication, organizational communication",1996,1319,computer science|human-computer interaction|message passing|natural language processing|communication theory|computer-mediated communication|communication|semantic evaluation|group communication|cognitive science|conversation analysis|dialogue management|scientific communication|organizational communication,https://openalex.org/W2020278455|https://openalex.org/W2168041406|https://openalex.org/W2148506018|https://openalex.org/W86887328
https://openalex.org/W1527575280,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models

Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a joint embedding space with images text (b): novel language model for decoding distributed representations from our space. Our effectively unifies image-text models neural models. We the structure-content disentangles structure of sentence to its content, conditioned on produced encoder. The encoder allows one rank sentences while decoder can generate descriptions scratch. Using LSTM encode sentences, match state-of-the-art performance Flickr8K Flickr30K without using object detections. also set new best results when 19-layer Oxford convolutional network. Furthermore show linear encoders, learned captures regularities terms vector arithmetic e.g. *image blue car* - ""blue"" + ""red"" is near red cars. Sample captions generated 800 are made available comparison.

semantic representation, computer science, visual-semantic embeddings, natural language processing, language model, vision language model, machine learning, deep learning, multimodal learning",2014,1306,semantic representation|computer science|visual-semantic embeddings|natural language processing|language model|vision language model|machine learning|deep learning|multimodal learning,https://openalex.org/W1514535095|https://openalex.org/W2950178297|https://openalex.org/W2405756170|https://openalex.org/W2139501017|https://openalex.org/W2425121537|https://openalex.org/W2886641317|https://openalex.org/W2949999304|https://openalex.org/W2963758027|https://openalex.org/W2962706528
https://openalex.org/W2136053273,F<scp>UNCTION AND</scp> M<scp>ECHANISM OF</scp> O<scp>RGANIC</scp> A<scp>NION</scp> E<scp>XUDATION FROM</scp> P<scp>LANT</scp> R<scp>OOTS</scp>,"F<scp>UNCTION AND</scp> M<scp>ECHANISM OF</scp> O<scp>RGANIC</scp> A<scp>NION</scp> E<scp>XUDATION FROM</scp> P<scp>LANT</scp> R<scp>OOTS</scp>

▪ Abstract The rhizosphere is the zone of soil immediately surrounding plant roots that modified by root activity. In this critical zone, plants perceive and respond to their environment. As a consequence normal growth development, large range organic inorganic substances are exchanged between soil, which inevitably leads changes in biochemical physical properties rhizosphere. Plants also modify response certain environmental signals stresses. Organic anions commonly detected region, exudation from has now been associated with nutrient deficiencies ion This review summarizes recent developments understanding function, mechanism, regulation anion roots. benefits derive presence described potential for biotechnology increase highlighted.

computer science, linguistics, graph theory, natural language processing, theory of computation, quantum chemistry, transition metal chemistry, communication, multi-terminal information theory, proton-coupled electron transfer, network mechanism, communications system, telecommunication network, system science, multiagent system",2001,1304,computer science|linguistics|graph theory|natural language processing|theory of computation|quantum chemistry|transition metal chemistry|communication|multi-terminal information theory|proton-coupled electron transfer|network mechanism|communications system|telecommunication network|system science|multiagent system,
https://openalex.org/W2127480961,Content-based book recommending using learning for text categorization,"Content-based book recommending using learning for text categorization

Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes dislikes. Most existing recommender use collaborative filtering methods that base recommendations other users' preferences. By contrast,content-based about an item itself make suggestions.This approach has the advantage being able recommend previously unrated items users with unique interests provide explanations for its recommendations. We describe content-based book recommending system utilizes extraction machine-learning algorithm text categorization. Initial experimental results demonstrate this can produce accurate

text categorization, computer science, content analysis, text mining, natural language processing, content-based book, machine learning, recommender system, data science, deep learning, content similarity detection, automatic classification",2000,1298,text categorization|computer science|content analysis|text mining|natural language processing|content-based book|machine learning|recommender system|data science|deep learning|content similarity detection|automatic classification,https://openalex.org/W2171960770
https://openalex.org/W1645937837,Building Natural-Language Generation Systems,"Building Natural-Language Generation Systems

This is a very short paper that briefly discusses some of the tasks NLG systems perform. It no research interest, but I have occasionally found it useful as way introducing to potential project collaborators who know nothing about field.

natural language processing, language, natural language generation, natural language, natural-language generation systems, language generation",1996,1294,natural language processing|language|natural language generation|natural language|natural-language generation systems|language generation,
https://openalex.org/W2031998113,Techniques and applications for sentiment analysis,"Techniques and applications for sentiment analysis

The main applications and challenges of one the hottest research areas in computer science.

computer science, text mining, natural language processing, affective computing, multimodal sentiment analysis, data science, analytics, emotion recognition, sentiment analysis",2013,1294,computer science|text mining|natural language processing|affective computing|multimodal sentiment analysis|data science|analytics|emotion recognition|sentiment analysis,https://openalex.org/W2019759670|https://openalex.org/W2250879510
https://openalex.org/W1938755728,Character-Aware Neural Language Models,"Character-Aware Neural Language Models

We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our employs convolutional network (CNN) and highway over characters, whose output is given to long short-term memory (LSTM) recurrent (RNN-LM). On English Penn Treebank par with existing state-of-the-art despite having 60% fewer parameters. languages rich morphology (Arabic, Czech, French, German, Spanish, Russian), outperforms word-level/morpheme-level LSTM baselines, again The results suggest many languages, character inputs sufficient for modeling. Analysis of word representations obtained from composition part reveals able encode, characters only, both semantic orthographic information.

computer science, linguistics, large language model, natural language processing, language model, language, neural network (machine learning), language learning",2015,1293,computer science|linguistics|large language model|natural language processing|language model|language|neural network (machine learning)|language learning,https://openalex.org/W2962739339|https://openalex.org/W2493916176|https://openalex.org/W2296283641|https://openalex.org/W2884001105|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2880875857|https://openalex.org/W2963042536
https://openalex.org/W2413794162,A Decomposable Attention Model for Natural Language Inference,"A Decomposable Attention Model for Natural Language Inference

We propose a simple neural architecture for natural language inference.Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable.On Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information.Adding intra-sentence takes minimum amount account yields further improvements.

decomposable attention model, hidden markov model, natural language processing, nlp task, natural language inference, computer science, language model, machine learning, machine learning research, deep learning, data science, computational linguistics",2016,1291,decomposable attention model|hidden markov model|natural language processing|nlp task|natural language inference|computer science|language model|machine learning|machine learning research|deep learning|data science|computational linguistics,https://openalex.org/W2896457183|https://openalex.org/W2936695845|https://openalex.org/W2912924812|https://openalex.org/W2963691697
https://openalex.org/W4221143046,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

We explore how generating a chain of thought -- series intermediate reasoning steps significantly improves the ability large language models to perform complex reasoning. In particular, we show such abilities emerge naturally in sufficiently via simple method called prompting, where few demonstrations are provided as exemplars prompting. Experiments on three that prompting performance range arithmetic, commonsense, and symbolic tasks. The empirical gains can be striking. For instance, 540B-parameter model with just eight achieves state art accuracy GSM8K benchmark math word problems, surpassing even finetuned GPT-3 verifier.

computer science, knowledge representation and reasoning, artificial intelligence, large language model, language model, natural language processing, reasoning system, machine learning, model-based reasoning, cognitive science",2022,1291,computer science|knowledge representation and reasoning|artificial intelligence|large language model|language model|natural language processing|reasoning system|machine learning|model-based reasoning|cognitive science,
https://openalex.org/W2460657278,3D Semantic Parsing of Large-Scale Indoor Spaces,"3D Semantic Parsing of Large-Scale Indoor Spaces

In this paper, we propose a method for semantic parsing the 3D point cloud of an entire building using hierarchical approach: first, raw data is parsed into semantically meaningful spaces (e.g. rooms, etc) that are aligned canonical reference coordinate system. Second, their structural and elements walls, columns, etc). Performing these with strong notation global space backbone our method. The alignment in first step injects priors from system second discovering elements. This allows diverse challenging scenarios as man-made indoor often show recurrent geometric patterns while appearance features can change drastically. We also argue identification essentially detection problem, rather than segmentation which commonly used. evaluated on new dataset several buildings covered area over 6, 000m <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> 215 million points, demonstrating robust results readily useful practical applications.

computer science, natural language processing, scene modeling, large-scale indoor spaces, semantic parsing, geospatial semantics",2016,1287,computer science|natural language processing|scene modeling|large-scale indoor spaces|semantic parsing|geospatial semantics,
https://openalex.org/W2251803266,"Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors","Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors

Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on distributional semantics block.Despite buzz surrounding these models, literature is still lacking a systematic comparison of predictive with classic, count-vector-based semantic approaches.In this paper, we perform such an extensive evaluation, wide range lexical tasks and across many parameter settings.The results, to our own surprise, show that fully justified, context-predicting obtain thorough resounding victory against their count-based counterparts.

linguistics, text mining, natural language processing, semantic analysis (linguistics), semantic similarity, semantic evaluation, cognitive science, computational linguistics, data science, semantics, semantic processing, systematic comparison, machine learning research, distributional semantics, semantic learning, context-predicting semantic vectors, context (linguistics)",2014,1286,linguistics|text mining|natural language processing|semantic analysis (linguistics)|semantic similarity|semantic evaluation|cognitive science|computational linguistics|data science|semantics|semantic processing|systematic comparison|machine learning research|distributional semantics|semantic learning|context-predicting semantic vectors|context (linguistics),https://openalex.org/W2250539671|https://openalex.org/W2265846598|https://openalex.org/W2125031621|https://openalex.org/W2250966211|https://openalex.org/W2970476646
https://openalex.org/W2970419734,Text Summarization with Pretrained Encoders,"Text Summarization with Pretrained Encoders

Yang Liu, Mirella Lapata. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint (EMNLP-IJCNLP). 2019.

computer science, text summarization, natural language processing, pretrained encoders, automatic summarization",2019,1284,computer science|text summarization|natural language processing|pretrained encoders|automatic summarization,https://openalex.org/W3034999214
https://openalex.org/W1916559533,Statistical machine translation,"Statistical machine translation

This introductory text to statistical machine translation (SMT) provides all of the theories and methods needed build a translator, such as Google Language Tools Babelfish. In general, techniques allow automatic systems be built quickly for any language-pair using only translated texts generic software. With increasing globalization, will central communication commerce. Based on courses tutorials, classroom-tested globally, it is ideal instruction or self-study, advanced undergraduates graduate students in computer science and/or computational linguistics, researchers natural language processing. The companion website open-source corpora tool-kits.

machine translation, statistical machine translation, natural language processing, statistics, neural machine translation, translation studies",2010,1282,machine translation|statistical machine translation|natural language processing|statistics|neural machine translation|translation studies,https://openalex.org/W2964308564|https://openalex.org/W2133564696|https://openalex.org/W2606974598|https://openalex.org/W2413794162|https://openalex.org/W2963970792
https://openalex.org/W3097777922,Conformer: Convolution-augmented Transformer for Speech Recognition,"Conformer: Convolution-augmented Transformer for Speech Recognition

Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent networks (RNNs).Transformer are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution transformers model dependencies an audio sequence a parameter-efficient way.To regard, propose convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms previous CNN achieving state-of-the-art accuracies.On widely used LibriSpeech benchmark, our achieves WER 2.1%/4.3%without using language 1.9%/3.9%with external on test/testother.We also observe competitive performance 2.7%/6.3%with small only 10M parameters.

speech technology, spoken language technology, computer engineering, natural language processing, convolution-augmented transformer, speech processing, computer science, speech recognition, convolutional neural network, deep learning",2020,1281,speech technology|spoken language technology|computer engineering|natural language processing|convolution-augmented transformer|speech processing|computer science|speech recognition|convolutional neural network|deep learning,
https://openalex.org/W2165599843,Online Learning for Latent Dirichlet Allocation,"Online Learning for Latent Dirichlet Allocation

We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on stochastic optimization with a natural gradient step, which we show converges to local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in stream. study performance several ways, by fitting 100-topic topic model 3.3M articles from Wikipedia single pass. demonstrate that finds models as good or better than found batch VB, and fraction time.

latent dirichlet allocation, computer science, information fusion, keyword extraction, text mining, natural language processing, online learning, information extraction, topic model, machine learning, data science, computational intelligence, deep learning, statistics, knowledge discovery, machine learning research, content similarity detection, information retrieval",2010,1272,latent dirichlet allocation|computer science|information fusion|keyword extraction|text mining|natural language processing|online learning|information extraction|topic model|machine learning|data science|computational intelligence|deep learning|statistics|knowledge discovery|machine learning research|content similarity detection|information retrieval,https://openalex.org/W2174706414|https://openalex.org/W2130339025
https://openalex.org/W2085574295,"From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory","From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory

Preface. 0: Preliminaries. 0.1. Theories of Meaning. 0.2. Logic. 0.3. Logic and Semantics. 0.4. Syntax. 1: DRT Predicate 1.1. Simple Sentences. 1.2. Models. 1.3. Negation. 1.4. Verification, Truth Accessibility. 1.5. From to 2: Quantification Connectives. 2.1. Conditionals. 2.2. Universal Quantification. 2.3. Disjunction. 2.4. Conjunction. 3: Loose Ends. 3.1. Reflexives. 3.2. Possessive Noun Phrases. 3.3. Proper Names. 3.4. Definite Descriptions. 3.5. Stipulated Identity Asserted Identity. 3.6. Predication. 3.7. Scope Amibiguity. 4: The Plural. 4.1. Introduction. 4.2. DRS-Construction for Plurals I. 4.3. Model Theory. 4.4. II. 5: Tense Aspect. 5.1. Semantics Temporal Reference. 5.2. Tensed 5.3. 5.4. Perspective. 5.5. Adverbials. 5.6. 5.7. Syntactic Rules. Bibliography. Table Construction Index Symbols, Features Feature Values. Subjects.

semantic representation, linguistics, philosophy of language, natural language processing, language, discourse analysis, semantics, formal semantics, formal language, modeltheoretic semantics, discourse representation theory, logic in computer science, formal logic",1995,1271,semantic representation|linguistics|philosophy of language|natural language processing|language|discourse analysis|semantics|formal semantics|formal language|modeltheoretic semantics|discourse representation theory|logic in computer science|formal logic,
https://openalex.org/W2172140247,On the Properties of Neural Machine Translation: Encoder-Decoder Approaches,"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches

Neural machine translation is a relatively new approach to statistical based purely on neural networks. The models often consist of an encoder and decoder. extracts fixed-length representation from variable-length input sentence, the decoder generates correct this representation. In paper, we focus analyzing properties using two models; RNN Encoder--Decoder newly proposed gated recursive convolutional network. We show that performs well short sentences without unknown words, but its performance degrades rapidly as length sentence number words increase. Furthermore, find network learns grammatical structure automatically.

linguistics, computer science, machine learning, computational intelligence, language learning, large language model, machine translation, language, machine learning research, natural language processing, iterative decoding, neural machine translation, recurrent neural network, deep learning, autoencoders, language model, neural computation",2014,1269,linguistics|computer science|machine learning|computational intelligence|language learning|large language model|machine translation|language|machine learning research|natural language processing|iterative decoding|neural machine translation|recurrent neural network|deep learning|autoencoders|language model|neural computation,https://openalex.org/W2962739339|https://openalex.org/W2133564696|https://openalex.org/W2963918774|https://openalex.org/W2787560479|https://openalex.org/W2963506925|https://openalex.org/W2938704169|https://openalex.org/W2963042536
https://openalex.org/W2139501017,Sequence to Sequence -- Video to Text,"Sequence to Sequence -- Video to Text

Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) output words) variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model generate captions videos. For exploit recurrent neural networks, specifically LSTMs, which demonstrated state-of-the-art performance in image caption generation. Our LSTM is trained on video-sentence pairs learns associate sequence frames words order description the event clip. naturally able learn as well generated sentences, i.e. language model. We evaluate several variants our that different visual features standard set YouTube two movie datasets (M-VAD MPII-MD).

video retrieval, video adaptation, natural language processing, linguistics, video summarization, video content analysis, sequence modelling",2015,1267,video retrieval|video adaptation|natural language processing|linguistics|video summarization|video content analysis|sequence modelling,https://openalex.org/W2963929190|https://openalex.org/W2550553598|https://openalex.org/W2425121537
https://openalex.org/W1996650435,"Are Good Texts Always Better? Interactions of Text Coherence, Background Knowledge, and Levels of Understanding in Learning From Text","Are Good Texts Always Better? Interactions of Text Coherence, Background Knowledge, and Levels of Understanding in Learning From Text

Two experiments, theoretically motivated by the construction-integration model of text comprehension (W. Kintsch, 1988), investigated role coherence in science texts. In Experiment 1, junior high school students' one three versions a biology was examined via free recall, written questions, and key-word sorting task. This study demonstrates advantages for globally coherent more explanatory text. 2, interactions among local global coherence, readers' background knowledge, levels understanding were examined. Using same methods as we four text, orthogonally varying coherence. We found that readers who know little about domain benefit from whereas high-knowledge minimally argue poorly forces knowledgeable to engage compensatory processing infer unstated relations These findings, however, depended on level understanding, base or situational, being measured tasks. Whereas free-recall measure text-based questions primarily tapped superficial inference problem-solving task relied situational provides evidence rewards be gained active are at situation rather than text-base understanding.

narrative, educational research, good texts, text coherence, language comprehension, communication, text simplification, cognitive science, educational psychology, textual practice, reading research, background knowledge, language learning, education, learning sciences, text mining, natural language processing, language-based approach, conceptual knowledge acquisition, language acquisition, semantic learning",1996,1265,narrative|educational research|good texts|text coherence|language comprehension|communication|text simplification|cognitive science|educational psychology|textual practice|reading research|background knowledge|language learning|education|learning sciences|text mining|natural language processing|language-based approach|conceptual knowledge acquisition|language acquisition|semantic learning,
https://openalex.org/W2146221819,Real-time American sign language recognition using desk and wearable computer based video,"Real-time American sign language recognition using desk and wearable computer based video

We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes user from desk mounted and achieves 92 percent word accuracy. second mounts in cap worn by 98 accuracy (97 with an unrestricted grammar). Both experiments use 40-word lexicon.

computer science, speech recognition, human-computer interaction, natural language processing, language, assistive technology, spoken language technology, wearable technology, wearable computer, language recognition, gesture recognition, american sign language, sign language",1998,1263,computer science|speech recognition|human-computer interaction|natural language processing|language|assistive technology|spoken language technology|wearable technology|wearable computer|language recognition|gesture recognition|american sign language|sign language,
https://openalex.org/W2131571251,word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method,"word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method

The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot traction lately, provides state-of-the-art word embeddings. learning models behind the are described in two research papers. We found description these papers to be somewhat cryptic hard follow. While motivations presentation may obvious neural-networks language-modeling crowd, we had struggle quite bit figure out rationale equations. This note is an attempt explain equation (4) (negative sampling) ""Distributed Representations Words Phrases their Compositionality"" by Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado Jeffrey Dean.

word embeddings, linguistics, negative-sampling word-embedding method, natural language processing, computational linguistics, principal component analysis",2014,1263,word embeddings|linguistics|negative-sampling word-embedding method|natural language processing|computational linguistics|principal component analysis,https://openalex.org/W2125031621|https://openalex.org/W3105625590|https://openalex.org/W2963042536
https://openalex.org/W4239946314,Sentiment strength detection in short informal text,"Sentiment strength detection in short informal text

Abstract A huge number of informal messages are posted every day in social network sites, blogs, and discussion forums. Emotions seem to be frequently important these texts for expressing friendship, showing support or as part online arguments. Algorithms identify sentiment strength needed help understand the role emotion this communication also inappropriate anomalous affective utterances, potentially associated with threatening behavior self others. Nevertheless, existing detection algorithms tend commercially oriented, designed opinions about products rather than user behaviors. This article partly fills gap a new algorithm, SentiStrength, extract from English text, using methods exploit de facto grammars spelling styles cyberspace. Applied MySpace comments lookup table term strengths optimized by machine learning, SentiStrength is able predict positive 60.6% accuracy negative 72.8% accuracy, both based upon scales 1–5. The former, but not latter, better baseline wide range general learning approaches.

sentiment strength detection, computer science, text mining, informal learning, affective computing, natural language processing, natural language generation, language, reading research, bias detection, part-of-speech tagging, linguistics, content analysis, content similarity detection, corpus linguistics, disinformation detection, structural linguistics, narrative, semantic evaluation, short informal text",2010,1261,sentiment strength detection|computer science|text mining|informal learning|affective computing|natural language processing|natural language generation|language|reading research|bias detection|part-of-speech tagging|linguistics|content analysis|content similarity detection|corpus linguistics|disinformation detection|structural linguistics|narrative|semantic evaluation|short informal text,https://openalex.org/W2084046180|https://openalex.org/W2740168486|https://openalex.org/W1987425720
https://openalex.org/W2096152098,A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization,"A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization

Abstract : A probabilistic analysis of the Rocchio relevance feedback algorithm, one most popular learning methods from information retrieval, is presented in a text categorization framework. The results version classifier and offers an explanation for TFIDF word weighting heuristic. classifier, its variant standard naive Bayes are compared on three tasks. suggest that algorithms preferable to heuristic classifier.

pattern recognition, computer science, keyword extraction, rocchio algorithm, text mining, language model, natural language processing, text categorization, text recognition, machine learning, document classification, biostatistics, data science, machine learning research, automatic classification, classification method",1997,1251,pattern recognition|computer science|keyword extraction|rocchio algorithm|text mining|language model|natural language processing|text categorization|text recognition|machine learning|document classification|biostatistics|data science|machine learning research|automatic classification|classification method,https://openalex.org/W2118020653|https://openalex.org/W1550206324|https://openalex.org/W2019759670|https://openalex.org/W2127480961
https://openalex.org/W4322718191,LLaMA: Open and Efficient Foundation Language Models,"LLaMA: Open and Efficient Foundation Language Models

We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. train our on trillions tokens, and show that it is possible state-of-the-art using publicly available datasets exclusively, without resorting proprietary inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) most benchmarks, LLaMA-65B competitive with the best models, Chinchilla-70B PaLM-540B. release all research community.

computer science, large language model, natural language processing, language model, language, language engineering, computational linguistics, programming language, ai foundation, foundation model",2023,1250,computer science|large language model|natural language processing|language model|language|language engineering|computational linguistics|programming language|ai foundation|foundation model,
https://openalex.org/W2949998441,Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews,"Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews

This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not down). The classification of review is predicted by the average semantic orientation phrases in that contain adjectives adverbs. A phrase has positive when it good associations (e.g., ""subtle nuances"") and negative bad ""very cavalier""). In this paper, calculated mutual information between given word ""excellent"" minus ""poor"". classified if its positive. achieves an accuracy 74% evaluated on 410 from Epinions, sampled four different domains (reviews automobiles, banks, movies, travel destinations). ranges 84% automobile to 66% movie reviews.

computer science, knowledge discovery, semantic evaluation, text mining, cognitive science, unsupervised classification, affective computing, computational linguistics, automatic classification, linguistics, machine learning, semantic interpretation, machine learning research, natural language processing, machine vision, multimodal sentiment analysis, principal component analysis, information retrieval, distributional semantics, semantic orientation",2002,1249,computer science|knowledge discovery|semantic evaluation|text mining|cognitive science|unsupervised classification|affective computing|computational linguistics|automatic classification|linguistics|machine learning|semantic interpretation|machine learning research|natural language processing|machine vision|multimodal sentiment analysis|principal component analysis|information retrieval|distributional semantics|semantic orientation,https://openalex.org/W2088622183
https://openalex.org/W2144554289,ICDAR 2015 competition on Robust Reading,"ICDAR 2015 competition on Robust Reading

Results of the ICDAR 2015 Robust Reading Competition are presented. A new Challenge 4 on Incidental Scene Text has been added to Challenges Born-Digital Images, Focused Images and Video Text. is run a newly acquired dataset 1,670 images evaluating Localisation, Word Recognition End-to-End pipelines. In addition, for 3 substantially updated with more video sequences accurate ground truth data. Finally, tasks assessing system performance have introduced all Challenges. The competition took place in first quarter 2015, received total 44 submissions. Only reported on. datasets, specification evaluation protocols presented together results brief summary participating methods.

computer science, text recognition, neuroimaging, multimodal signal processing, principal component analysis, image analysis, neuroscience, text segmentation, image representation, computer engineering, language model, image communication, reading research, multimedia information processing, robust reading, machine vision, natural language processing, optical character recognition, computer vision",2015,1244,computer science|text recognition|neuroimaging|multimodal signal processing|principal component analysis|image analysis|neuroscience|text segmentation|image representation|computer engineering|language model|image communication|reading research|multimedia information processing|robust reading|machine vision|natural language processing|optical character recognition|computer vision,https://openalex.org/W2605982830|https://openalex.org/W2343052201
https://openalex.org/W2343052201,Synthetic Data for Text Localisation in Natural Images,"Synthetic Data for Text Localisation in Natural Images

In this paper we introduce a new method for text detection in natural images. The comprises two contributions: First, fast and scalable engine to generate synthetic images of clutter. This overlays existing background way, accounting the local 3D scene geometry. Second, use train Fully-Convolutional Regression Network (FCRN) which efficiently performs bounding-box regression at all locations multiple scales an image. We discuss relation FCRN recently-introduced YOLO detector, as well other end-to-end object systems based on deep learning. resulting network significantly out current methods images, achieving F-measure 84.2% standard ICDAR 2013 benchmark. Furthermore, it can process 15 per second GPU.

computer vision, pattern recognition, natural images, information fusion, natural language processing, text processing, scene interpretation, computer science, text recognition, machine learning, text localisation, localization, deep learning, data science",2016,1242,computer vision|pattern recognition|natural images|information fusion|natural language processing|text processing|scene interpretation|computer science|text recognition|machine learning|text localisation|localization|deep learning|data science,https://openalex.org/W2605982830
https://openalex.org/W2794557536,Universal Sentence Encoder,"Universal Sentence Encoder

We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The are efficient and result in accurate performance on diverse Two variants of the allow trade-offs between accuracy compute resources. For both variants, we investigate report relationship model complexity, resource consumption, availability task training data, performance. Comparisons made with baselines use word level via pretrained embeddings as well do not any learning. find using sentence tends outperform transfer. With embeddings, observe surprisingly good minimal amounts supervised data a task. obtain encouraging results Word Embedding Association Tests (WEAT) targeted at detecting bias. Our pre-trained freely available download TF Hub.

computer science, artificial intelligence, natural language processing, machine learning, computational linguistics, deep learning, nlp task, machine translation, annotation tool, universal sentence encoder",2018,1236,computer science|artificial intelligence|natural language processing|machine learning|computational linguistics|deep learning|nlp task|machine translation|annotation tool|universal sentence encoder,https://openalex.org/W2970641574
https://openalex.org/W2010595692,Techniques for automatically correcting words in text,"Techniques for automatically correcting words in text

Research aimed at correcting words in text has focused on three progressively more difficult problems:(1) nonword error detection; (2) isolated-word correction; and (3) context-dependent work correction. In response to the first problem, efficient pattern-matching n -gram analysis techniques have been developed for detecting strings that do not appear a given word list. second variety of general application-specific spelling correction developed. Some them were based detailed studies patterns. third few experiments using natural-language-processing tools or statistical-language models carried out. This article surveys documented findings patterns, provides descriptions various detection techniques, reviews state art discusses research issues related all areas automatic text.

linguistics, speech recognition, keyword extraction, technology, natural language processing, language model, text mining, text recognition, computational linguistics, automatic annotation, text processing, spoken language technology, error correction",1992,1236,linguistics|speech recognition|keyword extraction|technology|natural language processing|language model|text mining|text recognition|computational linguistics|automatic annotation|text processing|spoken language technology|error correction,https://openalex.org/W2001496424|https://openalex.org/W4239946314
https://openalex.org/W2072725684,The implementation of the Cilk-5 multithreaded language,"The implementation of the Cilk-5 multithreaded language

The fifth release of the multithreaded language Cilk uses a provably good ""work-stealing"" scheduling algorithm similar to first system, but has been completely redesigned and runtime system reengineered. efficiency new implementation was aided by clear strategy that arose from theoretical analysis algorithm: concentrate on minimizing overheads contribute work, even at expense critical path. Although it may seem counterintuitive move onto path, this ""work-first"" principle led portable Cilk-5 in which typical cost spawning parallel thread is only between 2 6 times C function call variety contemporary machines. Many programs run one processor with virtually no degradation compared equivalent programs. This paper describes how work-first exploited design Cilk-5's compiler its system. In particular, we present novel ""two-clone"" compilation Dijkstra-like mutual-exclusion protocol for implementing ready deque work-stealing scheduler.

soft computing, formal methods, concurrent data structure, distributed system, computer engineering, multithreading (computer architecture), natural language processing, instruction-level parallelism, parallel computing, computer science, language, computational linguistics",1998,1236,soft computing|formal methods|concurrent data structure|distributed system|computer engineering|multithreading (computer architecture)|natural language processing|instruction-level parallelism|parallel computing|computer science|language|computational linguistics,
https://openalex.org/W2065813545,Positron Emission Tomographic Studies of the Processing of Singe Words,"Positron Emission Tomographic Studies of the Processing of Singe Words

PET images of blood flow change that were averaged across individuals used to identify brain areas related lexical (single-word) processing, A small number discrete activated during several task conditions including: modality-specific (auditory or visual) by passive word input, primary motor and premotor speech output, yet further tasks making semantic intentional demands.

linguistics, neuroimaging, intermediate representation, multiple scale, psycholinguistics, language, neuroscience, hearing research, reading research, language processing in the brain, phonology, positron emission tomography, speech production, singe words, natural language processing, neurolinguistics, speech processing, speech science, spoken language technology",1989,1232,linguistics|neuroimaging|intermediate representation|multiple scale|psycholinguistics|language|neuroscience|hearing research|reading research|language processing in the brain|phonology|positron emission tomography|speech production|singe words|natural language processing|neurolinguistics|speech processing|speech science|spoken language technology,
https://openalex.org/W2963969878,Adversarial Examples for Evaluating Reading Comprehension Systems,"Adversarial Examples for Evaluating Reading Comprehension Systems

Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these truly understand language remains unclear. To reward with real understanding abilities, we propose an adversarial evaluation scheme for Stanford Question Answering Dataset (SQuAD). Our method tests whether can answer questions about paragraphs contain adversarially inserted sentences, automatically generated distract computer without changing correct or misleading humans. In this setting, of sixteen published models drops from average 75% F1 score 36%; when adversary is allowed add ungrammatical sequences words, on four decreases further 7%. We hope our insights will motivate development new more precisely.

natural language processing, adversarial machine learning, language comprehension, comprehension systems, adversarial examples, language, reading research",2017,1231,natural language processing|adversarial machine learning|language comprehension|comprehension systems|adversarial examples|language|reading research,https://openalex.org/W2912924812|https://openalex.org/W2963847595
https://openalex.org/W2115867364,Word Segmentation: The Role of Distributional Cues,"Word Segmentation: The Role of Distributional Cues

One of the infant's first tasks in language acquisition is to discover words embedded a mostly continuous speech stream. This learning problem might be solved by using distributional cues word boundaries—for example, computing transitional probabilities between sounds input and relative strengths these hypothesize boundaries. The learner further aided language-specific prosodic correlated with As step testing hypotheses, we briefly exposed adults an artificial which only available for segmentation were syllables. Subjects able learn this language. Furthermore, addition certain served enhance performance. These results suggest that may play important role initial learners.

natural language processing, language, semantic processing, word segmentation, distributional cues",1996,1229,natural language processing|language|semantic processing|word segmentation|distributional cues,
https://openalex.org/W2612769033,Sentiment Analysis and Opinion Mining,"Sentiment Analysis and Opinion Mining

Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, emotions from written language. It one most active resear

knowledge discovery, sentiment analysis, social media, semantic evaluation, text mining, social medium data, communication, disinformation detection, affective computing, data science, news analytics, social medium mining, linguistics, opinion aggregation, opinion mining, machine learning research, natural language processing, content analysis, narrative, data mining",2012,1227,knowledge discovery|sentiment analysis|social media|semantic evaluation|text mining|social medium data|communication|disinformation detection|affective computing|data science|news analytics|social medium mining|linguistics|opinion aggregation|opinion mining|machine learning research|natural language processing|content analysis|narrative|data mining,https://openalex.org/W2108646579|https://openalex.org/W2964236337|https://openalex.org/W2306941105
https://openalex.org/W2612675303,A Deep Reinforced Model for Abstractive Summarization,"A Deep Reinforced Model for Abstractive Summarization

Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents summaries however these often include repetitive incoherent phrases. We introduce a neural network model with novel intra-attention that attends over the continuously generated separately, new training method combines standard supervised word prediction reinforcement learning (RL). Models trained only exhibit ""exposure bias"" - they assume ground truth is provided at each step during training. However, when combined global sequence of RL resulting become more readable. evaluate this CNN/Daily Mail New York Times datasets. Our obtains 41.16 ROUGE-1 score dataset, an improvement previous state-of-the-art models. Human evaluation also shows our produces higher quality summaries.

computer science, abstractive summarization, text mining, language model, natural language processing, deep reinforced model, machine learning, data science, deep learning, automatic summarization",2017,1212,computer science|abstractive summarization|text mining|language model|natural language processing|deep reinforced model|machine learning|data science|deep learning|automatic summarization,https://openalex.org/W2884001105|https://openalex.org/W2970419734|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2888482885
https://openalex.org/W2199803028,Predicting the semantic orientation of adjectives,"Predicting the semantic orientation of adjectives

We identify and validate from a large corpus constraints conjunctions on the positive or negative semantic orientation of conjoined adjectives. A log-linear regression model uses these to predict whether adjectives are same different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining across many adjectives, clustering algorithm separates into groups finally, labeled negative. Evaluations real data simulation experiments indicate high levels performance: classification precision more than 90% for that occur modest number corpus.

semantic orientation, semantic analysis (linguistics), nlp task, natural language processing, semantic interpretation, semantic processing, semantic similarity, semantics, language model, semantic evaluation, linguistics, language, computational linguistics",1997,1212,semantic orientation|semantic analysis (linguistics)|nlp task|natural language processing|semantic interpretation|semantic processing|semantic similarity|semantics|language model|semantic evaluation|linguistics|language|computational linguistics,https://openalex.org/W2108646579|https://openalex.org/W2022204871|https://openalex.org/W4211186029|https://openalex.org/W2084046180|https://openalex.org/W2019759670|https://openalex.org/W2126581182|https://openalex.org/W2031998113|https://openalex.org/W2148506018|https://openalex.org/W2159457224|https://openalex.org/W2088622183|https://openalex.org/W1987425720
https://openalex.org/W239563548,World Atlas of Language Structures,"World Atlas of Language Structures

I INTRODUCTION II THE MAPY AND ACCOMPANYING TEXTS 1. Phonology 2. Morphology 3. Nominal Categories 4. Syntax 5. Verbal 6. Word Order 7. Simple Clauses 8. Complex Sentences 9. Lexicon 10. Sign Languages 11. Other III REFERENCE MATERIAL INDEX CV

language resource, computer science, linguistics, structural linguistics, lexical resource, corpus linguistics, computational linguistics, language, language science, syntactic structure, language structures, structured vocabulary, general linguistics, world atlas, applied linguistics, language corpus, natural language processing, world english, context (linguistics)",2020,1211,language resource|computer science|linguistics|structural linguistics|lexical resource|corpus linguistics|computational linguistics|language|language science|syntactic structure|language structures|structured vocabulary|general linguistics|world atlas|applied linguistics|language corpus|natural language processing|world english|context (linguistics),
https://openalex.org/W1663984431,Advances in natural language processing,"Advances in natural language processing

Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human content. Early approaches to research focused on automating analysis linguistic structure developing basic technologies such as machine translation, speech recognition, synthesis. Today’s researchers refine make use tools in real-world applications, creating spoken dialogue systems speech-to-speech translation engines, mining social media information about health or finance, identifying sentiment emotion toward products services. We describe successes challenges this rapidly advancing area.

linguistics, artificial intelligence, natural language processing, language engineering, machine learning, computational linguistics, nlp task",2015,1210,linguistics|artificial intelligence|natural language processing|language engineering|machine learning|computational linguistics|nlp task,
https://openalex.org/W1593271688,Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,"Proceedings of the 40th Annual Meeting on Association for Computational Linguistics

This year's meeting is special. First, it the first ACL to be hosted jointly with our North-American chapter, NAACL. Second, also ACL's 40th anniversary meeting: association was created 13 June 1962, and its took place 25-26 August 1963 in Denver.Our field has expanded quite a bit since then, so have meetings. year, main conference program includes some 65 different refereed papers, covering most active research areas computational linguistics. And extended complementing that 19 system demonstrations, 4 tutorials, 12 Student Research no less than 11 pre- post-conference workshops presenting total of about 150 papers.

linguistics, natural language processing, natural language generation, computational linguistics, computational semantics, machine translation, corpus linguistics",2002,1206,linguistics|natural language processing|natural language generation|computational linguistics|computational semantics|machine translation|corpus linguistics,https://openalex.org/W2912924812|https://openalex.org/W4239946314
https://openalex.org/W1828401780,TextTiling: segmenting text into multi-paragraph subtopic passages,"TextTiling: segmenting text into multi-paragraph subtopic passages

TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. algorithm fully implemented shown to produce segmentation corresponds well human judgments the boundaries 12 texts. Multi-paragraph should be useful many text analysis tasks, including information retrieval summarization.

multi-paragraph subtopic passages, computer science, text mining, natural language processing, segmenting text, text segmentation, text processing, automatic summarization",1997,1203,multi-paragraph subtopic passages|computer science|text mining|natural language processing|segmenting text|text segmentation|text processing|automatic summarization,https://openalex.org/W2141766660
https://openalex.org/W2963339397,TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension,"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension

We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. show that, in comparison to other recently introduced large-scale datasets, (1) has relatively complex, compositional questions, (2) considerable syntactic lexical variability between questions corresponding answer-evidence sentences, (3) requires more cross sentence reasoning find answers. also two baseline algorithms: feature-based classifier state-of-the-art neural network, performs well SQuAD comprehension. Neither approach comes close human performance (23% 40% vs. 80%), suggesting is testbed worth significant future study.

language learning, large scale, natural language processing, cognitive science, language comprehension, computer science, large language model, language model, machine learning, large-scale datasets, machine learning research, challenge dataset, deep learning, data science, reading research",2017,1202,language learning|large scale|natural language processing|cognitive science|language comprehension|computer science|large language model|language model|machine learning|large-scale datasets|machine learning research|challenge dataset|deep learning|data science|reading research,https://openalex.org/W2896457183|https://openalex.org/W3099700870|https://openalex.org/W2912924812|https://openalex.org/W3011411500|https://openalex.org/W2889787757
https://openalex.org/W2783272285,Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding,"Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding

Top-N sequential recommendation models each user as a sequence of items interacted in the past and aims to predict top-N ranked that will likely interact »near future». The order interaction implies patterns play an important role where more recent have larger impact on next item. In this paper, we propose Convolutional Sequence Embedding Recommendation Model »Caser» solution address requirement. idea is embed into »image» time latent spaces learn local features image using convolutional filters. This approach provides unified flexible network structure for capturing both general preferences patterns. experiments public data sets demonstrated Caser consistently outperforms state-of-the-art methods variety common evaluation metrics.

computer science, natural language processing, top-n sequential recommendation, convolutional sequence, preference learning, machine learning, recommender system, data re-identification, cognitive science, sequential learning, data science, knowledge discovery, deep learning, sequence modelling, data mining",2018,1199,computer science|natural language processing|top-n sequential recommendation|convolutional sequence|preference learning|machine learning|recommender system|data re-identification|cognitive science|sequential learning|data science|knowledge discovery|deep learning|sequence modelling|data mining,
https://openalex.org/W2425121537,MSR-VTT: A Large Video Description Dataset for Bridging Video and Language,"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language

While there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited terms variability and complexity videos their associated language that they can recognize. This is part due to simplicity benchmarks, which mostly focus on specific fine-grained domains simple descriptions. researchers have provided several benchmark datasets for image captioning, we not aware any large-scale description dataset comprehensive categories yet diverse content. In this paper present MSR-VTT (standing ""MSRVideo Text"") a new understanding, especially emerging translating text. achieved by collecting 257 popular queries from commercial search engine, 118 each query. its version, provides 10K web clips 41.2 hours 200K clip-sentence pairs total, covering most visual content, representing largest sentence vocabulary. Each clip annotated about 20 sentences 1,327 AMT workers. We detailed analysis comparison complete set existing datasets, together summarization different state-of-the-art video-to-text approaches. also provide an extensive evaluation these approaches dataset, showing hybrid Recurrent Neural Networkbased approach, combines single-frame motion representations soft-attention pooling strategy, yields best generalization capability MSR-VTT.

language resource, computer science, video retrieval, language model, natural language processing, language, video understanding, computer vision, machine learning, multimedia retrieval, video adaptation, data science, multimedia information processing, deep learning, video interpretation, machine translation, multimedia computing, video summarization",2016,1195,language resource|computer science|video retrieval|language model|natural language processing|language|video understanding|computer vision|machine learning|multimedia retrieval|video adaptation|data science|multimedia information processing|deep learning|video interpretation|machine translation|multimedia computing|video summarization,
https://openalex.org/W2803193013,"The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English","The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English

The RAVDESS is a validated multimodal database of emotional speech and song. gender balanced consisting 24 professional actors, vocalizing lexically-matched statements in neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, disgust expressions, song contains fearful emotions. Each expression produced at two levels intensity, with an additional expression. All conditions are available face-and-voice, face-only, voice-only formats. set 7356 recordings were each rated 10 times on validity, genuineness. Ratings provided by 247 individuals who characteristic untrained research participants from America. A further 72 test-retest data. High validity intrarater reliability reported. Corrected accuracy composite ""goodness"" measures presented to assist researchers the selection stimuli. made freely under Creative Commons license can be downloaded https://doi.org/10.5281/zenodo.1188976.

linguistics, speech recognition, voice recognition, multimodal signal processing, emotion, image analysis, language, emotional speech, multimodal sentiment analysis, vocal expressions, multimodal set, ryerson audio-visual database, facial expression recognition, music, natural language processing, speech processing, affective computing, speech perception, north american english, multimedia retrieval, speech communication, emotion recognition, spoken language technology",2018,1192,linguistics|speech recognition|voice recognition|multimodal signal processing|emotion|image analysis|language|emotional speech|multimodal sentiment analysis|vocal expressions|multimodal set|ryerson audio-visual database|facial expression recognition|music|natural language processing|speech processing|affective computing|speech perception|north american english|multimedia retrieval|speech communication|emotion recognition|spoken language technology,
https://openalex.org/W2951534261,MS MARCO: A Human Generated MAchine Reading COmprehension Dataset,"MS MARCO: A Human Generated MAchine Reading COmprehension Dataset

We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with human generated answer and 182,669 completely rewritten answers. In addition, the contains 8,841,823 passages---extracted 3,563,535 web documents retrieved by Bing---that provide information necessary for curating natural language A question in MARCO may have multiple answers or no at all. Using this propose three different tasks varying levels difficulty: (i) predict if is answerable given set context passages, extract synthesize as would (ii) generate well-formed (if possible) based on passages that can be understood passage context, finally (iii) rank question. size fact questions are derived real user queries distinguishes other well-known publicly available datasets machine reading comprehension question-answering. believe real-world nature makes it attractive benchmarking question-answering models.

natural language processing, language, human generated machine, language comprehension, ms marco, machine learning, reading research, comprehension dataset",2016,1190,natural language processing|language|human generated machine|language comprehension|ms marco|machine learning|reading research|comprehension dataset,https://openalex.org/W2912924812|https://openalex.org/W2963339397|https://openalex.org/W2889787757
https://openalex.org/W4231796555,Computational Linguistics,"Computational Linguistics

Computational linguistics (CL) is an interdisciplinary mix of computer science and with additional insights drawn from areas such as psycholinguistics the philosophy language. Its primary concern computational modeling linguistic processes pursued a theoretically oriented exercise whose purpose either to provide models that help us gain insight into nature language human processing mechanism or support development software applications do useful things (an area sometimes referred technology). For some, term “computational linguistics” synonymous natural (NLP); however, perspective material provided here, NLP more applications-oriented than CL. Although we some pointers work describes applications, our focus here on theoretical underpinnings CL provides activities practical focus. The first in dates 1950s, when initial attempts were made automatically translate Russian English. Until late 1980s most field was concerned what might call symbolic systems, often involving large collections handwritten rules model phenomenon. Since there has been significant shift toward statistical methods, where generalizations are learned data rather being produced manually; this become possible only result combination of, one hand, vast amounts becoming available, particularly via World Wide Web, and, other immense increases power required execute many iterations over sets derive information them. In almost all subareas can generally divide periods before after “statistical revolution.” organized around conventional decomposition study phonology, morphology, syntax, semantics, pragmatics. Work primarily speech recognition synthesis not covered. Primarily English; again, much applicable languages, but English holds privileged position research.

computer science, linguistics, text mining, language model, natural language processing, language, computational linguistics, computational semantics, machine translation",2011,1187,computer science|linguistics|text mining|language model|natural language processing|language|computational linguistics|computational semantics|machine translation,https://openalex.org/W2092654472|https://openalex.org/W2141766660
https://openalex.org/W1585100903,Semantics: An International Handbook of Natural Language Meaning,"Semantics: An International Handbook of Natural Language Meaning

This handbook comprises an in-depth presentation of the state art in linguistic semantics. The articles present detailed, yet accessible, introductions to key issues, including analysis semantic categories and constructions, history research, theories theoretical frameworks, methodology, relationships with related fields They are essential reference for students researchers semantics allied disciplines psychology philosophy.

linguistics, natural language meaning, natural language processing, semantic analysis (linguistics), language, semantics, natural language, semantic interpretation, international handbook",2011,1184,linguistics|natural language meaning|natural language processing|semantic analysis (linguistics)|language|semantics|natural language|semantic interpretation|international handbook,
https://openalex.org/W2153848201,Named Entity Recognition in Tweets: An Experimental Study,"Named Entity Recognition in Tweets: An Experimental Study

People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-ner system doubles F1 score compared Stanford NER system. leverages redundancy inherent tweets achieve performance, using LabeledLDA exploit Freebase dictionaries as source distant supervision. outperforms co-training, increasing 25% over ten common entity types.

Our are available at: http://github.com/aritter/twitter_nlp

pattern recognition, computer science, keyword extraction, text mining, natural language processing, information extraction, entity recognition, social medium monitoring, social media, machine learning, communication, named-entity recognition, nlp task, journalism, experimental study, online information",2011,1181,pattern recognition|computer science|keyword extraction|text mining|natural language processing|information extraction|entity recognition|social medium monitoring|social media|machine learning|communication|named-entity recognition|nlp task|journalism|experimental study|online information,
https://openalex.org/W2742947407,Recent Trends in Deep Learning Based Natural Language Processing,"Recent Trends in Deep Learning Based Natural Language Processing

Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety model designs blossomed the context natural language (NLP). In this paper, we review significant deep related models that been employed for numerous NLP tasks provide walk-through their evolution. We also summarize, compare contrast various put forward detailed understanding past, present future NLP.

computer science, linguistics, language model, natural language processing, language, machine learning, computational linguistics, deep learning, natural language, nlp task",2017,1179,computer science|linguistics|language model|natural language processing|language|machine learning|computational linguistics|deep learning|natural language|nlp task,https://openalex.org/W3019166713
https://openalex.org/W2162311237,The Unified Medical Language System,"The Unified Medical Language System

In 1986, the National Library of Medicine began a long-term research and development project to build Unified Medical Language System (UMLS). The purpose UMLS is improve ability computer programs ""understand"" biomedical meaning in user inquiries use this understanding retrieve integrate relevant machine-readable information for users. Underlying effort assumption that timely access accurate up-to-date will decision making ultimately quality patient care research. distributed national experiment with strong element international collaboration. general strategy develop components through series successive approximations capabilities desired. Three experimental Knowledge Sources, Metathesaurus, Semantic Network, Information Sources Map have been developed are annually interested researchers, many whom tested evaluated them range applications. current developments high-speed, high-capacity networks converging ways great potential enhancing information.

health informatics, medical system, medical ontology, biomedical engineering, medical information system, clinical system, medical expert system, language, natural language processing, biomedical informatics, biomedical text mining, digital medicine, medicine, language model",1993,1177,health informatics|medical system|medical ontology|biomedical engineering|medical information system|clinical system|medical expert system|language|natural language processing|biomedical informatics|biomedical text mining|digital medicine|medicine|language model,https://openalex.org/W2159583324|https://openalex.org/W2169818249
https://openalex.org/W2152263452,A hierarchical phrase-based model for statistical machine translation,"A hierarchical phrase-based model for statistical machine translation

We present a statistical phrase-based translation model that uses hierarchical phrases---phrases contain subphrases. The is formally synchronous context-free grammar but learned from bitext without any syntactic information. Thus it can be seen as shift to the formal machinery of syntax-based systems linguistic commitment. In our experiments using BLEU metric, achieves relative improvement 7.5% over Pharaoh, state-of-the-art system.

linguistics, hierarchical phrase-based model, natural language processing, language model, machine learning, statistical machine translation, computational linguistics, machine translation",2005,1175,linguistics|hierarchical phrase-based model|natural language processing|language model|machine learning|statistical machine translation|computational linguistics|machine translation,https://openalex.org/W1663984431
https://openalex.org/W2008806374,ICDAR 2013 Robust Reading Competition,"ICDAR 2013 Robust Reading Competition

This report presents the final results of ICDAR 2013 Robust Reading Competition. The competition is structured in three Challenges addressing text extraction different application domains, namely born-digital images, real scene images and real-scene videos. are organised around specific tasks covering localisation, segmentation word recognition. took place first quarter 2013, received a total 42 submissions over offered. describes datasets ground truth specification, details performance evaluation protocols used along with brief summary participating methods.

image analysis, pattern recognition, computer science, natural language processing, text recognition, optical character recognition, computer vision, communication, multimedia retrieval, reading research, multimodal signal processing, robust speech recognition, text segmentation, multimedia information processing, image representation, machine vision, digital image processing, computer engineering",2013,1171,image analysis|pattern recognition|computer science|natural language processing|text recognition|optical character recognition|computer vision|communication|multimedia retrieval|reading research|multimodal signal processing|robust speech recognition|text segmentation|multimedia information processing|image representation|machine vision|digital image processing|computer engineering,https://openalex.org/W2605982830|https://openalex.org/W2144554289|https://openalex.org/W2343052201
https://openalex.org/W2101460669,The Minimum Description Length Principle,"The Minimum Description Length Principle

The minimum description length (MDL) principle is a powerful method of inductive inference, the basis statistical modeling, pattern recognition, and machine learning. It holds that best explanation, given limited set observed data, one permits greatest compression data. MDL methods are particularly well-suited for dealing with model selection, prediction, estimation problems in situations where models under consideration can be arbitrarily complex, overfitting data serious concern.This extensive, step-by-step introduction to Principle provides comprehensive reference (with an emphasis on conceptual issues) accessible graduate students researchers statistics, classification, learning, mining, philosophers interested foundations other applied sciences involve including biology, econometrics, experimental psychology. Part I basic overview concepts statistics information theory needed understand MDL. II treats universal coding, information-theoretic notion which built, part III gives formal treatment as inference based coding. IV exponential families their properties. text includes number summaries, paragraphs offering reader fast track through material, boxes highlighting most important concepts.

description logic, knowledge discovery, natural language processing, computer science, logic in computer science, language model, semantic evaluation, descriptional complexity",2007,1168,description logic|knowledge discovery|natural language processing|computer science|logic in computer science|language model|semantic evaluation|descriptional complexity,
https://openalex.org/W2130339025,Optimizing Semantic Coherence in Topic Models,"Optimizing Semantic Coherence in Topic Models

Latent variable models have the potential to add value large document collections by discovering interpretable, low-dimensional subspaces. In order for people use such models, however, they must trust them. Unfortunately, typical dimensionality reduction methods text, as latent Dirichlet allocation, often produce subspaces (topics) that are obviously flawed human domain experts. The contributions of this paper threefold: (1) An analysis ways in which topics can be flawed; (2) an automated evaluation metric identifying does not rely on annotators or reference outside training data; (3) a novel statistical topic model based significantly improves quality large-scale collection from National Institutes Health (NIH).

semantic coherence, topic model, natural language processing, data science, semantic processing, semantic interpretation",2011,1167,semantic coherence|topic model|natural language processing|data science|semantic processing|semantic interpretation,https://openalex.org/W2038043464|https://openalex.org/W1714665356
https://openalex.org/W1423339008,Parsing Natural Scenes and Natural Language with Recursive Neural Networks,"Parsing Natural Scenes and Natural Language with Recursive Neural Networks

Recursive structure is commonly found in the inputs of different modalities such as natural scene images or language sentences. Discovering this recursive helps us to not only identify units that an image sentence contains but also how they interact form a whole. We introduce max-margin prediction architecture based on neural networks can successfully recover both complex well The same algorithm be used provide competitive syntactic parser for sentences from Penn Treebank and outperform alternative approaches semantic segmentation, annotation classification. For segmentation our obtains new level state-of-the-art performance Stanford background dataset (78.1%). features parse tree Gist descriptors classification by 4%.

computer science, natural language processing, machine learning, recursive neural networks, natural scenes, natural language, syntactic parsing",2011,1166,computer science|natural language processing|machine learning|recursive neural networks|natural scenes|natural language|syntactic parsing,https://openalex.org/W2153579005|https://openalex.org/W2950133940|https://openalex.org/W2251939518|https://openalex.org/W2949547296|https://openalex.org/W2131744502|https://openalex.org/W2884001105|https://openalex.org/W2126725946|https://openalex.org/W2742947407|https://openalex.org/W71795751|https://openalex.org/W2250879510|https://openalex.org/W2170738476|https://openalex.org/W2963042536
https://openalex.org/W2966715458,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks,"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

We present ViLBERT (short for Vision-and-Language BERT), a model learning task-agnostic joint representations of image content and natural language. extend the popular BERT architecture to multi-modal two-stream model, pro-cessing both visual textual inputs in separate streams that interact through co-attentional transformer layers. pretrain our two proxy tasks on large, automatically collected Conceptual Captions dataset then transfer it multiple established vision-and-language -- question answering, commonsense reasoning, referring expressions, caption-based retrieval by making only minor additions base architecture. observe significant improvements across compared existing task-specific models achieving state-of-the-art all four tasks. Our work represents shift away from groundings between vision language as part task training towards treating grounding pretrainable transferable capability.

language learning, human-computer interaction, task analysis, machine vision, nlp task, natural language processing, vision language model, cognitive science, language comprehension, computer science, multilingual pretraining, language model, vision-and-language tasks, language processing in the brain, image representation, language, deep learning, image analysis",2019,1166,language learning|human-computer interaction|task analysis|machine vision|nlp task|natural language processing|vision language model|cognitive science|language comprehension|computer science|multilingual pretraining|language model|vision-and-language tasks|language processing in the brain|image representation|language|deep learning|image analysis,https://openalex.org/W3098605233
https://openalex.org/W71795751,Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions,"Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions

We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations multi-word phrases. In tasks these outperform other state-of-the-art approaches commonly used datasets, such as movie reviews, without using any pre-defined lexica or polarity shifting rules. also evaluate the model's ability to predict distributions new dataset confessions from experience project. The consists personal user stories annotated with multiple labels which, when aggregated, form multinomial distribution that captures emotional reactions. algorithm can more accurately over compared several competitive baselines.

autoencoders, computer science, natural language processing, predictive learning, semi-supervised recursive autoencoders, machine learning, deep learning, semi-supervised learning, sentiment distributions",2011,1165,autoencoders|computer science|natural language processing|predictive learning|semi-supervised recursive autoencoders|machine learning|deep learning|semi-supervised learning|sentiment distributions,https://openalex.org/W1832693441|https://openalex.org/W2251939518|https://openalex.org/W2949547296|https://openalex.org/W2108646579|https://openalex.org/W2120615054|https://openalex.org/W2131744502|https://openalex.org/W2884001105|https://openalex.org/W2562607067|https://openalex.org/W2265846598|https://openalex.org/W2949541494|https://openalex.org/W2964236337|https://openalex.org/W2742947407|https://openalex.org/W2250879510|https://openalex.org/W2170738476|https://openalex.org/W2252215182
https://openalex.org/W196214544,Generating Text with Recurrent Neural Networks,"Generating Text with Recurrent Neural Networks

Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able overcome the difficulties associated with training RNNs, making possible apply successfully challenging problems. In this paper we demonstrate power of RNNs trained new Hessian-Free optimizer (HF) by applying character-level language modeling tasks. The standard RNN architecture, while effective, ideally suited for such tasks, so introduce a variant uses multiplicative (or gated) connections which allow current input character determine transition matrix from one hidden state vector next. After HF five days on 8 high-end Graphics Processing Units, were surpass performance best previous single method – hierarchical non-parametric model. To our knowledge represents largest recurrent neural network application date.

computer science, text mining, natural language processing, large language model, natural language generation, machine learning, recurrent neural network, data science, computational intelligence, deep learning, language generation, neural network (machine learning), language learning",2011,1165,computer science|text mining|natural language processing|large language model|natural language generation|machine learning|recurrent neural network|data science|computational intelligence|deep learning|language generation|neural network (machine learning)|language learning,https://openalex.org/W2493916176|https://openalex.org/W2884001105|https://openalex.org/W1938755728|https://openalex.org/W2742947407|https://openalex.org/W3105625590|https://openalex.org/W2963758027|https://openalex.org/W2880875857|https://openalex.org/W2963042536
https://openalex.org/W2129120544,A statistical model-based voice activity detection,"A statistical model-based voice activity detection

In this letter, we develop a robust voice activity detector (VAD) for the application to variable-rate speech coding. The developed VAD employs decision-directed parameter estimation method likelihood ratio test. addition, propose an effective hang-over scheme which considers previous observations by first-order Markov process modeling of occurrences. According our simulation results, proposed shows significantly better performances than G.729B in low signal-to-noise (SNR) and vehicular noise environments.

computer science, speech recognition, signal processing, natural language processing, speech processing, speech analysis, voice recognition, digital signal processing, voice technology, acoustic signal processing, data science, statistical model",1999,1162,computer science|speech recognition|signal processing|natural language processing|speech processing|speech analysis|voice recognition|digital signal processing|voice technology|acoustic signal processing|data science|statistical model,
https://openalex.org/W2886641317,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning","Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning

We present a new dataset of image caption annotations, Conceptual Captions, which contains an order magnitude more images than the MS-COCO (Lin et al., 2014) and represents wider variety both styles. achieve this by extracting filtering annotations from billions webpages. also quantitative evaluations number captioning models show that model architecture based on Inception-ResNetv2 (Szegedy 2016) for image-feature extraction Transformer (Vaswani 2017) sequence modeling achieves best performance when trained Captions dataset.

image search, computer science, machine learning, image analysis, hypertext, image alt-text dataset, image retrieval, cognitive science, data science, image representation, computational imaging, deep learning, machine learning research, machine vision, conceptual captions, automatic image captioning, natural language processing, scene interpretation, computer vision, multimedia retrieval, automatic annotation",2018,1162,image search|computer science|machine learning|image analysis|hypertext|image alt-text dataset|image retrieval|cognitive science|data science|image representation|computational imaging|deep learning|machine learning research|machine vision|conceptual captions|automatic image captioning|natural language processing|scene interpretation|computer vision|multimedia retrieval|automatic annotation,https://openalex.org/W2966715458|https://openalex.org/W2968124245
https://openalex.org/W2123782500,Adaptive support-weight approach for correspondence search,"Adaptive support-weight approach for correspondence search

We present a new window-based method for correspondence search using varying support-weights. adjust the support-weights of pixels in given support window based on color similarity and geometric proximity to reduce image ambiguity. Our outperforms other local methods standard stereo benchmarks.

pattern recognition, computer science, correspondence search, natural language processing, pattern matching, tabu search, graph matching, information retrieval, data mining",2006,1159,pattern recognition|computer science|correspondence search|natural language processing|pattern matching|tabu search|graph matching|information retrieval|data mining,
https://openalex.org/W2972324944,What Does BERT Look at? An Analysis of BERT’s Attention,"What Does BERT Look at? An Analysis of BERT’s Attention

Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects language they are able to learn from unlabeled data. Most analysis has focused on model outputs (e.g., surprisal) or internal vector representations probing classifiers). Complementary these works, we propose methods for analyzing the attention mechanisms models and apply them BERT. BERT’s heads exhibit patterns attending delimiter tokens, specific positional offsets, broadly over whole sentence, with same layer often exhibiting similar behaviors. We further show that certain correspond well linguistic notions syntax coreference. For example, find attend direct objects verbs, determiners nouns, prepositions, coreferent mentions remarkably high accuracy. Lastly, an attention-based classifier use it demonstrate substantial syntactic information is captured attention.

psychology, behavioral sciences, developmental psychology, natural language processing, attention, cognitive science, eye tracking, behavioral research, behavioral neuroscience, psychological assessment, behavioral insight, emotion",2019,1159,psychology|behavioral sciences|developmental psychology|natural language processing|attention|cognitive science|eye tracking|behavioral research|behavioral neuroscience|psychological assessment|behavioral insight|emotion,https://openalex.org/W3105966348|https://openalex.org/W2968124245
https://openalex.org/W3105625590,Text Classification Algorithms: A Survey,"Text Classification Algorithms: A Survey

In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding machine learning methods to be able accurately classify many applications. Many approaches have achieved surpassing results natural language processing. The success these algorithms relies on their capacity understand models non-linear relationships within data. However, finding suitable structures, architectures, techniques for text classification is challenge researchers. this paper, brief overview discussed. This covers different feature extractions, dimensionality reduction methods, existing techniques, evaluations methods. Finally, limitations each technique application real-world problem are

pattern recognition, computer science, keyword extraction, text mining, natural language processing, text recognition, machine learning, data science, knowledge discovery, content similarity detection, machine learning research, text processing, automatic classification, information retrieval, text classification algorithms",2019,1155,pattern recognition|computer science|keyword extraction|text mining|natural language processing|text recognition|machine learning|data science|knowledge discovery|content similarity detection|machine learning research|text processing|automatic classification|information retrieval|text classification algorithms,
https://openalex.org/W2130337399,Automatic word sense discrimination,"Automatic word sense discrimination

This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts the ambiguous word. Words, contexts, and senses represented in Word Space, high-dimensional, real-valued space which closeness corresponds to semantic similarity. Similarity Space is second-order co-occurrence: two tokens contexts) word assigned same sense cluster if words they co-occur with turn occur training corpus. The automatic unsupervised both application: induced from corpus without labeled instances or other external knowledge sources. demonstrates good performance discrimination for sample natural artificial words.

word-sense disambiguation, distributional semantics, knowledge discovery, computational semantics, knowledge representation and reasoning, natural language processing, semantic processing, semantic similarity, speech processing, computer science, semantics, semantic evaluation, text mining, linguistics, language, computational linguistics",1998,1154,word-sense disambiguation|distributional semantics|knowledge discovery|computational semantics|knowledge representation and reasoning|natural language processing|semantic processing|semantic similarity|speech processing|computer science|semantics|semantic evaluation|text mining|linguistics|language|computational linguistics,https://openalex.org/W2118020653|https://openalex.org/W2436001372|https://openalex.org/W1916559533
https://openalex.org/W2946417913,BERT Rediscovers the Classical NLP Pipeline,"BERT Rediscovers the Classical NLP Pipeline

Pre-trained text encoders have rapidly advanced the state of art on many NLP tasks. We focus one such model, BERT, and aim to quantify where linguistic information is captured within network. find that model represents steps traditional pipeline in an interpretable localizable way, regions responsible for each step appear expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals can often does adjust this dynamically, revising lower-level decisions basis disambiguating from higher-level representations.

classical nlp pipeline, natural language processing, computer science, nlp task",2019,1147,classical nlp pipeline|natural language processing|computer science|nlp task,https://openalex.org/W2979826702|https://openalex.org/W2980282514|https://openalex.org/W3098824823|https://openalex.org/W2972324944
https://openalex.org/W2949999304,Generative Adversarial Text to Image Synthesis,"Generative Adversarial Text to Image Synthesis

Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far this goal. However, in recent years generic powerful recurrent neural network architectures have been developed to learn discriminative feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) begun generate highly compelling specific categories, such as faces, album covers, room interiors. In work, we develop a novel architecture GAN formulation effectively bridge these advances image model- ing, translating visual concepts characters pixels. We demonstrate the capability our model plausible birds flowers detailed descriptions.

computer science, computer vision, machine learning, computational intelligence, generative adversarial network, digital image processing, computational imaging, generative adversarial text, generative ai, synthetic image generation, natural language processing, image synthesis, data science, deep learning, digital imaging",2016,1143,computer science|computer vision|machine learning|computational intelligence|generative adversarial network|digital image processing|computational imaging|generative adversarial text|generative ai|synthetic image generation|natural language processing|image synthesis|data science|deep learning|digital imaging,
https://openalex.org/W4365799947,Proceedings of the 28th International Conference on Computational Linguistics,"Proceedings of the 28th International Conference on Computational Linguistics

Only eighteen months ago, I joined Leo, Horacio, Mónica, and Nuria on a tour of the delights that Barcelona venue would be offering us in September 2020.Together with Chengqing, we made great plans for fantastic intellectual social gathering our colleagues from far wide, Catalanstyle.We held to this plan as long could but second wave COVID-19 pandemic eventually put paid idea, conference is now virtual one.We also received record 2195 submissions -eclipsing by double already record-breaking number at last COLING 2018 Santa Fe.For these reasons, COLING2020 will remembered forced innovate do things differently.Transforming well-made become has obviously involved introducing de facto disruptive innovation, one taken out comfort zone experience.The Organising Committee thoroughly embraced challenge, doing everything they more; am awe them exciting have created COLING2020.We before rich programme over 653 papers, 7 tutorials, 22 workshops.I want take opportunity thank entire their extraordinary effort helping make possible.The PC Chairs (Núria Bel, Chengqing Zong) Local Organisation (Leo Wanner, Horacio Saggion, Mónica Domínguez) bore brunt organisational burden worked tirelessly despite other unexpected demands professional personal lives.Enormous efforts were Workshop (

linguistics, natural language processing, language model, computational linguistics, computational semantics",2020,1133,linguistics|natural language processing|language model|computational linguistics|computational semantics,
https://openalex.org/W2168041406,"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text","2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text

Abstract The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused the of medical concepts from patient reports; an assertion classification assigning types problem concepts; and relation that hold between problems, tests, treatments. i2b2 VA provided annotated reference standard corpus tasks. Using this standard, 22 systems were developed extraction, 21 classification, 16 classification. These showed machine learning approaches could be augmented with rule-based to determine concepts, assertions, relations. Depending task, can either provide input or post-process output learning. Ensembles classifiers, information unlabeled data, external knowledge sources help when training data are inadequate.

linguistics, clinical text, natural language processing, medical guideline, biomedical informatics",2011,1130,linguistics|clinical text|natural language processing|medical guideline|biomedical informatics,https://openalex.org/W2911489562|https://openalex.org/W2169818249
https://openalex.org/W2134800885,KenLM: Faster and Smaller Language Model Queries,"KenLM: Faster and Smaller Language Model Queries

We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The Probing structure uses linear probing hash tables is designed speed. Compared with the widely-used SRILM, our 2.4 times as fast while using 57% of memory. Trie trie bit-level packing, sorted records, interpolation search, optional quantization aimed at lower consumption. simultaneously less than smallest lossless baseline CPU fastest baseline. Our code open-source, thread-safe, integrated into Moses, cdec, Joshua translation systems. This paper describes several performance techniques used presents benchmarks against alternative implementations.

computer science, text mining, language model, natural language processing, language, large language model, cross-language retrieval, query optimization, multilingual pretraining, computational linguistics, data science, computational intelligence, knowledge discovery, machine learning research, machine translation, information retrieval",2011,1126,computer science|text mining|language model|natural language processing|language|large language model|cross-language retrieval|query optimization|multilingual pretraining|computational linguistics|data science|computational intelligence|knowledge discovery|machine learning research|machine translation|information retrieval,
https://openalex.org/W3019166713,A Survey of the Usages of Deep Learning for Natural Language Processing,"A Survey of the Usages of Deep Learning for Natural Language Processing

Over the last several years, field of natural language processing has been propelled forward by an explosion in use deep learning models. This article provides a brief introduction to and quick overview architectures methods. It then sifts through plethora recent studies summarizes large assortment relevant contributions. Analyzed research areas include core linguistic issues addition many applications computational linguistics. A discussion current state art is provided along with recommendations for future field.

computer science, linguistics, large language model, language model, natural language processing, language, language engineering, natural language interface, machine learning, computational linguistics, deep learning, natural language, nlp task, machine translation, spoken language technology",2021,1126,computer science|linguistics|large language model|language model|natural language processing|language|language engineering|natural language interface|machine learning|computational linguistics|deep learning|natural language|nlp task|machine translation|spoken language technology,
https://openalex.org/W2963758027,DenseCap: Fully Convolutional Localization Networks for Dense Captioning,"DenseCap: Fully Convolutional Localization Networks for Dense Captioning

We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images natural language. The task generalizes object detection when descriptions consist of single word, Image Captioning one predicted region covers full image. To address localization description jointly we propose Fully Convolutional Localization Network (FCLN) architecture that processes an image with single, efficient forward pass, no external proposals, can be trained end-to-end round optimization. is composed Network, novel layer, Recurrent Neural language model generates label sequences. evaluate our network on Visual Genome dataset, comprises 94,000 4,100,000 region-grounded captions. observe speed accuracy improvements over baselines based current state art approaches generation retrieval settings.

computer science, machine learning, convolutional localization networks, image analysis, information fusion, convolutional neural network, feature detection, cognitive science, object detection, data science, dense captioning, image representation, scene understanding, deep learning, machine learning research, machine vision, natural language processing, feature extraction, computer vision",2016,1117,computer science|machine learning|convolutional localization networks|image analysis|information fusion|convolutional neural network|feature detection|cognitive science|object detection|data science|dense captioning|image representation|scene understanding|deep learning|machine learning research|machine vision|natural language processing|feature extraction|computer vision,
https://openalex.org/W2889787757,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering","HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D. Manning. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018.

domain model, knowledge representation and reasoning, question answering, computer science, machine learning, computational intelligence, knowledge discovery, explainable multi-hop question, information fusion, artificial intelligence, explanation-based learning, information retrieval, natural language processing, data science, language learning",2018,1110,domain model|knowledge representation and reasoning|question answering|computer science|machine learning|computational intelligence|knowledge discovery|explainable multi-hop question|information fusion|artificial intelligence|explanation-based learning|information retrieval|natural language processing|data science|language learning,https://openalex.org/W2912924812
https://openalex.org/W2027676989,Learning Words from Context,"Learning Words from Context

SCHOOL CHILDREN appear to increase their vocabularies by thousands of words per year. Many have hypothesized that a large proportion this growth occurs through incidental learning from written context. However, experimental research has until now failed provide unequivocal support hypothesis. The present study attempted determine whether students do acquire measurable knowledge about unfamiliar while reading natural text. Fifty-seven eighth-grade average and above ability read either an expository or narrative text 1,000 in length. After reading, subjects completed two vocabulary assessment tasks on 15 target each passage (thus serving as controls for the not read), individual interview multiple-choice test, both designed tap partial word meanings. Results within-subject, hierarchical regression analyses showed small but statistically reliable gains Tentative extrapolations results current estimates volume children's lead us believe context accounts substantial during school years.

context model, linguistics, natural language processing, language, semantic evaluation, second language learning, lexical semantics, cognitive science, language acquisition, semantics, semantic processing, deep learning, distributional semantics, semantic learning, language learning, context (linguistics), second language acquisition",1985,1110,context model|linguistics|natural language processing|language|semantic evaluation|second language learning|lexical semantics|cognitive science|language acquisition|semantics|semantic processing|deep learning|distributional semantics|semantic learning|language learning|context (linguistics)|second language acquisition,
https://openalex.org/W2167980479,NCBI GEO: mining millions of expression profiles--database and tools,"NCBI GEO: mining millions of expression profiles--database and tools

The Gene Expression Omnibus (GEO) at the National Center for Biotechnology Information (NCBI) is largest fully public repository high-throughput molecular abundance data, primarily gene expression data. database has a flexible and open design that allows submission, storage retrieval of many data types. These include microarray-based experiments measuring mRNA, genomic DNA protein molecules, as well non-array-based technologies such serial analysis (SAGE) mass spectrometry proteomic technology. GEO currently holds over 30 000 submissions representing approximately half billion individual measurements, 100 organisms. Here, we describe recent developments facilitate effective mining visualization these Features are provided to examine from both experiment- gene-centric perspectives using user-friendly Web-based interfaces accessible those without computational or microarray-related analytical expertise. publicly through World Wide Web http://www.ncbi.nlm.nih.gov/geo .

computer science, machine learning, big data search, fuzzy set, bioinformatics, information fusion, ncbi geo, data privacy, profiling technique, data science, frequent pattern mining, gene expression profiling, data mining, mining millions, machine learning research, expression profiles, pattern mining, text mining, natural language processing, geography, knowledge discovery",2004,1106,computer science|machine learning|big data search|fuzzy set|bioinformatics|information fusion|ncbi geo|data privacy|profiling technique|data science|frequent pattern mining|gene expression profiling|data mining|mining millions|machine learning research|expression profiles|pattern mining|text mining|natural language processing|geography|knowledge discovery,https://openalex.org/W2154362705
https://openalex.org/W2252123671,Abstract Meaning Representation for Sembanking,"Abstract Meaning Representation for Sembanking

We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands English sentences. hope that sembank simple, whole-sentence structures will spur new work statistical natural understanding and generation, like Penn Treebank encouraged on parsing. This paper gives an overview AMR tools associated with it.

semantic representation, computer science, knowledge representation and reasoning, linguistics, natural language processing, language, semantic evaluation, abstract interpretation, abstract meaning representation, semantics, nlp task, semantic interpretation, semantic parsing",2013,1100,semantic representation|computer science|knowledge representation and reasoning|linguistics|natural language processing|language|semantic evaluation|abstract interpretation|abstract meaning representation|semantics|nlp task|semantic interpretation|semantic parsing,
https://openalex.org/W2150692003,Automatic linguistic indexing of pictures by a statistical modeling approach,"Automatic linguistic indexing of pictures by a statistical modeling approach

Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to problem. Categorized images are used train dictionary hundreds models each representing concept. Images any given concept regarded as instances stochastic process that characterizes the To measure extent association between textual description concept, likelihood occurrence based on characterizing computed. A high indicates strong association. our experimental implementation, focus particular group processes, is, two-dimensional multiresolution hidden Markov (2D MHMMs). We implemented tested ALIP (Automatic Linguistic Indexing Pictures) system photographic database 600 different concepts, with about 40 training images. The evaluated quantitatively using more than 4,600 outside compared random annotation scheme. Experiments have demonstrated good accuracy its potential

image analysis, word embeddings, linguistics, keyword extraction, language model, natural language processing, semantic similarity, automatic linguistic indexing, machine learning, computational linguistics, data science, automatic annotation, machine learning research, automatic summarization",2003,1096,image analysis|word embeddings|linguistics|keyword extraction|language model|natural language processing|semantic similarity|automatic linguistic indexing|machine learning|computational linguistics|data science|automatic annotation|machine learning research|automatic summarization,
https://openalex.org/W2250879510,Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification,"Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification

We present a method that learns word embedding for Twitter sentiment classification in this paper.Most existing algorithms learning continuous representations typically only model the syntactic context of words but ignore text.This is problematic analysis as they usually map with similar opposite polarity, such good and bad, to neighboring vectors.We address issue by sentimentspecific (SSWE), which encodes information representation words.Specifically, we develop three neural networks effectively incorporate supervision from polarity text (e.g.sentences or tweets) their loss functions.To obtain large scale training corpora, learn sentiment-specific massive distant-supervised tweets collected positive negative emoticons.Experiments on applying SS-WE benchmark dataset SemEval 2013 show (1) SSWE feature performs comparably hand-crafted features top-performed system; (2) performance further improved concatenating set.

word embeddings, computer science, disinformation detection, text mining, natural language processing, machine learning, sentiment-specific word, data science, knowledge discovery, deep learning, content similarity detection, social media, twitter sentiment classification",2014,1095,word embeddings|computer science|disinformation detection|text mining|natural language processing|machine learning|sentiment-specific word|data science|knowledge discovery|deep learning|content similarity detection|social media|twitter sentiment classification,https://openalex.org/W2884001105|https://openalex.org/W2964236337|https://openalex.org/W2250966211
https://openalex.org/W2218318129,Contractive Auto-Encoders: Explicit Invariance During Feature Extraction,"Contractive Auto-Encoders: Explicit Invariance During Feature Extraction

We present in this paper a novel approach for training deterministic auto-encoders. show that by adding well chosen penalty term to the classical reconstruction cost function, we can achieve results equal or surpass those attained other regularized auto-encoders as denoising on range of datasets. This corresponds Frobenius norm Jacobian matrix encoder activations with respect input. localized space contraction which turn yields robust features activation layer. Furthermore, how is related both and it be seen link between non-deterministic find empirically helps carve representation better captures local directions variation dictated data, corresponding lower-dimensional non-linear manifold, while being more invariant vast majority orthogonal manifold. Finally, using learned initialize MLP, state art classification error datasets, surpassing methods pretraining.

image analysis, pattern recognition, computer science, autoencoders, natural language processing, feature learning, feature extraction, machine learning, feature detection, explicit invariance, data science, deep learning, machine learning research, feature (computer vision), contractive auto-encoders",2011,1095,image analysis|pattern recognition|computer science|autoencoders|natural language processing|feature learning|feature extraction|machine learning|feature detection|explicit invariance|data science|deep learning|machine learning research|feature (computer vision)|contractive auto-encoders,
https://openalex.org/W2021032538,How Many Levels of Processing Are There in Lexical Access?,"How Many Levels of Processing Are There in Lexical Access?

Abstract The patterns of semantic errors in speaking and writing are used to constrain claims about the structure lexical access mechanisms speech written language production. It is argued that it not necessary postulate a modality-neutral level representation (lemma) intermediate between lexical-semantic representations modality-specific representations. A dual-stage model proposed which first stage involves selection semantically syntactically specified, forms, second specific phonological (orthographic) content for selected lexemes.

linguistics, semantic processing, natural language processing, lexical access",1997,1094,linguistics|semantic processing|natural language processing|lexical access,
https://openalex.org/W2148506018,Sentiment analysis,"Sentiment analysis

This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from document, instead classifying the whole document into negative.The essential issues in are identify how expressed texts and whether expressions indicate (favorable) (unfavorable) opinions toward subject. In order improve accuracy analysis, it is important properly semantic relationships between By applying syntactic parser lexicon, our prototype system achieved high precision (75-95%, depending on data) finding within Web pages news articles.

semantic analysis (linguistics), text mining, affective computing, natural language processing, applied linguistics, bias detection, multimodal sentiment analysis, opinion aggregation, linguistics, content analysis, disinformation detection, poetics, sentiment analysis, discourse analysis, context (linguistics), narrative, cognitive science, semantic evaluation, communication",2003,1091,semantic analysis (linguistics)|text mining|affective computing|natural language processing|applied linguistics|bias detection|multimodal sentiment analysis|opinion aggregation|linguistics|content analysis|disinformation detection|poetics|sentiment analysis|discourse analysis|context (linguistics)|narrative|cognitive science|semantic evaluation|communication,https://openalex.org/W2108646579|https://openalex.org/W2022204871|https://openalex.org/W4211186029|https://openalex.org/W2562607067|https://openalex.org/W2126581182|https://openalex.org/W1964613733
https://openalex.org/W2163107094,GENIA corpus—a semantically annotated corpus for bio-textmining,"GENIA corpus—a semantically annotated corpus for bio-textmining

Abstract Motivation: Natural language processing (NLP) methods are regarded as being useful to raise the potential of text mining from biological literature. The lack an extensively annotated corpus this literature, however, causes a major bottleneck for applying NLP techniques. GENIA is developed provide reference materials let techniques work bio-textmining. Results: version 3.0 consisting 2000 MEDLINE abstracts has been released with more than 400 000 words and almost 100 annotations terms. Availability: freely available at http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA Keywords: Text Mining, Information Extraction, Corpus, Language Processing, Computational Molecular Biology *To whom correspondence should be addressed.

language corpus, genia corpus, computer science, text mining, natural language processing, biomedical text mining, annotation tool, data science, knowledge discovery, text processing, genomics, machine translation, corpus linguistics, biomedical informatics, bioinformatics",2003,1085,language corpus|genia corpus|computer science|text mining|natural language processing|biomedical text mining|annotation tool|data science|knowledge discovery|text processing|genomics|machine translation|corpus linguistics|biomedical informatics|bioinformatics,https://openalex.org/W2970771982
https://openalex.org/W1904457459,A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER),"A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER)

Describes a system developed at NIST to produce composite automatic speech recognition (ASR) output when the outputs of multiple ASR systems are available, and for which, in many cases, has lower error rate than any individual systems. The implements ""voting"" or rescoring process reconcile differences outputs. We refer this as Recognizer Output Voting Error Reduction (ROVER) system. As additional knowledge sources added an (e.g. acoustic language models), rates typically decreased. This paper describes post-recognition which models generated by independent that can be combined used generate with reduced rate. To accomplish this, into single, minimal-cost word transition network (WTN) via iterative applications dynamic programming (DP) alignments. resulting is searched selects sequence lowest score.

post-processing system, pattern recognition, computer science, linguistics, speech recognition, language model, natural language processing, language, speech processing, text recognition, error reduction, computational linguistics, language recognition, text processing, machine learning research, post-processing, recognizer output, error correction",2002,1082,post-processing system|pattern recognition|computer science|linguistics|speech recognition|language model|natural language processing|language|speech processing|text recognition|error reduction|computational linguistics|language recognition|text processing|machine learning research|post-processing|recognizer output|error correction,
https://openalex.org/W2306941105,Affective Computing and Sentiment Analysis,"Affective Computing and Sentiment Analysis

Understanding emotions is an important aspect of personal development and growth, as such it a key tile for the emulation human intelligence. Besides being advancement AI, emotion processing also closely related task polarity detection. The opportunity to automatically capture general public's sentiments about social events, political movements, marketing campaigns, product preferences has raised interest in both scientific community, exciting open challenges, business world, remarkable fallouts financial market prediction. This led emerging fields affective computing sentiment analysis, which leverage human-computer interaction, information retrieval, multimodal signal distilling people's from ever-growing amount online data.

computer science, disinformation detection, text mining, natural language processing, affective computing, multimodal sentiment analysis, emotion recognition, sentiment analysis",2016,1081,computer science|disinformation detection|text mining|natural language processing|affective computing|multimodal sentiment analysis|emotion recognition|sentiment analysis,
https://openalex.org/W131533222,Automatically Constructing a Corpus of Sentential Paraphrases.,"Automatically Constructing a Corpus of Sentential Paraphrases.

An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora sentential paraphrases. This paper describes creation recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as whether pair constitutes paraphrase. The corpus was created using heuristic extraction techniques conjunction an SVM-based classifier select likely sentence-level paraphrases from large topicclustered news data. These pairs were then submitted human judges, who confirmed that 67% fact semantically equivalent. In addition describing itself, we explore number issues arose defining guidelines for raters.

sentential paraphrases, computer science, paraphrase, natural language processing, computational linguistics, semantic interpretation, corpus linguistics",2005,1078,sentential paraphrases|computer science|paraphrase|natural language processing|computational linguistics|semantic interpretation|corpus linguistics,https://openalex.org/W2896457183|https://openalex.org/W3034999214|https://openalex.org/W2996428491|https://openalex.org/W2923014074|https://openalex.org/W2936695845|https://openalex.org/W3011411500|https://openalex.org/W3105966348|https://openalex.org/W2953356739
https://openalex.org/W86887328,Large-Scale Named Entity Disambiguation Based on Wikipedia Data,"Large-Scale Named Entity Disambiguation Based on Wikipedia Data

This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from large encyclopedic collection Web search results. It describes in detail paradigm employed extraction process Wikipedia. Through maximizing agreement between contextual Wikipedia context document, as well among category tags associated with candidate entities, implemented shows high accuracy both news stories articles.

computer science, knowledge graph, machine learning, information fusion, cognitive science, data science, content similarity detection, large-scale datasets, wikipedia data, entity summarization, machine learning research, text mining, natural language processing, web search, knowledge discovery, named-entity recognition, entity disambiguation, semantic web, information retrieval",2007,1075,computer science|knowledge graph|machine learning|information fusion|cognitive science|data science|content similarity detection|large-scale datasets|wikipedia data|entity summarization|machine learning research|text mining|natural language processing|web search|knowledge discovery|named-entity recognition|entity disambiguation|semantic web|information retrieval,
https://openalex.org/W3105966348,TinyBERT: Distilling BERT for Natural Language Understanding,"TinyBERT: Distilling BERT for Natural Language Understanding

Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce size while maintaining accuracy, we first propose a novel Transformer distillation method that specially designed for knowledge (KD) Transformer-based models. By leveraging this new KD method, plenty encoded in large “teacher” BERT can be effectively transferred small “student” TinyBERT. Then, introduce two-stage learning framework TinyBERT, which performs at both pre-training task-specific stages. This ensures TinyBERT capture general-domain well BERT. TinyBERT4 with 4 layers empirically effective achieves more than 96.8% performance its teacher BERT-Base GLUE benchmark, being 7.5x smaller 9.4x faster inference. also better 4-layer state-of-the-art baselines distillation, only ~28% parameters ~31% time them. Moreover, TinyBERT6 6 on-par BERT-Base.

distilling bert, computer science, natural language interface, language model, natural language processing, machine learning, computational linguistics, natural language understanding, deep learning, knowledge distillation, natural language, machine translation, nlp task, automatic annotation tool",2020,1072,distilling bert|computer science|natural language interface|language model|natural language processing|machine learning|computational linguistics|natural language understanding|deep learning|knowledge distillation|natural language|machine translation|nlp task|automatic annotation tool,https://openalex.org/W3030163527
https://openalex.org/W2151162785,The CNN paradigm,"The CNN paradigm

A concise tutorial description of the cellular neural network (CNN) paradigm is given, along with a precise taxonomy. The CNN defined, and canonical equations are described. importance many independent input signal arrays, adaptive templates, multilayer capability emphasized motivated by examples. It shown how simply wave-type partial differential equation can be generated.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

deepfakes, sparse neural network, neural network (machine learning), computer science, machine learning research, deep learning, journalism, pattern recognition, machine vision, natural language processing, multimedia information processing, image representation, large-scale datasets, machine learning, generative adversarial network, cnn paradigm, cognitive science, convolutional neural network, image communication",1993,1072,deepfakes|sparse neural network|neural network (machine learning)|computer science|machine learning research|deep learning|journalism|pattern recognition|machine vision|natural language processing|multimedia information processing|image representation|large-scale datasets|machine learning|generative adversarial network|cnn paradigm|cognitive science|convolutional neural network|image communication,
https://openalex.org/W2740168486,A Survey on Hate Speech Detection using Natural Language Processing,"A Survey on Hate Speech Detection using Natural Language Processing

This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, amount online is also increasing. Due to massive scale web, methods that automatically detect are required. Our describes key areas have been explored recognize these types utterances using natural language processing. We discuss limits those approaches.

computer science, linguistics, speech recognition, disinformation detection, natural language processing, speech processing, speech analysis, hate speech detection, communication, computational linguistics, hate speech, misbehaviour detection, bias detection",2017,1072,computer science|linguistics|speech recognition|disinformation detection|natural language processing|speech processing|speech analysis|hate speech detection|communication|computational linguistics|hate speech|misbehaviour detection|bias detection,
https://openalex.org/W2953356739,ERNIE: Enhanced Language Representation with Informative Entities,"ERNIE: Enhanced Language Representation with Informative Entities

Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, existing rarely consider incorporating knowledge graphs (KGs), which provide structured facts for better understanding. We argue that informative entities in KGs enhance with external knowledge. In this paper, we utilize both textual train an enhanced model (ERNIE), take full advantage lexical, syntactic, information simultaneously. The experimental results have demonstrated ERNIE achieves significant improvements knowledge-driven tasks, meanwhile is comparable state-of-the-art other common code datasets will available future.

computer science, knowledge discovery, semantic representation, enhanced language representation, information fusion, cross-lingual representation, nlp task, semantic evaluation, text mining, entity summarization, informative entities, language resource, semantics, computational linguistics, linguistics, language engineering, language, natural language processing, language model, semantic web",2019,1071,computer science|knowledge discovery|semantic representation|enhanced language representation|information fusion|cross-lingual representation|nlp task|semantic evaluation|text mining|entity summarization|informative entities|language resource|semantics|computational linguistics|linguistics|language engineering|language|natural language processing|language model|semantic web,https://openalex.org/W3011411500
https://openalex.org/W2015143272,An audio-visual corpus for speech perception and automatic speech recognition,"An audio-visual corpus for speech perception and automatic speech recognition

An audio-visual corpus has been collected to support the use of common material in speech perception and automatic recognition studies. The consists high-quality audio video recordings 1000 sentences spoken by each 34 talkers. Sentences are simple, syntactically identical phrases such as ""place green at B 4 now"". Intelligibility tests using signals suggest that is easily identifiable quiet low levels stationary noise. annotated available on web for research use.

computer science, speech recognition, speech interface, automatic speech recognition, natural language processing, speech processing, digital signal processing, speech perception, machine learning, multimodal signal processing, audio-visual corpus, speech science, speech communication, emotion recognition, spoken language technology, hearing research, speech corpus",2006,1067,computer science|speech recognition|speech interface|automatic speech recognition|natural language processing|speech processing|digital signal processing|speech perception|machine learning|multimodal signal processing|audio-visual corpus|speech science|speech communication|emotion recognition|spoken language technology|hearing research|speech corpus,
https://openalex.org/W2170738476,Convolutional Neural Network Architectures for Matching Natural Language Sentences,"Convolutional Neural Network Architectures for Matching Natural Language Sentences

Semantic matching is of central importance to many natural language tasks \cite{bordes2014semantic,RetrievalQA}. A successful algorithm needs adequately model the internal structures objects and interaction between them. As a step toward this goal, we propose convolutional neural network models for two sentences, by adapting strategy in vision speech. The proposed not only nicely represent hierarchical sentences with their layer-by-layer composition pooling, but also capture rich patterns at different levels. Our are rather generic, requiring no prior knowledge on language, can hence be applied nature languages. empirical study variety demonstrates efficacy its superiority competitor models.

knowledge discovery, neural architecture search, nlp task, natural language processing, machine translation, natural language sentences, semantic interpretation, natural language generation, computational intelligence, computer science, neural machine translation, language model, machine learning, large language model, language, deep learning, data science, computational linguistics",2015,1065,knowledge discovery|neural architecture search|nlp task|natural language processing|machine translation|natural language sentences|semantic interpretation|natural language generation|computational intelligence|computer science|neural machine translation|language model|machine learning|large language model|language|deep learning|data science|computational linguistics,https://openalex.org/W2884001105|https://openalex.org/W2413794162|https://openalex.org/W3019166713
https://openalex.org/W2963506925,Six Challenges for Neural Machine Translation,"Six Challenges for Neural Machine Translation

We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. show both deficiencies improvements over the quality phrase-based statistical translation.

language learning, natural language processing, machine translation, computer science, neural machine translation, language model, linguistics, language, deep learning, computational linguistics",2017,1055,language learning|natural language processing|machine translation|computer science|neural machine translation|language model|linguistics|language|deep learning|computational linguistics,https://openalex.org/W3019166713|https://openalex.org/W2938704169
https://openalex.org/W2133148365,The role of strong syllables in segmentation for lexical access.,"The role of strong syllables in segmentation for lexical access.

initiate a very large number of access attempts, by far the majority which are futile.' Phonetic transcriptions two second syllables each set.

linguistics, natural language processing, lexical access, text segmentation, strong syllables",1988,1054,linguistics|natural language processing|lexical access|text segmentation|strong syllables,
https://openalex.org/W2131785201,Semantics and complexity of SPARQL,"Semantics and complexity of SPARQL

SPARQL is the standard language for querying RDF data. In this article, we address systematically formal study of database aspects SPARQL, concentrating in its graph pattern matching facility. We provide a compositional semantics core part and complexity evaluation several fragments language. Among other results, show that general patterns PSPACE-complete. identify large class patterns, defined by imposing simple natural syntactic restriction, where query problem can be solved more efficiently. This restriction gives rise to well-designed patterns. coNP-complete Moreover, rewriting rules whose application may have considerable impact cost evaluating queries.

computer science, knowledge representation and reasoning, web semantics, natural language processing, semantic evaluation, semantic interoperability, automated reasoning, semantics, semantic web, logic in computer science",2009,1053,computer science|knowledge representation and reasoning|web semantics|natural language processing|semantic evaluation|semantic interoperability|automated reasoning|semantics|semantic web|logic in computer science,
https://openalex.org/W3185341429,"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing","Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

This article surveys and organizes research works in a new paradigm natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, trains model to take an input x predict output y as P ( y|x ), prompt-based learning is based on models that the probability of text directly. To use these perform prediction tasks, original modified using template into textual string prompt x′ has some unfilled slots, then used probabilistically fill information obtain final x̂ , from can be derived. framework powerful attractive for number reasons: It allows pre-trained massive amounts raw text, by defining prompting function able few-shot or even zero-shot adapting scenarios with few no labeled data. In this article, introduce basics promising paradigm, describe unified set mathematical notations cover wide variety existing work, organize work along several dimensions, e.g., choice models, prompts, tuning strategies. make field more accessible interested beginners, not only systematic review highly structured typology concepts but also release other resources, website NLPedia–Pretrain including constantly updated survey paperlist.

computer science, natural language processing, language model, predictive learning, machine learning, nlp task, machine learning research",2023,1053,computer science|natural language processing|language model|predictive learning|machine learning|nlp task|machine learning research,
https://openalex.org/W2038702914,SMILES. 2. Algorithm for generation of unique SMILES notation,"SMILES. 2. Algorithm for generation of unique SMILES notation

ADVERTISEMENT RETURN TO ISSUEPREVArticleNEXTSMILES. 2. Algorithm for generation of unique SMILES notationDavid Weininger, Arthur and Joseph L. WeiningerCite this: J. Chem. Inf. Comput. Sci. 1989, 29, 2, 97–101Publication Date (Print):May 1, 1989Publication History Published online1 May 2002Published inissue 1 1989https://pubs.acs.org/doi/10.1021/ci00062a008https://doi.org/10.1021/ci00062a008research-articleACS PublicationsRequest reuse permissionsArticle Views4516Altmetric-Citations881LEARN ABOUT THESE METRICSArticle Views are the COUNTER-compliant sum full text article downloads since November 2008 (both PDF HTML) across all institutions individuals. These metrics regularly updated to reflect usage leading up last few days.Citations number other articles citing this article, calculated by Crossref daily. Find more information about citation counts.The Altmetric Attention Score is a quantitative measure attention that research has received online. Clicking on donut icon will load page at altmetric.com with additional details score social media presence given article. how calculated. Share Add toView InAdd Full Text ReferenceAdd Description ExportRISCitationCitation abstractCitation referencesMore Options onFacebookTwitterWechatLinked InRedditEmail Other access options Get e-Alerts

pattern recognition, computer science, linguistics, soft computing, machine learning, computational linguistics, communication, image representation, combinatorial optimization, image communication, computational creativity, unique smiles notation, facial expression recognition, creative computing, natural language processing, affective computing, biometrics, facial recognition system, combinatorial pattern matching",1989,1051,pattern recognition|computer science|linguistics|soft computing|machine learning|computational linguistics|communication|image representation|combinatorial optimization|image communication|computational creativity|unique smiles notation|facial expression recognition|creative computing|natural language processing|affective computing|biometrics|facial recognition system|combinatorial pattern matching,
https://openalex.org/W2251135946,Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,"Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks

Two problems arise when using distant supervision for relation extraction.First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data.However, heuristic can fail, resulting wrong label problem.In addition, previous approaches, statistical models have typically been applied ad hoc features.The noise that originates from feature extraction process cause poor performance.In paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning address these two problems.To solve first problem, supervised problem which uncertainty of instance labels taken into account.To latter avoid engineering instead adopt convolutional architecture piecewise max pooling automatically learn relevant features.Experiments show our method effective outperforms several competitive baseline methods.

computer science, machine learning, knowledge representation and reasoning, information fusion, natural language generation, cognitive science, data science, computational intelligence, deep learning, machine learning research, statistical relational learning, relationship extraction, relation extraction, text mining, natural language processing, large language model, knowledge discovery, knowledge extraction, causal relation extraction, distant supervision",2015,1050,computer science|machine learning|knowledge representation and reasoning|information fusion|natural language generation|cognitive science|data science|computational intelligence|deep learning|machine learning research|statistical relational learning|relationship extraction|relation extraction|text mining|natural language processing|large language model|knowledge discovery|knowledge extraction|causal relation extraction|distant supervision,https://openalex.org/W2953356739|https://openalex.org/W2515462165
https://openalex.org/W2334889010,Probabilistic Topic Models,"Probabilistic Topic Models

Many chapters in this book illustrate that applying a statistical method
such as latent semantic analysis (LSA; Landauer & Dumais, 1997;
Landauer, Foltz, Laham, 1998) to large databases can yield insight into
human cognition. The LSA approach makes three claims: information be derived from word-document co-occurrence matrix;
that dimensionality reduction is an essential part of derivation; and
that words and documents represented points Euclidean space.
This chapter pursues consistent with the first two of
these claims, but differs third, describing class models
in which properties are expressed in
terms probabilistic topics.

probabilistic topic models, disinformation detection, topic model, text mining, natural language processing, probabilistic system, machine learning, probability theory, statistics, machine learning research, probabilistic reasoning",2015,1048,probabilistic topic models|disinformation detection|topic model|text mining|natural language processing|probabilistic system|machine learning|probability theory|statistics|machine learning research|probabilistic reasoning,https://openalex.org/W2108646579|https://openalex.org/W168564468|https://openalex.org/W2113459411|https://openalex.org/W658020064|https://openalex.org/W2098062695|https://openalex.org/W2108420397
https://openalex.org/W2119854905,Presupposition Projection as Anaphora Resolution,"Presupposition Projection as Anaphora Resolution

The present paper presents an anaphoric account of presupposition. It is argued that presuppositional expressions should not be seen as referring expressions, nor presupposition to explicated in terms some non-standard logic. notion relegated a pragmatic theory either. Instead are claimed which have internal structure and semantic content. In fact they only differ from pronouns other semantically less loaded anaphors more descriptive this enables them create antecedent case discourse does provide one. If their capacity accommodate taken into can treated by basically the same mechanism handles resolution pronouns. elaborated framework representation theory. shown factors interfere anaphors. resulting neither classified wholly pragmatic. Section 1 survey standing problems projection discusses major competing approaches. An argumentation for purely given section 2. 3 coding extension final devoted discussion constraints govern

computer science, anaphora resolution, natural language processing, presupposition projection, presupposition",1992,1047,computer science|anaphora resolution|natural language processing|presupposition projection|presupposition,
https://openalex.org/W2964167098,End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,"End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures

We present a novel end-to-end neural model to extract entities and relations between them.Our recurrent network based captures both word sequence dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on sequential LSTM-RNNs.This allows our jointly represent with shared parameters in single model.We further encourage detection of during training use entity relation extraction via pretraining scheduled sampling.Our improves over the stateof-the-art feature-based end-toend extraction, achieving 12.1% 5.7% relative error reductions F1score ACE2005 ACE2004, respectively.We also show that LSTM-RNN compares favorably state-of-the-art CNN (in F1-score) nominal classification (SemEval-2010 Task 8).Finally, we an extensive ablation analysis several components.

relationship extraction, knowledge discovery, query expansion, tree structures, computer science, language model, machine learning research, text mining, deep learning, sequence modelling, information fusion, natural language processing, fuzzy logic, natural language generation, machine learning, linguistics, data science, keyword extraction, end-to-end relation extraction, information extraction",2016,1041,relationship extraction|knowledge discovery|query expansion|tree structures|computer science|language model|machine learning research|text mining|deep learning|sequence modelling|information fusion|natural language processing|fuzzy logic|natural language generation|machine learning|linguistics|data science|keyword extraction|end-to-end relation extraction|information extraction,
https://openalex.org/W2963691697,AllenNLP: A Deep Semantic Natural Language Processing Platform,"AllenNLP: A Deep Semantic Natural Language Processing Platform

Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer. Proceedings of Workshop for NLP Open Source Software (NLP-OSS). 2018.

knowledge representation and reasoning, machine learning, semantic representation, language, nlp task, semantic evaluation, semantic processing, natural language processing, semantic learning, computational linguistics, deep learning, semantic parsing, semantic web",2018,1040,knowledge representation and reasoning|machine learning|semantic representation|language|nlp task|semantic evaluation|semantic processing|natural language processing|semantic learning|computational linguistics|deep learning|semantic parsing|semantic web,https://openalex.org/W2979826702|https://openalex.org/W2970771982|https://openalex.org/W3034238904|https://openalex.org/W2968124245
https://openalex.org/W3098605233,CodeBERT: A Pre-Trained Model for Programming and Natural Languages,"CodeBERT: A Pre-Trained Model for Programming and Natural Languages

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Zhou. Findings of the Association for Computational Linguistics: EMNLP 2020.

programming language, natural language processing, pre-trained model, computer science, language model, machine learning, language engineering",2020,1039,programming language|natural language processing|pre-trained model|computer science|language model|machine learning|language engineering,
https://openalex.org/W2938704169,The Curious Case of Neural Text Degeneration,"The Curious Case of Neural Text Degeneration

Despite considerable advancements with deep neural language models, the enigma of text degeneration persists when these models are tested as generators. The counter-intuitive empirical observation is that even though use likelihood training objective leads to high quality for a broad range understanding tasks, using decoding bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human machine text. addition, find strategies alone can dramatically effect text, generated from exactly same model. Our findings motivate Nucleus Sampling, simple but effective method draw best out generation. By sampling dynamic nucleus probability distribution, which allows diversity while effectively truncating less reliable tail resulting better demonstrates yielding enhanced without sacrificing fluency coherence.

neural text degeneration, computer science, natural language processing, neuroscience, machine learning, cognitive science, neural computation, language processing in the brain, text processing",2019,1033,neural text degeneration|computer science|natural language processing|neuroscience|machine learning|cognitive science|neural computation|language processing in the brain|text processing,https://openalex.org/W3030163527
https://openalex.org/W2880875857,Contextual String Embeddings for Sequence Labeling,"Contextual String Embeddings for Sequence Labeling

Recent advances in language modeling using recurrent neural networks have made it viable to model as distributions over characters. By learning predict the next character on basis of previous characters, such models been shown automatically internalize linguistic concepts words, sentences, subclauses and even sentiment. In this paper, we propose leverage internal states a trained produce novel type word embedding which refer contextual string embeddings. Our proposed embeddings distinct properties that they (a) are without any explicit notion words thus fundamentally sequences (b) contextualized by their surrounding text, meaning same will different depending its use. We conduct comparative evaluation against find our highly useful for downstream tasks: across four classic sequence labeling tasks consistently outperform state-of-the-art. particular, significantly work English German named entity recognition (NER), allowing us report new state-of-the-art F1-scores CoNLL03 shared task. release all code pre-trained simple-to-use framework research community, enable reproduction these experiments application other https://github.com/zalandoresearch/flair

sequence labeling, computer science, contextual string embeddings, natural language processing",2018,1029,sequence labeling|computer science|contextual string embeddings|natural language processing,https://openalex.org/W2896457183|https://openalex.org/W3035390927|https://openalex.org/W3019166713|https://openalex.org/W3037109418
https://openalex.org/W3037109418,Stanza: A Python Natural Language Processing Toolkit for Many Human Languages,"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages

We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, named entity recognition. have trained on total of 112 datasets, the Universal Dependencies treebanks other multilingual corpora, show that same architecture generalizes well achieves competitive performance all languages tested. Additionally, includes native interface Java Stanford CoreNLP software, which further extends its functionality cover tasks such as coreference resolution relation extraction. Source code, documentation, pretrained models are available at https://stanfordnlp.github.io/stanza/.

many human languages, machine translation, language, nlp task, natural language processing, computational linguistics",2020,1027,many human languages|machine translation|language|nlp task|natural language processing|computational linguistics,
https://openalex.org/W1998042868,End-to-end scene text recognition,"End-to-end scene text recognition

This paper focuses on the problem of word detection and recognition in natural images. The is significantly more challenging than reading text scanned documents, has only recently gained attention from computer vision community. Sub-components problem, such as cropped image recognition, have been studied isolation [7, 4, 20]. However, what unclear how these recent approaches contribute to solving end-to-end recognition. We fill this gap by constructing evaluating two systems. first, representing de facto state-of-the-art, a stage pipeline consisting followed leading OCR engine. second system rooted generic object an extension our previous work [20]. show that latter approach achieves superior performance. While scene generally treated with highly domain-specific methods, results demonstrate suitability applying methods. Adopting opens door for real world benefit rapid advances taking place

computational imaging, pattern recognition, computer science, information fusion, language model, natural language processing, character recognition, scene understanding, text recognition, machine learning, text segmentation, data science, knowledge discovery, deep learning, image representation, text processing, machine vision, computer engineering",2011,1026,computational imaging|pattern recognition|computer science|information fusion|language model|natural language processing|character recognition|scene understanding|text recognition|machine learning|text segmentation|data science|knowledge discovery|deep learning|image representation|text processing|machine vision|computer engineering,https://openalex.org/W2144554289|https://openalex.org/W2343052201
https://openalex.org/W2102098892,The minimum description length principle in coding and modeling,"The minimum description length principle in coding and modeling

We review the principles of minimum description length and stochastic complexity as used in data compression statistical modeling. Stochastic is formulated solution to optimum universal coding problems extending Shannon's basic source theorem. The normalized maximized likelihood, mixture, predictive codings are each shown achieve within asymptotically vanishing terms. assess performance criterion both from vantage point quality accuracy inference. Context tree modeling, density estimation, model selection Gaussian linear regression serve examples.

computer science, algebraic coding theory, natural language processing, program analysis, description logic, data modeling, code representation",1998,1024,computer science|algebraic coding theory|natural language processing|program analysis|description logic|data modeling|code representation,
https://openalex.org/W4205807230,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing

EMNLP 2021 is one of the first hybrid conferences in field natural language processing.It also for us, organizing team, uncharted domain.Organizing a conference has felt like two conferences, virtual and an in-person one, which seamlessly must work together with kind multi-task objective make experience synergistic successful both remotely person.With this challenge come opportunities.The format allows remote participation that held onsite Punta Cana, The Dominican Republic, creating real feeling those who do not want to travel many miles from other side world increase their carbon footprint, have budget restrictions traveling.We welcome you all!As previous years, purpose General Chair's preface express thanks amazing team chairs whose heroic efforts made possible.The includes:• Programme Chairs -Xuanjing Huang, Lucia Specia Scott Yih -who did tremendous job manage reviewing process set up outstanding scientific program.• Senior Area Chairs, Reviewers expertise enabled authors learn reviews deliver papers improved on original submissions.• Demonstration -Heike Adel Shuming Shi selected demonstrations

empirical methods, natural language processing, language, machine learning, nlp task",2021,1020,empirical methods|natural language processing|language|machine learning|nlp task,https://openalex.org/W2436001372
https://openalex.org/W2159457224,Opinion Word Expansion and Target Extraction through Double Propagation,"Opinion Word Expansion and Target Extraction through Double Propagation

Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, lexicon expansion target extraction. Opinion targets (targets, for short) are entities their attributes on which opinions have been expressed. To perform the tasks, found that there several syntactic relations link words targets. These can be identified using dependency parser then utilized expand initial extract This proposed method is based bootstrapping. We call it double propagation propagates information between A key advantage only needs an start bootstrapping process. Thus, semi-supervised use word seeds. evaluation, compare with state-of-the-art methods standard product review test collection. The results show our approach outperforms these existing significantly.

linguistics, target extraction, text mining, natural language processing, information extraction, opinion word expansion, double propagation",2011,1017,linguistics|target extraction|text mining|natural language processing|information extraction|opinion word expansion|double propagation,https://openalex.org/W2108646579|https://openalex.org/W2031998113|https://openalex.org/W2612769033
https://openalex.org/W3169483174,mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer,"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer

Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Barua, Colin Raffel. Proceedings of the 2021 Conference North American Chapter Association for Computational Linguistics: Human Language Technologies. 2021.

computer science, text mining, natural language processing, language model, multilingual pretraining, deep learning, language learning, multilingualism",2021,1017,computer science|text mining|natural language processing|language model|multilingual pretraining|deep learning|language learning|multilingualism,
https://openalex.org/W2252215182,Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts,"Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts

Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because the limited contextual information that they normally contain. Effectively solving this task requires strategies combine small text content with prior knowledge use more than just bag-of-words. In work we propose a new deep convolutional neural network exploits from characterto sentence-level to perform sentiment texts. We apply our approach for two corpora different domains: Stanford Treebank (SSTb), which contains movie reviews; corpus (STS), messages. For SSTb corpus, achieves state-of-the-art results sentence prediction in both binary positive/negative classification, 85.7% accuracy, fine-grained 48.3% accuracy. STS accuracy 86.4%.

short texts, computer science, convolutional neural network, text mining, natural language processing, machine learning, computational linguistics, data science, deep learning, sentiment analysis",2014,1013,short texts|computer science|convolutional neural network|text mining|natural language processing|machine learning|computational linguistics|data science|deep learning|sentiment analysis,https://openalex.org/W2493916176|https://openalex.org/W2963012544|https://openalex.org/W2884001105|https://openalex.org/W2964236337|https://openalex.org/W2742947407|https://openalex.org/W3019166713|https://openalex.org/W2515462165|https://openalex.org/W2963042536
https://openalex.org/W2016534914,Computing the Meanings of Words in Reading: Cooperative Division of Labor Between Visual and Phonological Processes.,"Computing the Meanings of Words in Reading: Cooperative Division of Labor Between Visual and Phonological Processes.

Are words read visually (by means of a direct mapping from orthography to semantics) or phonologically phonology semantics)? The authors addressed this long-standing debate by examining how large-scale computational model based on connectionist principles would solve the problem and comparing model's performance people's. In contrast previous models, present uses an architecture in which meanings are jointly determined 2 components, with division labor between them affected nature mappings codes. is consistent variety behavioral phenomena, including results studies homophones pseudohomophones thought support other theories, illustrates efficient processing can be achieved using multiple simultaneous constraints.

phonological processes, linguistics, natural language processing, speech processing, language, language comprehension, reading research, cognitive science, language processing in the brain, cooperative division, language learning, phonology",2004,1009,phonological processes|linguistics|natural language processing|speech processing|language|language comprehension|reading research|cognitive science|language processing in the brain|cooperative division|language learning|phonology,
https://openalex.org/W1488252886,Learning Structural Descriptions From Examples,"Learning Structural Descriptions From Examples

Massachusetts Institute of Technology. Dept. Electrical Engineering. Thesis. 1970. Ph.D.

computer science, natural language processing, machine learning, data science, deep learning, description logic, structural descriptions",1970,1008,computer science|natural language processing|machine learning|data science|deep learning|description logic|structural descriptions,
https://openalex.org/W2515462165,Neural Relation Extraction with Selective Attention over Instances,"Neural Relation Extraction with Selective Attention over Instances

Distant supervised relation extraction has been widely used to find novel relational facts from text.However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt performance of extraction.To alleviate this issue, we propose a sentence-level attention-based model for extraction.In model, employ convolutional neural networks embed semantics sentences.Afterwards, build attention over multiple instances, which is expected dynamically reduce weights those instances.Experimental results on real-world datasets show that, our can make full use all informative sentences effectively influence labelled instances.Our achieves significant consistent improvements as compared baselines.

natural language processing, machine learning, imitative learning, cognitive science, neural computation, deep learning, selective attention, neural relation extraction, attention, relationship extraction",2016,1003,natural language processing|machine learning|imitative learning|cognitive science|neural computation|deep learning|selective attention|neural relation extraction|attention|relationship extraction,https://openalex.org/W2953356739
https://openalex.org/W2962826786,End-to-end attention-based large vocabulary speech recognition,"End-to-end attention-based large vocabulary speech recognition

Many state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) Systems are hybrids of neural networks and Hidden Markov Models (HMMs). Recently, more direct end-to-end methods have been investigated, in which architectures were trained to model sequences characters [1,2]. To our knowledge, all these approaches relied on Connectionist Temporal Classification [3] modules. We investigate an alternative method for sequence modelling based attention mechanism that allows a Recurrent Neural Network (RNN) learn alignments between input frames output labels. show how this setup can be applied LVCSR by integrating the decoding RNN with n-gram language speeding up its operation constraining selections made reducing source lengths pooling information over time. accuracies similar other HMM-free RNN-based reported Wall Street Journal corpus.

computer science, speech recognition, large language model, natural language processing, language model, spoken language technology, attention",2016,1003,computer science|speech recognition|large language model|natural language processing|language model|spoken language technology|attention,https://openalex.org/W2327501763|https://openalex.org/W2963929190
https://openalex.org/W2752172973,A Simple but Tough-to-Beat Baseline for Sentence Embeddings,"A Simple but Tough-to-Beat Baseline for Sentence Embeddings

The success of neural network methods for computing word embeddings has motivated generating semantic longer pieces text, such as sentences and paragraphs. Surprisingly, Wieting et al (ICLR'16) showed that complicated are outperformed, especially in out-of-domain (transfer learning) settings, by simpler involving mild retraining basic linear regression. method al. requires with a substantial labeled dataset Paraphrase Database (Ganitkevitch al., 2013). 

The current paper goes further, showing the following completely unsupervised sentence embedding is formidable baseline: Use computed using one popular on unlabeled corpus like Wikipedia, represent weighted average vectors, then modify them bit PCA/SVD. This weighting improves performance about 10% to 30% textual similarity tasks, beats sophisticated supervised including RNN's LSTM's. It even al.'s embeddings. simple should be used baseline beat future, when training data scarce or nonexistent. also gives theoretical explanation above latent variable generative model sentences, which extension Arora (TACL'16) new smoothing terms allow 
words occurring out context, well high probabilities words and, not all contexts.

computer science, natural language processing, sentence embeddings, tough-to-beat baseline, baseline correction",2017,1001,computer science|natural language processing|sentence embeddings|tough-to-beat baseline|baseline correction,https://openalex.org/W2963918774
https://openalex.org/W2127314673,Distributional clustering of English words,"Distributional clustering of English words

We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts which they appear, entropy between those is used as similarity measure clustering. Clusters average context derived from given probabilities cluster membership. In many cases, clusters can be thought encoding coarse sense distinctions. Deterministic annealing find lowest distortion sets clusters: parameter increases, existing become unstable subdivide, yielding hierarchical ""soft"" data. basis class models word coocurrence, evaluated with respect held-out test

linguistics, english, text mining, natural language processing, language, english words, clustering, distributional clustering, document clustering, english language",1993,997,linguistics|english|text mining|natural language processing|language|english words|clustering|distributional clustering|document clustering|english language,https://openalex.org/W1659833910|https://openalex.org/W2158139315|https://openalex.org/W2158108973|https://openalex.org/W2166776180|https://openalex.org/W2199803028|https://openalex.org/W2130337399
https://openalex.org/W2088622183,Learning extraction patterns for subjective expressions,"Learning extraction patterns for subjective expressions

This paper presents a bootstrapping process that learns linguistically rich extraction patterns for subjective (opinionated) expressions. High-precision classifiers label unannotated data to automatically create large training set, which is then given an pattern learning algorithm. The learned are used identify more sentences. many and increases recall while maintaining high precision.

natural language processing, information extraction, machine learning, computational linguistics, data science, knowledge discovery, subjective expressions, extraction patterns",2003,996,natural language processing|information extraction|machine learning|computational linguistics|data science|knowledge discovery|subjective expressions|extraction patterns,https://openalex.org/W2108646579|https://openalex.org/W2022204871|https://openalex.org/W4211186029|https://openalex.org/W2084046180|https://openalex.org/W2126581182|https://openalex.org/W1964613733|https://openalex.org/W2031998113|https://openalex.org/W4239946314
https://openalex.org/W2164455818,A Machine Learning Approach to Coreference Resolution of Noun Phrases,"A Machine Learning Approach to Coreference Resolution of Noun Phrases

In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The learns from small, annotated corpus and the task includes resolving not just certain type phrase (e.g., pronouns) but rather general phrases. It also does restrict entity types phrases; that is, is assigned whether they are “organization,” “person,” or other types. We evaluate our on common data sets (namely, MUC-6 MUC-7 corpora) obtain encouraging results, indicating task, holds promise achieves accuracy comparable nonlearning approaches. Our system first learning-based offers performance state-of-the-art systems these sets.

noun phrases, computer science, natural language processing, machine learning, coreference resolution",2001,995,noun phrases|computer science|natural language processing|machine learning|coreference resolution,
https://openalex.org/W2970608575,ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks,"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

We present ViLBERT (short for Vision-and-Language BERT), a model learning task-agnostic joint representations of image content and natural language. extend the popular BERT architecture to multi-modal two-stream model, processing both visual textual inputs in separate streams that interact through co-attentional transformer layers. pretrain our two proxy tasks on large, automatically collected Conceptual Captions dataset then transfer it multiple established vision-and-language -- question answering, commonsense reasoning, referring expressions, caption-based retrieval by making only minor additions base architecture. observe significant improvements across compared existing task-specific models achieving state-of-the-art all four tasks. Our work represents shift away from groundings between vision language as part task training towards treating grounding pretrainable transferable capability.

language comprehension, image analysis, computer science, language learning, human-computer interaction, multilingual pretraining, task analysis, language, nlp task, vision language model, natural language processing, image representation, deep learning, machine vision, vision-and-language tasks, language processing in the brain, cognitive science, language model",2019,993,language comprehension|image analysis|computer science|language learning|human-computer interaction|multilingual pretraining|task analysis|language|nlp task|vision language model|natural language processing|image representation|deep learning|machine vision|vision-and-language tasks|language processing in the brain|cognitive science|language model,https://openalex.org/W3094502228|https://openalex.org/W3098605233|https://openalex.org/W2968124245
https://openalex.org/W2964010806,Tensor Fusion Network for Multimodal Sentiment Analysis,"Tensor Fusion Network for Multimodal Sentiment Analysis

Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of to a multimodal setup where other relevant modalities accompany language. In this paper, we pose problem as modeling intra-modality and inter-modality dynamics. We introduce novel model, termed Tensor Fusion Networks, learns both such dynamics end-to-end. The proposed approach tailored for volatile nature spoken language in online videos well accompanying gestures voice. experiments, our model outperforms state-of-the-art approaches unimodal analysis.

communication, image analysis, computer science, machine learning, multimodal signal processing, information fusion, multimodal sentiment analysis, machine learning research, feature fusion, natural language processing, affective computing, multimodal sensor fusion, data science, deep learning, tensor fusion network, multimodal learning",2017,993,communication|image analysis|computer science|machine learning|multimodal signal processing|information fusion|multimodal sentiment analysis|machine learning research|feature fusion|natural language processing|affective computing|multimodal sensor fusion|data science|deep learning|tensor fusion network|multimodal learning,https://openalex.org/W2884001105|https://openalex.org/W2964236337
https://openalex.org/W2124819629,Collostructions: Investigating the interaction of words and constructions,"Collostructions: Investigating the interaction of words and constructions

This paper introduces an extension of collocational analysis that takes into account grammatical structure and is specifically geared to investigating the interaction lexemes constructions associated with them. The method framed in a construction-based approach language, i.e. it assumes grammar consists signs (form-meaning pairs) thus not fundamentally different from lexicon. applied linguistic expressions at various levels abstraction (words, semi-fixed phrases, argument structures, tense, aspect mood). has two main applications: first, increase adequacy description by providing objective way identifying meaning construction determining degree which particular slots prefer or are restricted set lexemes; second, provide data for theory-building.

applied linguistics, morphology (linguistics), linguistics, syntactic structure, natural language processing, language, syntax, communication, interactional linguistics, discourse analysis, semantics, narrative, language science, context (linguistics)",2003,993,applied linguistics|morphology (linguistics)|linguistics|syntactic structure|natural language processing|language|syntax|communication|interactional linguistics|discourse analysis|semantics|narrative|language science|context (linguistics),
https://openalex.org/W1987425720,Sentiment strength detection for the social web,"Sentiment strength detection for the social web

Abstract Sentiment analysis is concerned with the automatic extraction of sentiment‐related information from text. Although most sentiment addresses commercial tasks, such as extracting opinions product reviews, there increasing interest in affective dimension social web, and Twitter particular. Most algorithms are not ideally suited to this task because they exploit indirect indicators that can reflect genre or topic instead. Hence, used process web texts identify spurious patterns caused by topics rather than phenomena. This article assesses an improved version algorithm SentiStrength for strength detection across primarily uses direct indications sentiment. The results six diverse data sets (MySpace, Twitter, YouTube, Digg, Runners World, BBC Forums) indicate 2 successful sense performing better a baseline approach all both supervised unsupervised cases. always machine‐learning approaches sentiment, however, particularly weaker positive news‐related discussions. Overall, suggest that, even unsupervised, robust enough be applied wide variety different contexts.

computer science, social medium mining, sentiment strength detection, deception detection, content analysis, disinformation detection, social web, social computing, communication, semantic evaluation, data science, online information, keyword extraction, world wide web, text mining, natural language processing, affective computing, knowledge discovery, social media",2011,993,computer science|social medium mining|sentiment strength detection|deception detection|content analysis|disinformation detection|social web|social computing|communication|semantic evaluation|data science|online information|keyword extraction|world wide web|text mining|natural language processing|affective computing|knowledge discovery|social media,https://openalex.org/W2084046180|https://openalex.org/W2250879510
https://openalex.org/W1985697096,A Statistical Approach to Mechanized Encoding and Searching of Literary Information,"A Statistical Approach to Mechanized Encoding and Searching of Literary Information

Written communication of ideas is carried out on the basis statistical probability in that a writer chooses level subject specificity and combination words which he feels will convey most meaning. Since this process varies among individuals since similar are therefore relayed at different levels by means words, problem literature searching machines still presents major difficulties. A approach to be outlined various steps system based described. Steps include analysis collection documents field interest, establishment set “notions” vocabulary they expressed, compilation thesaurus-type dictionary index, automatic encoding machine with aid such dictionary, topological notations (such as branched structures), recording coded information, pattern for finding pertinent programming appropriate carry search.

narrative extraction, computer science, linguistics, text mining, natural language processing, literary information, information extraction, machine learning, computational linguistics, data science, knowledge discovery, similarity search, machine learning research, search technique, document analysis, information retrieval",1957,992,narrative extraction|computer science|linguistics|text mining|natural language processing|literary information|information extraction|machine learning|computational linguistics|data science|knowledge discovery|similarity search|machine learning research|search technique|document analysis|information retrieval,
https://openalex.org/W2787893582,Predicting the semantic orientation of adjectives,"Predicting the semantic orientation of adjectives

We identify and validate from a large corpus constraints conjunctions on the positive or negative semantic orientation of conjoined adjectives. A log-linear regression model uses these to predict whether adjectives are same different orientations, achieving 82% accuracy in this task when each conjunction is considered independently. Combining across many adjectives, clustering algorithm separates into groups finally, labeled negative. Evaluations real data simulation experiments indicate high levels performance: classification precision more than 90% for that occur modest number corpus.

linguistics, semantic interpretation, semantic similarity, semantic analysis (linguistics), language, nlp task, semantic evaluation, semantics, natural language processing, semantic processing, computational linguistics, semantic orientation, language model",1997,991,linguistics|semantic interpretation|semantic similarity|semantic analysis (linguistics)|language|nlp task|semantic evaluation|semantics|natural language processing|semantic processing|computational linguistics|semantic orientation|language model,https://openalex.org/W2108646579|https://openalex.org/W2019759670|https://openalex.org/W1964613733|https://openalex.org/W2949998441|https://openalex.org/W2612769033|https://openalex.org/W2088622183
https://openalex.org/W2891177506,Universal Sentence Encoder for English,"Universal Sentence Encoder for English

Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, Ray Kurzweil. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2018.

linguistics, english, natural language processing, language engineering, language, computational linguistics, nlp task, machine translation, english language, annotation tool, universal sentence encoder",2018,989,linguistics|english|natural language processing|language engineering|language|computational linguistics|nlp task|machine translation|english language|annotation tool|universal sentence encoder,
https://openalex.org/W2340954483,Abusive Language Detection in Online User Content,"Abusive Language Detection in Online User Content

Detection of abusive language in user generated online content has become an issue increasing importance recent years. Most current commercial methods make use blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples hate speech. In this work, we develop a machine learning based method to detect speech on comments from two domains which outperforms state-of-the-art deep approach. We also corpus annotated for language, the first its kind. Finally, our detection tool analyze over time different settings further enhance knowledge behavior.

computer science, disinformation detection, abusive language detection, natural language processing, user-generated content, online user content, online information",2016,985,computer science|disinformation detection|abusive language detection|natural language processing|user-generated content|online user content|online information,https://openalex.org/W2740168486
https://openalex.org/W2113772582,Subtlex-UK: A New and Improved Word Frequency Database for British English,"Subtlex-UK: A New and Improved Word Frequency Database for British English

We present word frequencies based on subtitles of British television programmes. show that the SUBTLEX-UK explain more variance in lexical decision times Lexicon Project than National Corpus and SUBTLEX-US frequencies. In addition to form frequencies, we also measures contextual diversity part-of-speech specific children programmes, bigram giving researchers English access full range norms recently made available for other languages. Finally, introduce a new measure frequency, Zipf scale, which hope will stop current misunderstandings frequency effect.

language resource, linguistics, language assessment, computational linguistics, english language, language, terminology extraction, historical linguistics, general linguistics, writer identification, language corpus, english, text mining, natural language processing, computational lexicology, australian english, british english, corpus linguistics, language monitoring",2014,981,language resource|linguistics|language assessment|computational linguistics|english language|language|terminology extraction|historical linguistics|general linguistics|writer identification|language corpus|english|text mining|natural language processing|computational lexicology|australian english|british english|corpus linguistics|language monitoring,
https://openalex.org/W1714665356,A biterm topic model for short texts,"A biterm topic model for short texts

Uncovering the topics within short texts, such as tweets and instant messages, has become an important task for many content analysis applications. However, directly applying conventional topic models (e.g. LDA PLSA) on texts may not work well. The fundamental reason lies in that implicitly capture document-level word co-occurrence patterns to reveal topics, thus suffer from severe data sparsity documents. In this paper, we propose a novel way modeling referred biterm model (BTM). Specifically, BTM learn by generation of (i.e. biterms) whole corpus. major advantages are 1) explicitly enhance learning; 2) uses aggregated corpus learning solve problem sparse at document-level. We carry out extensive experiments real-world text collections. results demonstrate our approach can discover more prominent coherent significantly outperform baseline methods several evaluation metrics. Furthermore, find even normal showing potential generality wider usage new model.

computer science, linguistics, concept drift, topic model, computational linguistics, information fusion, natural language generation, text segmentation, data science, short texts, language model, deep learning, machine learning research, text mining, natural language processing, large language model, biterm topic model, knowledge discovery, sequence modelling",2013,980,computer science|linguistics|concept drift|topic model|computational linguistics|information fusion|natural language generation|text segmentation|data science|short texts|language model|deep learning|machine learning research|text mining|natural language processing|large language model|biterm topic model|knowledge discovery|sequence modelling,
https://openalex.org/W2963042536,A Primer on Neural Network Models for Natural Language Processing,"A Primer on Neural Network Models for Natural Language Processing

Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such image recognition and speech processing. More recently, network models started to be applied also textual natural language signals, again with very promising results. This tutorial surveys from perspective of processing research, an attempt bring natural-language researchers up speed techniques. The covers input encoding for tasks, feed-forward networks, convolutional recurrent recursive well computation graph abstraction automatic gradient computation.

neural network models, nlp task, natural language processing, neural network (machine learning), computer science, language model, machine learning, deep learning",2016,976,neural network models|nlp task|natural language processing|neural network (machine learning)|computer science|language model|machine learning|deep learning,https://openalex.org/W2884001105|https://openalex.org/W2964236337|https://openalex.org/W2742947407
https://openalex.org/W2948947170,What Does BERT Learn about the Structure of Language?,"What Does BERT Learn about the Structure of Language?

BERT is a recent language representation model that has surprisingly performed well in diverse understanding benchmarks. This result indicates the possibility networks capture structural information about language. In this work, we provide novel support for claim by performing series of experiments to unpack elements English structure learned BERT. Our findings are fourfold. BERT’s phrasal captures phrase-level lower layers. The intermediate layers compose rich hierarchy linguistic information, starting with surface features at bottom, syntactic middle followed semantic top. requires deeper while tracking subject-verb agreement handle long-term dependency problem. Finally, compositional scheme underlying mimics classical, tree-like structures.

linguistics, psycholinguistics, structural linguistics, natural language processing, language, language comprehension, cognitive science, language development, language acquisition, language science, language learning, second language acquisition",2019,975,linguistics|psycholinguistics|structural linguistics|natural language processing|language|language comprehension|cognitive science|language development|language acquisition|language science|language learning|second language acquisition,
https://openalex.org/W2120831831,Putting Feelings Into Words,"Putting Feelings Into Words

Putting feelings into words (affect labeling) has long been thought to help manage negative emotional experiences; however, the mechanisms by which affect labeling produces this benefit remain largely unknown. Recent neuroimaging studies suggest a possible neurocognitive pathway for process, but methodological limitations of previous have prevented strong inferences from being drawn. A functional magnetic resonance imaging study was conducted remedy these limitations. The results indicated that labeling, relative other forms encoding, diminished response amygdala and limbic regions images. Additionally, produced increased activity in single brain region, right ventrolateral prefrontal cortex (RVLPFC). Finally, RVLPFC during were inversely correlated, relationship mediated medial (MPFC). These may diminish reactivity along MPFC amygdala.

affect, poetics, linguistics, psycholinguistics, affective neuroscience, language grounding, natural language processing, narrative, language, affective computing, communication, emotional intelligence, semantics, semantic processing, speech communication, emotion recognition, emotion, emotional response",2007,973,affect|poetics|linguistics|psycholinguistics|affective neuroscience|language grounding|natural language processing|narrative|language|affective computing|communication|emotional intelligence|semantics|semantic processing|speech communication|emotion recognition|emotion|emotional response,
https://openalex.org/W2061271742,A stochastic parts program and noun phrase parser for unrestricted text,"A stochastic parts program and noun phrase parser for unrestricted text

Article Free Access Share on A stochastic parts program and noun phrase parser for unrestricted text Author: Kenneth Ward Church Bell Laboratories, Murray Hill, N. J. J.View Profile Authors Info & Claims ANLC '88: Proceedings of the second conference Applied natural language processingFebruary 1988 Pages 136–143https://doi.org/10.3115/974235.974260Online:09 February 1988Publication History 279citation1,990DownloadsMetricsTotal Citations279Total Downloads1,990Last 12 Months83Last 6 weeks9 Get Citation AlertsNew Alert added!This alert has been successfully added will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log in to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

computer science, keyword extraction, natural language processing, unrestricted text, computational linguistics, stochastic parts program, noun phrase parser, syntactic parsing",1988,971,computer science|keyword extraction|natural language processing|unrestricted text|computational linguistics|stochastic parts program|noun phrase parser|syntactic parsing,https://openalex.org/W1996430422|https://openalex.org/W2010595692|https://openalex.org/W2199803028|https://openalex.org/W2127314673|https://openalex.org/W2164455818
https://openalex.org/W1942214758,Saliency detection by multi-context deep learning,"Saliency detection by multi-context deep learning

Low-level saliency cues or priors do not produce good enough detection results especially when the salient object presents in a low-contrast background with confusing visual appearance. This issue raises serious problem for conventional approaches. In this paper, we tackle by proposing multi-context deep learning framework detection. We employ Convolutional Neural Networks to model of objects images. Global context and local are both taken into account, jointly modeled unified framework. To provide better initialization training neural networks, investigate different pre-training strategies, task-specific scheme is designed make modeling suited Furthermore, recently proposed contemporary models ImageNet Image Classification Challenge tested, their effectiveness investigated. Our approach extensively evaluated on five public datasets, experimental show significant consistent improvements over state-of-the-art methods.

image analysis, computational imaging, computer science, information fusion, natural language processing, scene interpretation, scene understanding, computer vision, machine learning, feature detection, saliency detection, cognitive science, object detection, data science, deep learning, automatic classification, multi-context deep learning, machine vision",2015,967,image analysis|computational imaging|computer science|information fusion|natural language processing|scene interpretation|scene understanding|computer vision|machine learning|feature detection|saliency detection|cognitive science|object detection|data science|deep learning|automatic classification|multi-context deep learning|machine vision,
https://openalex.org/W2027979924,CoNLL-X shared task on multilingual dependency parsing,"CoNLL-X shared task on multilingual dependency parsing

Each year the Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their systems exactly same data sets, order to better compare systems. The tenth CoNLL (CoNLL-X) saw task Multilingual Dependency Parsing. In this paper, we describe how treebanks for 13 languages were converted into dependency format parsing performance was measured. We also give an overview of approaches that took results they achieved. Finally, try draw general conclusions about multi-lingual parsing: What makes particular language, treebank or annotation scheme easier harder parse phenomena are challenging any parser?

computer science, linguistics, multilingual dependency, dependency linguistics, natural language processing, computational linguistics, nlp task, semantic interpretation",2006,966,computer science|linguistics|multilingual dependency|dependency linguistics|natural language processing|computational linguistics|nlp task|semantic interpretation,
https://openalex.org/W2963963856,Neural Responding Machine for Short-Text Conversation,"Neural Responding Machine for Short-Text Conversation

Lifeng Shang, Zhengdong Lu, Hang Li. Proceedings of the 53rd Annual Meeting Association for Computational Linguistics and 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.

communication, spoken language technology, neural computation, neuroscience, natural language processing, neural network (machine learning), short-text conversation, computer science, neural machine translation, language model, cognitive communication, deep learning",2015,966,communication|spoken language technology|neural computation|neuroscience|natural language processing|neural network (machine learning)|short-text conversation|computer science|neural machine translation|language model|cognitive communication|deep learning,https://openalex.org/W2963206148|https://openalex.org/W1591706642|https://openalex.org/W2963167310
https://openalex.org/W2962706528,Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN),"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)

In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of word given previous words and an image. Image captions are generated by sampling from distribution. The consists two sub-networks: deep recurrent neural network sentences convolutional images. These sub-networks interact with each other in layer to form whole m-RNN model. effectiveness our is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, 30K MS COCO). Our outperforms state-of-the-art methods. addition, apply retrieval tasks retrieving images or sentences, achieves significant performance improvement over methods which optimize ranking objective function retrieval. project page work is: www.stat.ucla.edu/~junhua.mao/m-RNN.html .

coreference resolution, computer science, recurrent neural network, language model, text mining, deep learning, human-computer interaction, annotation tool, narrative extraction, natural language processing, deep captioning, natural language generation, image representation, machine learning, data science, cognitive science, semantic evaluation, multimodal signal processing, image communication",2014,965,coreference resolution|computer science|recurrent neural network|language model|text mining|deep learning|human-computer interaction|annotation tool|narrative extraction|natural language processing|deep captioning|natural language generation|image representation|machine learning|data science|cognitive science|semantic evaluation|multimodal signal processing|image communication,https://openalex.org/W1514535095|https://openalex.org/W2950178297|https://openalex.org/W2963206148|https://openalex.org/W2550553598|https://openalex.org/W2963223306|https://openalex.org/W2405756170|https://openalex.org/W2139501017|https://openalex.org/W2425121537|https://openalex.org/W2949999304
https://openalex.org/W2963167310,Deep Reinforcement Learning for Dialogue Generation,"Deep Reinforcement Learning for Dialogue Generation

Recent neural models of dialogue generation offer great promise for generating responses conversational agents, but tend to be shortsighted, predicting utterances one at a time while ignoring their influence on future outcomes.Modeling the direction is crucial coherent, interesting dialogues, need which led traditional NLP draw reinforcement learning.In this paper, we show how integrate these goals, applying deep learning model reward in chatbot dialogue.The simulates dialogues between two virtual using policy gradient methods sequences that display three useful properties: informativity, coherence, and ease answering (related forward-looking function).We evaluate our diversity, length as well with human judges, showing proposed algorithm generates more interactive manages foster sustained conversation simulation.This work marks first step towards based long-term success dialogues.

computer science, deep reinforcement learning, reinforcement learning, natural language processing, dialogue generation, language learning",2016,964,computer science|deep reinforcement learning|reinforcement learning|natural language processing|dialogue generation|language learning,https://openalex.org/W2884001105|https://openalex.org/W2938704169
https://openalex.org/W2968124245,VisualBERT: A Simple and Performant Baseline for Vision and Language,"VisualBERT: A Simple and Performant Baseline for Vision and Language

We propose VisualBERT, a simple and flexible framework for modeling broad range of vision-and-language tasks. VisualBERT consists stack Transformer layers that implicitly align elements an input text regions in associated image with self-attention. further two visually-grounded language model objectives pre-training on caption data. Experiments four tasks including VQA, VCR, NLVR2, Flickr30K show outperforms or rivals state-of-the-art models while being significantly simpler. Further analysis demonstrates can ground to without any explicit supervision is even sensitive syntactic relationships, tracking, example, associations between verbs corresponding their arguments.

computer science, human-computer interaction, visual language, visual reasoning, language model, natural language processing, language, language assessment, vision language model, performant baseline, computer vision, visualization, cognitive science, visual perception, image representation, baseline correction, visual question answering",2019,960,computer science|human-computer interaction|visual language|visual reasoning|language model|natural language processing|language|language assessment|vision language model|performant baseline|computer vision|visualization|cognitive science|visual perception|image representation|baseline correction|visual question answering,https://openalex.org/W3094502228
https://openalex.org/W2161793142,Text Mining Infrastructure in<i>R</i>,"Text Mining Infrastructure in<i>R</i>

During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present <strong>tm</strong> package which provides framework for applications within R. give survey on facilities in R explain how typical application tasks can be carried out using our framework. techniques count-based analysis methods, clustering, classification string kernels.

computer science, information technology, topic model, text mining, natural language processing, text mining infrastructure, data science, knowledge discovery, content similarity detection, text processing",2008,959,computer science|information technology|topic model|text mining|natural language processing|text mining infrastructure|data science|knowledge discovery|content similarity detection|text processing,
https://openalex.org/W2054125330,A probabilistic earley parser as a psycholinguistic model,"A probabilistic earley parser as a psycholinguistic model

In human sentence processing, cognitive load can be defined many ways. This report considers a definition of in terms the total probability structural options that have been disconfirmed at some point sentence: surprisal word wi given its prefix wo...i-1 on phrase-structural language model. These loads efficiently calculated using probabilistic Earley parser (Stolcke, 1995) which is interpreted as generating predictions about reading time word-by-word basis. Under grammatical assumptions supported by corpus-frequency data, operation Stolcke's correctly predicts processing phenomena associated with garden path ambiguity and subject/object relative asymmetry.

psycholinguistic model, computer science, linguistics, psycholinguistics, language model, natural language processing, speech processing, speech perception, language comprehension, machine learning, cognitive science, semantic interpretation, probabilistic earley parser",2001,959,psycholinguistic model|computer science|linguistics|psycholinguistics|language model|natural language processing|speech processing|speech perception|language comprehension|machine learning|cognitive science|semantic interpretation|probabilistic earley parser,
https://openalex.org/W2888482885,"Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization","Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization

We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create short, one-sentence news summary answering the question “What article about?”. collect real-world, large-scale dataset this by harvesting online articles from British Broadcasting Corporation (BBC). propose novel model conditioned on article’s topics based entirely convolutional neural networks. demonstrate experimentally that architecture captures long-range dependencies in document recognizes pertinent content, outperforming oracle system state-of-the-art approaches when evaluated automatically humans.

automatic summarization, knowledge discovery, annotation tool, natural language processing, narrative summarization, topic model, cognitive science, computer science, language model, machine learning, extreme summarization, text mining, deep learning, data science",2018,958,automatic summarization|knowledge discovery|annotation tool|natural language processing|narrative summarization|topic model|cognitive science|computer science|language model|machine learning|extreme summarization|text mining|deep learning|data science,https://openalex.org/W3034999214|https://openalex.org/W2970419734
https://openalex.org/W2150009200,The Neural Architecture of the Language Comprehension Network: Converging Evidence from Lesion and Connectivity Analyses,"The Neural Architecture of the Language Comprehension Network: Converging Evidence from Lesion and Connectivity Analyses

While traditional models of language comprehension have focused on the left posterior temporal cortex as neurological basis for comprehension, lesion and functional imaging studies indicate involvement an extensive network cortical regions. However, full extent this white matter pathways that contribute to it remain be characterized. In earlier voxel-based lesion-symptom mapping analysis data from aphasic patients (Dronkers et al., 2004), several brain regions in hemisphere were found critical comprehension: middle gyrus (MTG), anterior part Brodmann's area 22 superior (anterior STG/BA22), sulcus (STS) extending into 39 (STS/BA39), orbital inferior frontal (BA47) (BA46). Here, we investigated associated with these using diffusion tensor healthy subjects. We also used resting-state magnetic resonance assess connectivity profiles Fiber tractography analyses indicated MTG, STG/BA22, STS/BA39 BA47 are a richly interconnected extends additional frontal, parietal two hemispheres. The occipito-frontal fasciculus, arcuate fasciculus longitudinal fasciculi, well transcallosal projections via tapetum most prominent bridging important comprehension. MTG showed particularly structural pattern which is consistent severity impairments lesions suggests central role region

natural language processing, neurolinguistics, language, language network, language comprehension, neuroscience, machine learning, recurrent neural network, neural architecture, cognitive science, converging evidence, neural computation, semantic processing, connectivity analyses, language processing in the brain, neural network (machine learning), language science, language comprehension network",2011,957,natural language processing|neurolinguistics|language|language network|language comprehension|neuroscience|machine learning|recurrent neural network|neural architecture|cognitive science|converging evidence|neural computation|semantic processing|connectivity analyses|language processing in the brain|neural network (machine learning)|language science|language comprehension network,
https://openalex.org/W2963096510,Hierarchical Neural Story Generation,"Hierarchical Neural Story Generation

We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. collect large dataset 300K human-written stories paired with writing prompts from an online forum. Our enables hierarchical generation, where the model first generates premise, then transforms it into passage text. gain further improvements novel form fusion improves relevance to prompt, adding new gated multi-scale self-attention mechanism long-range context. Experiments show over strong baselines on both automated human evaluations. Human judges prefer generated by our approach those non-hierarchical factor two one.

computer science, natural language generation, large language model, language, language generation, generative ai, narrative extraction, natural language processing, text mining, narrative representation, narrative, cognitive science, language model",2018,951,computer science|natural language generation|large language model|language|language generation|generative ai|narrative extraction|natural language processing|text mining|narrative representation|narrative|cognitive science|language model,https://openalex.org/W3019166713|https://openalex.org/W2938704169|https://openalex.org/W2888482885
https://openalex.org/W2108420397,Joint sentiment/topic model for sentiment analysis,"Joint sentiment/topic model for sentiment analysis

Sentiment analysis or opinion mining aims to use automated tools detect subjective information such as opinions, attitudes, and feelings expressed in text. This paper proposes a novel probabilistic modeling framework based on Latent Dirichlet Allocation (LDA), called joint sentiment/topic model (JST), which detects sentiment topic simultaneously from Unlike other machine learning approaches classification often require labeled corpora for classifier training, the proposed JST is fully unsupervised. The has been evaluated movie review dataset classify polarity minimum prior have also explored further improve accuracy. Preliminary experiments shown promising results achieved by JST.

computer science, information fusion, disinformation detection, text mining, natural language processing, topic model, multimodal sentiment analysis, machine learning, data science, knowledge discovery, social medium data, analytics, machine learning research, social media, sentiment analysis",2009,948,computer science|information fusion|disinformation detection|text mining|natural language processing|topic model|multimodal sentiment analysis|machine learning|data science|knowledge discovery|social medium data|analytics|machine learning research|social media|sentiment analysis,https://openalex.org/W2108646579|https://openalex.org/W2113459411|https://openalex.org/W4211186029|https://openalex.org/W2612769033
https://openalex.org/W1410460,Handbook of Natural Language Processing,"Handbook of Natural Language Processing

The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter expands the content to include emerging areas, such as sentiment analysis.New EditionGreater

computer science, linguistics, keyword extraction, text mining, language model, natural language processing, language, natural language generation, language engineering, machine learning, computational linguistics, nlp task",2010,947,computer science|linguistics|keyword extraction|text mining|language model|natural language processing|language|natural language generation|language engineering|machine learning|computational linguistics|nlp task,https://openalex.org/W2108646579
https://openalex.org/W4246858749,Relevance based language models,"Relevance based language models

We explore the relation between classical probabilistic models of information retrieval and emerging language modeling approaches. It has long been recognized that primary obstacle to effective performance is need estimate arelevance model: probabilities words in relevant class. propose a novel technique for estimating these using query alone. demonstrate our can produce highly accurate relevance models, addressing important notions synonymy polysemy. Our experiments show outperforming baseline systems on TREC TDT tracking tasks. The main contribution this work an formal method model with no training data.

relevance logic, computer science, text mining, language model, natural language processing, language, large language model, machine learning, semantic evaluation, relevance feedback, data science, knowledge discovery, machine learning research, information retrieval",2001,945,relevance logic|computer science|text mining|language model|natural language processing|language|large language model|machine learning|semantic evaluation|relevance feedback|data science|knowledge discovery|machine learning research|information retrieval,
https://openalex.org/W2963970792,Language modeling with gated convolutional networks,"Language modeling with gated convolutional networks

The pre-dominant approach to language modeling date is based on recurrent neural networks. Their success this task often linked their ability capture unbounded context. In paper we develop a finite context through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose novel simplified gating mechanism that outperforms Oord et al. (2016b) and investigate the impact of key architectural decisions. proposed achieves state-of-the-art WikiText-103 benchmark, even though it features long-term dependencies, as well competitive results Google Billion Words benchmark. Our model reduces latency score sentence by an order magnitude compared baseline. To our knowledge, first time non-recurrent with strong models these large scale tasks.

computer science, linguistics, convolutional neural network, large language model, language model, natural language processing, language, language engineering, gated convolutional networks, neural machine translation, machine learning, computational linguistics, computational intelligence, deep learning, language recognition, machine translation, spoken language technology, language learning",2017,944,computer science|linguistics|convolutional neural network|large language model|language model|natural language processing|language|language engineering|gated convolutional networks|neural machine translation|machine learning|computational linguistics|computational intelligence|deep learning|language recognition|machine translation|spoken language technology|language learning,https://openalex.org/W2745461083|https://openalex.org/W2964110616|https://openalex.org/W2884001105|https://openalex.org/W2970476646|https://openalex.org/W2938704169|https://openalex.org/W2888482885|https://openalex.org/W2963096510
https://openalex.org/W4245349101,An Overview of the KL‐ONE Knowledge Representation System*,"An Overview of the KL‐ONE Knowledge Representation System*

KL‐ONE is a system for representing knowledge in Artificial Intelligence programs. It has been developed and refined over long period used both basic research implemented knowledge‐based systems number of places the AI community. Here we present kernel ideas KL‐ONE, emphasizing its ability to form complex structured descriptions. In addition detailing all KL‐ONE's description‐forming structures, discuss bit philosophy underlying system, highlight notions taxonomy classification that are central it, include an extended example use classifier recognition task.

knowledge representation and reasoning, knowledge-based system, knowledge-based reasoning, natural language processing, language model, language, distributed knowledge, knowledge integration, knowledge discovery, knowledge extraction, knowledge architecture, knowledge management",1985,944,knowledge representation and reasoning|knowledge-based system|knowledge-based reasoning|natural language processing|language model|language|distributed knowledge|knowledge integration|knowledge discovery|knowledge extraction|knowledge architecture|knowledge management,
https://openalex.org/W1583758360,Classifiers: A Typology of Noun Categorization Devices,"Classifiers: A Typology of Noun Categorization Devices

Almost all languages have some grammatical means for categorizing nouns. This book provides a comprehensive and original analysis of noun categorization devices over the world. It will interest typologists, those working in fields morphosyntactic variation lexical semantics, as well anthropologists other scholars interested mechanisms human cognition.

pattern recognition, linguistics, natural language processing, cognitive science, semantics, linguistic typology, nlp task, automatic classification, classification method, noun categorization devices",2000,942,pattern recognition|linguistics|natural language processing|cognitive science|semantics|linguistic typology|nlp task|automatic classification|classification method|noun categorization devices,
