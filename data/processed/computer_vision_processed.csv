id,title,content,year,cited_by_count,keywords,children
https://openalex.org/W1963623641,A survey of image registration techniques,"A survey of image registration techniques

Registration is a fundamental task in image processing used to match two or more pictures taken, for example, at different times, from sensors, viewpoints. Virtually all large systems which evaluate images require the registration of images, closely related operation, as an intermediate step. Specific examples where significant component include matching target with real-time scene recognition, monitoring global land usage using satellite stereo recover shape autonomous navigation, and aligning medical modalities diagnosis. Over years, broad range techniques has been developed various types data problems. These have independently studied several applications, resulting body research. This paper organizes this material by establishing relationship between variations type can most appropriately be applied. Three major are distinguished. The first due differences acquisition cause misaligned. To register spatial transformation found will remove these variations. class transformations must searched find optimal determined knowledge about type. turn influences general technique that should taken. second those also acquisition, but cannot modeled easily such lighting atmospheric conditions. usually effects intensity values, they may spatial, perspective distortions. third interest object movements, growths, other changes. Variations not directly removed registration, make difficult since exact no longer possible. In particular, it critical removed. Knowledge characteristics each variation effect choice feature space, similarity measure, search strategy up final technique. All viewed combinations choices. framework useful understanding merits relationships wide variety existing assisting selection suitable specific problem.

image analysis, pattern recognition, computer science, image registration techniques, computer vision, image registration, image classification, digital image processing",1992,4015,image analysis|pattern recognition|computer science|image registration techniques|computer vision|image registration|image classification|digital image processing,https://openalex.org/W2131938193
https://openalex.org/W2164568552,Methods of combining multiple classifiers and their applications to handwriting recognition,"Methods of combining multiple classifiers and their applications to handwriting recognition

Possible solutions to the problem of combining classifiers can be divided into three categories according levels information available from various classifiers. Four approaches based on different methodologies are proposed for solving this problem. One is suitable individual such as Bayesian, k-nearest-neighbor, and distance The other could used any kind On applying these methods combine several recognizing totally unconstrained handwritten numerals, experimental results show that performance improved significantly. For example, US zipcode database, 98.9% recognition with 0.90% substitution 0.2% rejection obtained, well high reliability 95% recognition, 0% substitution, 5% rejection.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, pattern recognition, computer science, information fusion, character recognition, multiple classifier system, computer vision, machine learning, feature fusion, multiple classifiers, deep learning, machine learning research, automatic classification, classification method",1992,2186,image analysis|pattern recognition|computer science|information fusion|character recognition|multiple classifier system|computer vision|machine learning|feature fusion|multiple classifiers|deep learning|machine learning research|automatic classification|classification method,
https://openalex.org/W1990005524,Rapid Automated Algorithm for Aligning and Reslicing PET Images,"Rapid Automated Algorithm for Aligning and Reslicing PET Images

A computer algorithm for the three-dimensional (3D) alignment of PET images is described. To align two images, calculates ratio one image to other on a voxel-by-voxel basis and then iteratively moves relative another minimize variance this across voxels. Since method relies anatomic information in rather than external fiducial markers, it can be applied retrospectively. Validation studies using 3D brain phantom show that aligns acquired at wide variety positions with maximum positional errors are usually less width voxel (1.745 mm). Simulated cortical activation sites do not interfere alignment. Global quantitation from realignment <2%. Regional due partial volume effects largest when gantry rotated by large angles or bed translated axially one-half interplane distance. such effects, used prospectively, during acquisition, reposition scanner match an earlier study. Computation requires 3–6 min Sun SPARCstation 2.

image analysis, pattern recognition, computer science, computational imaging, medical image computing, positron emission tomography, medical imaging, radiology, biomedical imaging, image stitching, medical image analysis, computer vision, pet images, image reconstruction, image representation, reconstruction technique, digital image processing",1992,1947,image analysis|pattern recognition|computer science|computational imaging|medical image computing|positron emission tomography|medical imaging|radiology|biomedical imaging|image stitching|medical image analysis|computer vision|pet images|image reconstruction|image representation|reconstruction technique|digital image processing,https://openalex.org/W2131938193|https://openalex.org/W2104612543
https://openalex.org/W2144158572,A CFAR adaptive matched filter detector,"A CFAR adaptive matched filter detector

An adaptive algorithm for radar target detection using an antenna array is proposed. The detector derived in a manner similar to that of the generalized likelihood-ratio test (GLRT) but contains simplified statistic limiting case GLRT detector. This analyzed performance signals on boresight, as well when signal direction misaligned with look direction.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, pattern recognition, computer science, filter (signal processing), radar image processing, computer vision, adaptive filter, filter design, filter detector",1992,1417,image analysis|pattern recognition|computer science|filter (signal processing)|radar image processing|computer vision|adaptive filter|filter design|filter detector,
https://openalex.org/W2106588364,Tree visualization with tree-maps,"Tree visualization with tree-maps

article Free Access Share on Tree visualization with tree-maps: 2-d space-filling approach Author: Ben Shneiderman Univ. of Maryland, College Park ParkView Profile Authors Info & Claims ACM Transactions GraphicsVolume 11Issue 1pp 92–99https://doi.org/10.1145/102377.115768Published:02 January 1992Publication History 955citation7,810DownloadsMetricsTotal Citations955Total Downloads7,810Last 12 Months539Last 6 weeks58 Get Citation AlertsNew Alert added!This alert has been successfully added and will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click the button below.Manage my Alert!Please log in to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

computer science, data and information visualization, scientific visualization, image analysis, information visualization, graph theory, visualization, data science, forest ecosystem, tree language, tree growth, computational visualization, forest research, tree visualization, interactive visualization, computer graphic, computer vision, geography, forest ecology",1992,1307,computer science|data and information visualization|scientific visualization|image analysis|information visualization|graph theory|visualization|data science|forest ecosystem|tree language|tree growth|computational visualization|forest research|tree visualization|interactive visualization|computer graphic|computer vision|geography|forest ecology,
https://openalex.org/W1530454533,Geometric invariance in computer vision,"Geometric invariance in computer vision

Part 1 Foundations: algebraic invariants - invariant theory and enumerative combinatorics of young tableaux, Shreeram S. Abhyankar, geometric interpretation joint conic invariants, Joseph L. Mundy, et al, an experimental evaluation projective Christopher Coelho, al the projection two non-coplanar conics, Stephen J. Maybank non-existence general-case view-invariants, Brian Burns, non-algebraic curves noise resistant curves, Isaac Weiss, semi-differential Luc Van Gool, for in three dimensions, Michael H. Brill, numerical differential Brown, recognizing general curved objects efficiently, Andrew Zisserman, fitting affine conics to Deepak Kapur projectively decomposition planar shapes, Stefan Carlsson from multiple views linear methods photogrammetry model-matching, Eamon B. Barrett, nonplanar disambiguating stereo matches with spatio-temporal surfaces, Olivier Faugeras Theo Papadopoulo. 2 Applications: transformation indexing, Haim Wolfson Yehezkel Lamdan model-based recognition, John E. Hopcroft, object recognition based on moment (or algebraic) Gabriel Taubin David Cooper fast using Charles A. Rothwell, toward 3D image contours, Jean Ponce Kriegman relative positioning uncalibrated cameras, Roger Mohr, al. Appendix: geometry machine vision, Mundy Zisserman.

geometric invariance, computer science, computer vision",1992,1024,geometric invariance|computer science|computer vision,
https://openalex.org/W2095727900,"Multilayer perceptron, fuzzy sets, and classification","Multilayer perceptron, fuzzy sets, and classification

A fuzzy neural network model based on the multilayer perceptron, using backpropagation algorithm, and capable of classification patterns is described. The input vector consists membership values to linguistic properties while output defined in terms class values. This allows efficient modeling uncertain with appropriate weights being assigned backpropagated errors depending upon at corresponding outputs. During training, learning rate gradually decreased discrete steps until converges a minimum error solution. effectiveness algorithm demonstrated speech recognition problem. results are compared those conventional MLP, Bayes classifier, other related models.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

computer science, fuzzy pattern recognition, fuzzy logic, computer vision, machine learning, deep learning, machine learning research, multilayer perceptron, machine vision, fuzzy set",1992,1018,computer science|fuzzy pattern recognition|fuzzy logic|computer vision|machine learning|deep learning|machine learning research|multilayer perceptron|machine vision|fuzzy set,
https://openalex.org/W2046911213,Feature-based image metamorphosis,"Feature-based image metamorphosis

article Free Access Share on Feature-based image metamorphosis Authors: Thaddeus Beier Silicon Graphics Computer Systems201 I Shoreline Blvd. Mountain View CA CAView Profile , Shawn Neely Pacific Data Images, 1111 Karlsbad Drive, Sunnyvale Authors Info & Claims ACM SIGGRAPH GraphicsVolume 26Issue 2July 1992 pp 35–42https://doi.org/10.1145/142920.134003Online:01 July 1992Publication History 686citation6,194DownloadsMetricsTotal Citations686Total Downloads6,194Last 12 Months240Last 6 weeks51 Get Citation AlertsNew Alert added!This alert has been successfully added and will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click the button below.Manage my Alert!Please log in to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

image similarity, feature-based image metamorphosis, digital image processing, computer vision, pattern recognition, computational imaging, machine vision, content-based image retrieval, computer science, image manipulation, scene interpretation, image representation, feature (computer vision), computer graphic, image analysis",1992,922,image similarity|feature-based image metamorphosis|digital image processing|computer vision|pattern recognition|computational imaging|machine vision|content-based image retrieval|computer science|image manipulation|scene interpretation|image representation|feature (computer vision)|computer graphic|image analysis,
https://openalex.org/W2120041316,A filter bank for the directional decomposition of images: theory and design,"A filter bank for the directional decomposition of images: theory and design

The authors introduce a directionally oriented 2-D filter bank with the property that individual channels may be critically sampled without loss of information. passband regions component filters are wedge-shaped and thus provide directional It is shown these outputs maximally decimated to achieve minimum sample representation in way permits original signal exactly reconstructed. discuss theory for decomposition reconstruction. In addition, implementation issues addressed where realizations based on both recursive nonrecursive considered.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

pattern recognition, computer science, directional decomposition, machine learning, image denoising, automatic classification, principal component analysis, image analysis, data science, image representation, computational imaging, filtering technique, multimedia information processing, machine vision, digital image processing, spatial filtering, computer vision, multimedia retrieval, design",1992,879,pattern recognition|computer science|directional decomposition|machine learning|image denoising|automatic classification|principal component analysis|image analysis|data science|image representation|computational imaging|filtering technique|multimedia information processing|machine vision|digital image processing|spatial filtering|computer vision|multimedia retrieval|design,
https://openalex.org/W2039306215,Multitarget-Multisensor Tracking: Applications and Advances,"Multitarget-Multisensor Tracking: Applications and Advances

Practical Aspects of Multisensor Tracking. Survey Assignment Techniques for Multitarget IMM Estimator with Nearest Neighbor Joint Probabilistic Data Association. Multiassignment Tracking a Large Number Overlapping Objects. Finite Difference Methods Nonlinear Filtering and Automatic Target Recognition. Scale Ground Single Multiple MTI Sensors. Radar Systems Modeling ECM Waveform Detection/Tracking Performance at the System Level. Engineer's Guide to Variable-Structure Multiple-Model Estimation

image analysis, computer science, information fusion, sensor, object tracking, multitarget-multisensor tracking, multi-sensor management, computer vision, machine learning, motion detection, multi-sensor information fusion, data science, moving object tracking, machine vision, digital image processing",1992,769,image analysis|computer science|information fusion|sensor|object tracking|multitarget-multisensor tracking|multi-sensor management|computer vision|machine learning|motion detection|multi-sensor information fusion|data science|moving object tracking|machine vision|digital image processing,
https://openalex.org/W2049348043,Image compression using the 2-D wavelet transform,"Image compression using the 2-D wavelet transform

The 2-D orthogonal wavelet transform decomposes images into both spatial and spectrally local coefficients. transformed coefficients were coded hierarchically individually quantized in accordance with the estimated noise sensitivity of human visual system (HVS). algorithm can be mapped easily onto VLSI. For Miss America Lena monochrome images, technique gave high to acceptable quality reconstruction at compression ratios 0.3-0.2 0.64-0.43 bits per pixel (bpp), respectively.

image analysis, computational imaging, computer science, lossy compression, image compression, computer vision, wavelet, image representation, digital image processing",1992,719,image analysis|computational imaging|computer science|lossy compression|image compression|computer vision|wavelet|image representation|digital image processing,https://openalex.org/W2053691921
https://openalex.org/W2053691921,Embedded image coding using zerotrees of wavelet coefficients,"Embedded image coding using zerotrees of wavelet coefficients

The embedded zerotree wavelet algorithm (EZW) is a simple, yet remarkably effective, image compression algorithm, having the property that bits in bit stream are generated order of importance, yielding fully code. code represents sequence binary decisions distinguish an from ""null"" image. Using coding encoder can terminate encoding at any point thereby allowing target rate or distortion metric to be met exactly. Also, given stream, decoder cease decoding and still produce exactly same would have been encoded corresponding truncated stream. In addition producing EZW consistently produces results competitive with virtually all known algorithms on standard test images. Yet this performance achieved technique requires absolutely no training, pre-stored tables codebooks, prior knowledge source. based four key concepts: (1) discrete transform hierarchical subband decomposition, (2) prediction absence significant information across scales by exploiting self-similarity inherent images, (3) entropy-coded successive-approximation quantization, (4) universal lossless data which via adaptive arithmetic coding.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, computational imaging, computer science, image compression, image coding, digital signal processing, computer vision, wavelet coefficients, wavelet, image representation, wavelet theory, principal component analysis, digital image processing",1993,4862,image analysis|computational imaging|computer science|image compression|image coding|digital signal processing|computer vision|wavelet coefficients|wavelet|image representation|wavelet theory|principal component analysis|digital image processing,
https://openalex.org/W2102796633,High confidence visual recognition of persons by a test of statistical independence,"High confidence visual recognition of persons by a test of statistical independence

A method for rapid visual recognition of personal identity is described, based on the failure a statistical test independence. The most unique phenotypic feature visible in person's face detailed texture each eye's iris. iris real-time video image encoded into compact sequence multi-scale quadrature 2-D Gabor wavelet coefficients, whose most-significant bits comprise 256-byte ""iris code"". Statistical decision theory generates identification decisions from Exclusive-OR comparisons complete codes at rate 4000 per second, including calculation confidence levels. distributions observed empirically such imply theoretical ""cross-over"" error one 131000 when criterion adopted that would equalize false accept and reject rates. In typical case, given mean degree code agreement, levels correspond formally to conditional probability about 10/sup 31/.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, pattern recognition, statistical independence, computer vision, visual diagnosis, biometrics, biostatistics, visual perception, statistics, image representation, image similarity, human identification, vision recognition, machine vision, face detection",1993,3211,image analysis|pattern recognition|statistical independence|computer vision|visual diagnosis|biometrics|biostatistics|visual perception|statistics|image representation|image similarity|human identification|vision recognition|machine vision|face detection,https://openalex.org/W2114212719
https://openalex.org/W2099838434,The Lens Opacities Classification System III,"The Lens Opacities Classification System III

To develop the Lens Opacities Classification System III (LOCS III) to overcome limitations inherent in lens classification using LOCS II. These include unequal intervals between standards, only one standard for color grading, use of integer and wide 95% tolerance limits.The contains an expanded set standards that were selected from Longitudinal Study Cataract slide library at Center Clinical Research, Boston, Mass. It consists six slit-lamp images grading nuclear (NC) opalescence (NO), five retroillumination cortical cataract (C), posterior subcapsular (P) cataract. severity is graded on a decimal scale, have regularly spaced scale. The limits are reduced 2.0 each class with II 0.7 opalescence, color, 0.5 cataract, 1.0 III, excellent interobserver agreement.The improved system age-related

image analysis, pattern recognition, optical image recognition, computer vision, localization, image representation, machine learning research, earth science, digital image processing",1993,2647,image analysis|pattern recognition|optical image recognition|computer vision|localization|image representation|machine learning research|earth science|digital image processing,
https://openalex.org/W2113341759,Face recognition: features versus templates,"Face recognition: features versus templates

Two new algorithms for computer recognition of human faces, one based on the computation a set geometrical features, such as nose width and length, mouth position, chin shape, second almost-gray-level template matching, are presented. The results obtained testing sets show about 90% correct using features perfect matching.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, pattern recognition, computer science, computational imaging, machine vision, image classification, object recognition, feature extraction, computer vision, machine learning, feature detection, biometrics, image representation, face recognition, machine learning research, design, feature (computer vision)",1993,2446,image analysis|pattern recognition|computer science|computational imaging|machine vision|image classification|object recognition|feature extraction|computer vision|machine learning|feature detection|biometrics|image representation|face recognition|machine learning research|design|feature (computer vision),https://openalex.org/W2159686933|https://openalex.org/W2123921160|https://openalex.org/W2106390385
https://openalex.org/W2223285751,Parts and Wholes in Face Recognition,"Parts and Wholes in Face Recognition

Are faces recognized using more holistic representations than other types of stimuli? Taking representation to mean without an internal part structure, we interpret the available evidence on this issue and then design new empirical tests. Based previous research, reasoned that if a portion object corresponds explicitly represented in hierarchical visual representation, when is presented isolation it will be identified relatively easily did not have status part. The hypothesis face recognition therefore predicts disproportionately whole as isolated part, relative parts wholes kinds stimuli. This prediction was borne out three experiments: subjects were accurate at identifying faces, object, they same isolation, even though both tested forced-choice format differed only by one In contrast, stimuli--scrambled inverted houses--did show advantage for identification recognition.

pattern recognition, computer science, visual science, face recognition, image analysis, information fusion, feature detection, cognitive science, data science, image representation, computational imaging, scene understanding, deep learning, machine vision, object recognition, feature extraction, computer vision, biometrics, image classification",1993,2079,pattern recognition|computer science|visual science|face recognition|image analysis|information fusion|feature detection|cognitive science|data science|image representation|computational imaging|scene understanding|deep learning|machine vision|object recognition|feature extraction|computer vision|biometrics|image classification,
https://openalex.org/W1646998587,"Estimation and Tracking: Principles, Techniques, and Software","Estimation and Tracking: Principles, Techniques, and Software

Brief review of linear algebra and systems brief probability theory statistics some basic concepts in estimation static dynamic with random inputs state discrete-time for Kinematic models computational aspects extensions continuous-time nonlinear adaptive manoeuvering targets problem solutions.

image analysis, computer science, measurement, estimation theory, object tracking, tracking system, computer vision, machine learning, data science, moving object tracking, statistics, analytics, state estimation, machine vision",1993,1946,image analysis|computer science|measurement|estimation theory|object tracking|tracking system|computer vision|machine learning|data science|moving object tracking|statistics|analytics|state estimation|machine vision,
https://openalex.org/W2093191240,"&lt;title&gt;QBIC project: querying images by content, using color, texture, and shape&lt;/title&gt;","&lt;title&gt;QBIC project: querying images by content, using color, texture, and shape&lt;/title&gt;

In the query by image content (QBIC) project we are studying methods to large on-line databases using images' as basis of queries. Examples use include color, texture, and shape objects regions. Potential applications medical (`Give me other images that contain a tumor with texture like this one'), photo-journalism have blue at top red bottom'), many others in art, fashion, cataloging, retailing, industry. Key issues derivation computation attributes provide useful functionality, retrieval based on similarity opposed exact match, example or user drawn image, interfaces, refinement navigation, high dimensional database indexing, automatic semi-automatic population. We currently prototype system written X/Motif C running an RS/6000 allows variety queries, test over 1000 populated from commercially available photo clip art images. paper present main algorithms for color sketch use, show results, discuss future directions.

image analysis, pattern recognition, computer science, computational imaging, content-based image retrieval, image search, image retrieval, computer vision, visualization, multimedia retrieval, visual question answering, querying images, multimedia information processing, image representation, machine vision, digital image processing, image database",1993,1793,image analysis|pattern recognition|computer science|computational imaging|content-based image retrieval|image search|image retrieval|computer vision|visualization|multimedia retrieval|visual question answering|querying images|multimedia information processing|image representation|machine vision|digital image processing|image database,
https://openalex.org/W2168977926,Texture analysis and classification with tree-structured wavelet transform,"Texture analysis and classification with tree-structured wavelet transform

A multiresolution approach based on a modified wavelet transform called the tree-structured or packets is proposed. The development of this motivated by observation that large class natural textures can be modeled as quasi-periodic signals whose dominant frequencies are located in middle frequency channels. With transform, it possible to zoom into any desired channels for further decomposition. In contrast, conventional pyramid-structured performs decomposition low-frequency progressive texture classification algorithm which not only computationally attractive but also has excellent performance developed. present method compared with several other methods.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

image analysis, pattern recognition, computer science, computational imaging, image classification, tree-structured wavelet transform, object recognition, computer vision, object categorization, wavelet, image representation, texture analysis, machine learning research, automatic classification, machine vision, digital image processing",1993,1341,image analysis|pattern recognition|computer science|computational imaging|image classification|tree-structured wavelet transform|object recognition|computer vision|object categorization|wavelet|image representation|texture analysis|machine learning research|automatic classification|machine vision|digital image processing,
https://openalex.org/W2073470731,Imaging vector fields using line integral convolution,"Imaging vector fields using line integral convolution

Imaging vector fields has applications in science, art, image processing and special effects. An effective new approach is to use linear curvilinear filtering techniques locally blur textures along a field. This builds on several previous texture generation techniques[8, 9, 11, 14, 15, 17, 23]. It is, however, unique because it local, one-dimensional independent of any predefined geometry or texture. The technique general capable imaging arbitrary two- three-dimensional fields. local nature the algorithm lends itself highly parallel efficient implementations. Furthermore, filter rendering detail very intricate Combining this with other ? like periodic motion results richly informative striking images. can also produce novel

image analysis, computational imaging, computer science, biomedical imaging, computer vision, machine learning, applied mathematics, digital imaging, novel imaging method, vector space model, deep learning, 3d imaging, line integral convolution, vector fields, machine vision, digital image processing",1993,1220,image analysis|computational imaging|computer science|biomedical imaging|computer vision|machine learning|applied mathematics|digital imaging|novel imaging method|vector space model|deep learning|3d imaging|line integral convolution|vector fields|machine vision|digital image processing,
https://openalex.org/W2130416988,The CNN universal machine: an analogic array computer,"The CNN universal machine: an analogic array computer

A new invention, the cellular neural network (CNN) universal machine and supercomputer, is presented. This first algorithmically programmable analog array computer having real-time supercomputer power on a single chip. The CNN described, emphasizing its programmability as well global distributed memory logic, high throughput via electromagnetic waves, complex cells that may be used also for simulating broad class of PDEs. Its implementation, chip, along with use multichip supercomputer. Other types hardware implementations are briefly discussed. Details algorithmic aspects type analogic (analog logic) algorithms, software (language, compiler, code, etc.) explained, brief description available workstation implementing these concepts. range applications reviewed, including neuromorphic computing, physics, chemistry, bionics.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

pattern recognition, computer science, convolutional neural network, analogic array computer, computer vision, machine learning, neural architecture search, sequential learning, cnn universal machine, computational intelligence, deep learning, machine learning research, neural network (machine learning), machine vision",1993,970,pattern recognition|computer science|convolutional neural network|analogic array computer|computer vision|machine learning|neural architecture search|sequential learning|cnn universal machine|computational intelligence|deep learning|machine learning research|neural network (machine learning)|machine vision,
https://openalex.org/W2107693148,Texture classification by wavelet packet signatures,"Texture classification by wavelet packet signatures

This correspondence introduces a new approach to characterize textures at multiple scales. The performance of wavelet packet spaces are measured in terms sensitivity and selectivity for the classification twenty-five natural textures. Both energy entropy metrics were computed each incorporated into distinct scale space representations, where (channel) reflected specific orientation sensitivity. Wavelet representations classified without error by simple two-layer network classifier. An analyzing function large regularity (D/sub 20/) was shown be slightly more efficient representation discrimination than similar with fewer vanishing moments 6/) In addition, from standard decomposition alone (17 features) provided included our study. reliability exhibited texture signatures based on packets analysis suggest that multiresolution properties such transforms beneficial accomplishing segmentation, subtle texture.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

computational imaging, pattern recognition, computer science, texture classification, wavelet packet signatures, feature extraction, computer vision, wavelet, multimedia information processing, digital image processing",1993,866,computational imaging|pattern recognition|computer science|texture classification|wavelet packet signatures|feature extraction|computer vision|wavelet|multimedia information processing|digital image processing,
https://openalex.org/W2034139177,A Practical Guide to Wavelet Analysis,"A Practical Guide to Wavelet Analysis

A practical step-by-step guide to wavelet analysis is given, with examples taken from time series of the El Niño–Southern Oscillation (ENSO). The includes a comparison windowed Fourier transform, choice an appropriate basis function, edge effects due finite-length series, and relationship between scale frequency. New statistical significance tests for power spectra are developed by deriving theoretical white red noise processes using these establish levels confidence intervals. It shown that smoothing in or can be used increase spectrum. Empirical formulas given effect on Extensions such as filtering, Hovmöller, cross-wavelet spectra, coherence described. give quantitative measure changes ENSO variance interdecadal timescales. Using new datasets extend back 1871, Niño3 sea surface temperature Southern index show significantly higher during 1880–1920 1960–90, lower 1920–60, well possible 15-yr modulation variance. Hovmöller level pressure shows significant variations 2–8-yr both longitude time.

image analysis, pattern recognition, computer science, computational imaging, speech processing, digital signal processing, computer vision, applied mathematics, multimodal signal processing, waveform analysis, wavelet, statistical signal processing, statistics, wave theory, wavelet theory, practical guide, machine vision",1998,12936,image analysis|pattern recognition|computer science|computational imaging|speech processing|digital signal processing|computer vision|applied mathematics|multimodal signal processing|waveform analysis|wavelet|statistical signal processing|statistics|wave theory|wavelet theory|practical guide|machine vision,
https://openalex.org/W2128272608,A model of saliency-based visual attention for rapid scene analysis,"A model of saliency-based visual attention for rapid scene analysis

A visual attention system, inspired by the behavior and neuronal architecture of early primate is presented. Multiscale image features are combined into a single topographical saliency map. dynamical neural network then selects attended locations in order decreasing saliency. The system breaks down complex problem scene understanding rapidly selecting, computationally efficient manner, conspicuous to be analyzed detail.

computer science, vision recognition, information fusion, scene analysis, scene interpretation, cognitive science, vision language model, data science, deep learning, pattern recognition, machine learning, computer vision, computational imaging, scene understanding, rapid scene analysis, saliency-based visual attention, machine vision, image analysis, image representation",1998,10626,computer science|vision recognition|information fusion|scene analysis|scene interpretation|cognitive science|vision language model|data science|deep learning|pattern recognition|machine learning|computer vision|computational imaging|scene understanding|rapid scene analysis|saliency-based visual attention|machine vision|image analysis|image representation,https://openalex.org/W3214102110|https://openalex.org/W3212386989|https://openalex.org/W2938260698
https://openalex.org/W2217896605,Neural network-based face detection,"Neural network-based face detection

We present a neural network-based upright frontal face detection system. A retinally connected network examines small windows of an image and decides whether each window contains face. The system arbitrates between multiple networks to improve performance over single network. straightforward procedure for aligning positive examples training. To collect negative examples, we use bootstrap algorithm, which adds false detections into the training set as progresses. This eliminates difficult task manually selecting nonface must be chosen span entire space images. Simple heuristics, such using fact that faces rarely overlap in images, can further accuracy. Comparisons with several other state-of-the-art systems are presented, showing our has comparable terms false-positive rates.

image analysis, computer science, computer vision, machine learning, feature detection, object detection, deep learning, facial recognition system, neural network (machine learning), machine vision, face detection, facial expression recognition",1998,3521,image analysis|computer science|computer vision|machine learning|feature detection|object detection|deep learning|facial recognition system|neural network (machine learning)|machine vision|face detection|facial expression recognition,https://openalex.org/W2159017231|https://openalex.org/W2123921160|https://openalex.org/W3214102110|https://openalex.org/W2106390385|https://openalex.org/W2121601095|https://openalex.org/W2102605133|https://openalex.org/W4312443924
https://openalex.org/W2129534965,Multiscale vessel enhancement filtering,"Multiscale vessel enhancement filtering

The multiscale second order local structure of an image (Hessian) is examined with the purpose developing a vessel enhancement filter. A vesselness measure obtained on basis all eigenvalues Hessian. This tested two dimensional DSA and three aortoiliac cerebral MRA data. Its clinical utility shown by simultaneous noise background suppression in maximum intensity projections volumetric displays.

image analysis, computer science, computer vision, super-resolution imaging, digital image processing, biomedical engineering, biomedical imaging, filtering technique, spatial filtering, computational imaging, vascular image, medical imaging, machine vision, multiscale modeling, image enhancement, medical image computing",1998,3315,image analysis|computer science|computer vision|super-resolution imaging|digital image processing|biomedical engineering|biomedical imaging|filtering technique|spatial filtering|computational imaging|vascular image|medical imaging|machine vision|multiscale modeling|image enhancement|medical image computing,
https://openalex.org/W2131938193,A pyramid approach to subpixel registration based on intensity,"A pyramid approach to subpixel registration based on intensity

We present an automatic subpixel registration algorithm that minimizes the mean square intensity difference between a reference and test data set, which can be either images (two-dimensional) or volumes (three-dimensional). It uses explicit spline representation of in conjunction with processing, is based on coarse-to-fine iterative strategy (pyramid approach). The minimization performed according to new variation (ML*) Marquardt-Levenberg for nonlinear least-square optimization. geometric deformation model global three-dimensional (3-D) affine transformation optionally restricted rigid-body motion (rotation translation), combined isometric scaling. also includes optional adjustment image contrast differences. obtain excellent results intramodality positron emission tomography (PET) functional magnetic resonance imaging (fMRI) data. conclude multiresolution refinement more robust than comparable single-stage method, being less likely trapped into false local optimum. In addition, our improved version faster.

image analysis, computational imaging, computer science, image formation, computer vision, applied mathematics, image reconstruction, image representation, image registration, machine vision, digital image processing",1998,2844,image analysis|computational imaging|computer science|image formation|computer vision|applied mathematics|image reconstruction|image representation|image registration|machine vision|digital image processing,
https://openalex.org/W2127717018,Fingerprint image enhancement: algorithm and performance evaluation,"Fingerprint image enhancement: algorithm and performance evaluation

In order to ensure that the performance of an automatic fingerprint identification/verification system will be robust with respect quality input images, it is essential incorporate a enhancement algorithm in minutiae extraction module. We present fast algorithm, which can adaptively improve clarity ridge and valley structures images based on estimated local orientation frequency. have evaluated image using goodness index extracted accuracy online verification system. Experimental results show incorporating improves both accuracy.

image analysis, pattern recognition, computer science, computational imaging, fingerprint image enhancement, biomedical imaging, feature extraction, automatic identification, computer vision, image enhancement, feature detection, human identification, digital image processing",1998,2091,image analysis|pattern recognition|computer science|computational imaging|fingerprint image enhancement|biomedical imaging|feature extraction|automatic identification|computer vision|image enhancement|feature detection|human identification|digital image processing,
https://openalex.org/W585975565,"Two-Dimensional Phase Unwrapping: Theory, Algorithms, and Software","Two-Dimensional Phase Unwrapping: Theory, Algorithms, and Software

Introduction to Phase Unwrapping. Line Integrals, Residues, and 2-D Data, Quality Maps, Masks, Filters. Path-Following Methods. Minimum-Norm Comparisons Conclusion. Appendices. Index.

image analysis, computational imaging, computer science, two-dimensional phase unwrapping, phase retrieval, precision engineering, computer graphic, computer vision, applied mathematics, reflection removal, data science, deep learning, surface reconstruction, texture analysis, interferometry, machine vision, digital image processing, computational geometry",1998,1986,image analysis|computational imaging|computer science|two-dimensional phase unwrapping|phase retrieval|precision engineering|computer graphic|computer vision|applied mathematics|reflection removal|data science|deep learning|surface reconstruction|texture analysis|interferometry|machine vision|digital image processing|computational geometry,
https://openalex.org/W1555614281,Introductory Techniques for 3-D Computer Vision,"Introductory Techniques for 3-D Computer Vision

From the Publisher:
FEATURES:


Provides a guide to well-tested theory and algorithms including solutions of problems encountered in modern computer vision.
Contains many practical hints highlighted book.
Develops two parallel tracks presentation, showing how fundamental are solved using both intensity range images, most popular types images used today.
Each chapter contains notes on literature, review questions, numerical exercises, projects.
Provides an Internet list for accessing links test demos, archives additional learning material.

computer science, 3d vision, introductory techniques, multi-view geometry, computer vision, 3d imaging, 3d object recognition, machine vision, 3d computer vision",1998,1950,computer science|3d vision|introductory techniques|multi-view geometry|computer vision|3d imaging|3d object recognition|machine vision|3d computer vision,
https://openalex.org/W2159686933,Example-based learning for view-based human face detection,"Example-based learning for view-based human face detection

We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution face patterns by means a few view-based ""face"" and ""nonface"" model clusters. At each image location, difference feature vector is computed between local pattern distribution-based model. A trained classifier determines, based on measurements, whether or not exists at current location. show empirically that distance metric we adopt computing vectors, clusters include our model, are both critical success system.

image analysis, pattern recognition, computer science, human-computer interaction, explanation-based learning, computer vision, machine learning, instance-based learning, machine vision, face detection, example-based learning",1998,1778,image analysis|pattern recognition|computer science|human-computer interaction|explanation-based learning|computer vision|machine learning|instance-based learning|machine vision|face detection|example-based learning,https://openalex.org/W2159686933|https://openalex.org/W2159017231|https://openalex.org/W2123921160|https://openalex.org/W2121601095|https://openalex.org/W2124351082|https://openalex.org/W2102605133|https://openalex.org/W3018757597
https://openalex.org/W2104612543,"Automated Image Registration: I. General Methods and Intrasubject, Intramodality Validation","Automated Image Registration: I. General Methods and Intrasubject, Intramodality Validation

Purpose We sought to describe and validate an automated image registration method(AIR 3.0) based on matching of voxel intensities. Method Different cost functions, different minimization methods, various sampling, smoothing, editing strategies were compared. Internal consistency measures used place limits accuracy for MRI data, absolute was measured using a brain phantom PET data. Results All consistent with subvoxel intrasubject, intramodality registration. Estimated structural images in the 75 150 μm range. Sparse data sampling reduced times minutes only modest loss accuracy. Conclusion The algorithm described is robust flexible tool that can be address variety problems. Registration tailored meet needs by optimizing tradeoffs between speed

computational imaging, computer science, medical imaging, intramodality validation, computer vision, image registration, digital image processing",1998,1727,computational imaging|computer science|medical imaging|intramodality validation|computer vision|image registration|digital image processing,
https://openalex.org/W2107636931,GTM: The Generative Topographic Mapping,"GTM: The Generative Topographic Mapping

Latent variable models represent the probability density of data in a space several dimensions terms smaller number latent, or hidden, variables. A familiar example is factor analysis, which based on linear transformation between latent and space. In this article, we introduce form nonlinear model called generative topographic mapping, for parameters can be determined using expectation-maximization algorithm. GTM provides principled alternative to widely used self-organizing map (SOM) Kohonen (1982) overcomes most significant limitations SOM. We demonstrate performance algorithm toy problem simulated from flow diagnostics multiphase oil pipeline.

pattern recognition, computer science, generative adversarial network, machine learning, point cloud, geographical information science, image representation, computational imaging, generative model, generative topographic mapping, deep learning, machine learning research, earth science, computer-generated imagery, machine vision, digital image processing, computer vision, geography, cover mapping",1998,1349,pattern recognition|computer science|generative adversarial network|machine learning|point cloud|geographical information science|image representation|computational imaging|generative model|generative topographic mapping|deep learning|machine learning research|earth science|computer-generated imagery|machine vision|digital image processing|computer vision|geography|cover mapping,
https://openalex.org/W2114212719,A human identification technique using images of the iris and wavelet transform,"A human identification technique using images of the iris and wavelet transform

A new approach for recognizing the iris of human eye is presented. Zero-crossings wavelet transform at various resolution levels are calculated over concentric circles on iris, and resulting one-dimensional (1-D) signals compared with model features using different dissimilarity functions.

pattern recognition, computer science, wavelet transform, image analysis, biomedical imaging, neuroscience, feature detection, image representation, computational imaging, hit identification, machine vision, digital image processing, face detection, human identification technique, medical image computing, computer vision, biometrics, iris biometrics, human identification",1998,1142,pattern recognition|computer science|wavelet transform|image analysis|biomedical imaging|neuroscience|feature detection|image representation|computational imaging|hit identification|machine vision|digital image processing|face detection|human identification technique|medical image computing|computer vision|biometrics|iris biometrics|human identification,
https://openalex.org/W2103913786,Total variation blind deconvolution,"Total variation blind deconvolution

In this paper, we present a blind deconvolution algorithm based on the total variational (TV) minimization method proposed. The motivation for regularizing with TV norm is that it extremely effective recovering edges of images as well some blurring functions, e.g., motion blur and out-of-focus blur. An alternating (AM)implicit iterative scheme devised to recover image simultaneously identify point spread function (psf). Numerical results indicate quite robust, converges very fast (especially discontinuous blur), both psf can be recovered under presence high noise level. Finally, remark psf's without sharp edges, Gaussian blur, also identified through approach.

computational imaging, total variation, digital signal processing, deconvolution, computer vision, digital subtraction angiography, applied mathematics, image restoration, deblurring, machine vision, digital image processing, signal reconstruction",1998,1139,computational imaging|total variation|digital signal processing|deconvolution|computer vision|digital subtraction angiography|applied mathematics|image restoration|deblurring|machine vision|digital image processing|signal reconstruction,https://openalex.org/W2103913786
https://openalex.org/W2028687412,Layered depth images,"Layered depth images

Article Free Access Share on Layered depth images Authors: Jonathan Shade Univ. of Washington, Seattle SeattleView Profile , Steven Gortler Harvard Univ., MA MAView Li-wei He Stanford Stanford, CA CAView Richard Szeliski Microsoft Research ResearchView Authors Info & Claims SIGGRAPH '98: Proceedings the 25th annual conference Computer graphics and interactive techniquesJuly 1998 Pages 231–242https://doi.org/10.1145/280814.280882Published:24 July 1998Publication History 691citation4,644DownloadsMetricsTotal Citations691Total Downloads4,644Last 12 Months478Last 6 weeks65 Get Citation AlertsNew Alert added!This alert has been successfully added will be sent to:You notified whenever a record that you have chosen cited.To manage your preferences, click button below.Manage my Alert!Please log in to account Save BinderSave BinderCreate New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF

image analysis, pattern recognition, computer science, computational imaging, depth images, biomedical imaging, computer graphic, computer vision, underwater imaging, motion detection, depth map, image representation, earth science, remote sensing, shift detection, machine vision, digital image processing, computer stereo vision",1998,1071,image analysis|pattern recognition|computer science|computational imaging|depth images|biomedical imaging|computer graphic|computer vision|underwater imaging|motion detection|depth map|image representation|earth science|remote sensing|shift detection|machine vision|digital image processing|computer stereo vision,
https://openalex.org/W2121947440,Normalized cuts and image segmentation,"Normalized cuts and image segmentation

We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies image data, our aims at extracting global impression of an image. treat segmentation as graph partitioning criterion, normalized cut, segmenting graph. The cut criterion measures both total dissimilarity between different groups well similarity within groups. show that efficient computational technique based generalized eigenvalue can be used to optimize this criterion. applied static images, motion sequences, found results very encouraging.

computer science, computer vision, image segmentation, data normalization",2000,14761,computer science|computer vision|image segmentation|data normalization,https://openalex.org/W2104019579|https://openalex.org/W2152161678
https://openalex.org/W2167667767,A flexible new technique for camera calibration,"A flexible new technique for camera calibration

We propose a flexible technique to easily calibrate camera. It only requires the camera observe planar pattern shown at few (at least two) different orientations. Either or can be freely moved. The motion need not known. Radial lens distortion is modeled. proposed procedure consists of closed-form solution, followed by nonlinear refinement based on maximum likelihood criterion. Both computer simulation and real data have been used test very good results obtained. Compared with classical techniques which use expensive equipment such as two three orthogonal planes, easy flexible. advances 3D vision one more step from laboratory environments world use.

computer science, calibration, computer vision, camera calibration",2000,12438,computer science|calibration|computer vision|camera calibration,https://openalex.org/W2167667767
https://openalex.org/W2033419168,The FERET evaluation methodology for face-recognition algorithms,"The FERET evaluation methodology for face-recognition algorithms

Two of the most critical requirements in support producing reliable face-recognition systems are a large database facial images and testing procedure to evaluate systems. The Face Recognition Technology (FERET) program has addressed both issues through FERET establishment tests. To date, 14,126 from 1,199 individuals included database, which is divided into development sequestered portions database. In September 1996, administered third series primary objectives test were 1) assess state art, 2) identify future areas research, 3) measure algorithm performance.

image analysis, pattern recognition, computer science, information fusion, feret evaluation methodology, computer vision, machine learning, feature detection, biometrics, cognitive science, identification method, data science, image representation, face-recognition algorithms, facial recognition system, human identification, machine vision, face detection",2000,4608,image analysis|pattern recognition|computer science|information fusion|feret evaluation methodology|computer vision|machine learning|feature detection|biometrics|cognitive science|identification method|data science|image representation|face-recognition algorithms|facial recognition system|human identification|machine vision|face detection,https://openalex.org/W2123921160|https://openalex.org/W2121601095
https://openalex.org/W2112737587,The Cricket location-support system,"The Cricket location-support system

This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, location-dependent applications. It allows applications running on mobile static nodes to learn their physical location by using listeners that hear analyze information from beacons spread throughout building. Cricket is result several design goals, including user privacy, decentralized administration, network heterogeneity, low cost. Rather than explicitly tracking location, helps devices where they are lets them decide whom advertise this to; it does not rely any centralized management or control there no explicit coordination between beacons; provides regardless type connectivity; each device made off-the-shelf components costs less U.S. $10. We describe randomized algorithm used transmit information, use concurrent radio ultrasonic signals infer distance, listener inference algorithms overcome multipath interference, practical beacon configuration positioning techniques improve accuracy. Our experience with shows such as in-building active maps can be developed little effort manual configuration.

computer science, technology, applied geography, dynamic positioning, geographic information system, wireless sensor network, mobile computing, information fusion, mobile positioning data, real-time monitoring, unmanned aerial vehicle, drone, cricket location-support system, localization, location tracking, wireless communication, computer vision, geography, positioning system",2000,3738,computer science|technology|applied geography|dynamic positioning|geographic information system|wireless sensor network|mobile computing|information fusion|mobile positioning data|real-time monitoring|unmanned aerial vehicle|drone|cricket location-support system|localization|location tracking|wireless communication|computer vision|geography|positioning system,
https://openalex.org/W1561442812,Morphological Image Analysis: Principles and Applications,"Morphological Image Analysis: Principles and Applications

From the Publisher:
The purpose of this book is to provide readers with an in-depth presentation principles and applications morphological image analysis. This achieved through a step by process starting from basic operators extending most recent advances which have proven their practical usefulness. self-contained volume will be valuable all engineers, scientists, practitioners interested in analysis processing digital images.

image analysis, computational imaging, computer science, morphology, bioimage analysis, morphological image analysis, computer vision, morphology (biology), image representation, texture analysis, shape analysis, morphological evidence, digital image processing, morphological analysis",2000,3548,image analysis|computational imaging|computer science|morphology|bioimage analysis|morphological image analysis|computer vision|morphology (biology)|image representation|texture analysis|shape analysis|morphological evidence|digital image processing|morphological analysis,
https://openalex.org/W2295936755,Image inpainting,"Image inpainting

Inpainting, the technique of modifying an image in undetectable form, is as ancient art itself. The goals and applications inpainting are numerous, from restoration damaged paintings photographs to removal/replacement selected objects. In this paper, we introduce a novel algorithm for digital still images that attempts replicate basic techniques used by professional restorators. After user selects regions be restored, automatically fills-in these with information surrounding them. fill-in done such way isophote lines arriving at regions' boundaries completed inside. contrast previous approaches, here introduced does not require specify where comes from. This (and fast way), thereby allowing simultaneously numerous containing completely different structures backgrounds. addition, no limitations imposed on topology region inpainted. Applications include old film; removal superimposed text like dates, subtitles, or publicity; entire objects microphones wires special effects.

image analysis, computational imaging, image formation, image communication, image restoration, computer vision, image inpainting, art, image manipulation, image representation, visual science, art history, image denoising, digital image processing, inpainting",2000,3358,image analysis|computational imaging|image formation|image communication|image restoration|computer vision|image inpainting|art|image manipulation|image representation|visual science|art history|image denoising|digital image processing|inpainting,https://openalex.org/W2536599074|https://openalex.org/W2963420272|https://openalex.org/W3212516020
https://openalex.org/W2142069714,Online and off-line handwriting recognition: a comprehensive survey,"Online and off-line handwriting recognition: a comprehensive survey

Handwriting has continued to persist as a means of communication and recording information in day-to-day life even with the introduction new technologies. Given its ubiquity human transactions, machine recognition handwriting practical significance, reading handwritten notes PDA, postal addresses on envelopes, amounts bank checks, fields forms, etc. This overview describes nature language, how it is transduced into electronic data, basic concepts behind written language algorithms. Both online case (which pertains availability trajectory data during writing) off-line scanned images) are considered. Algorithms for preprocessing, character word recognition, performance systems indicated. Other application, like signature verification, writer authentification, learning tools also

image analysis, pattern recognition, computer science, information fusion, feature learning, character recognition, text recognition, computer vision, machine learning, data science, deep learning, image representation, machine learning research, off-line handwriting recognition, machine vision, writer identification, handwriting, computer engineering",2000,2446,image analysis|pattern recognition|computer science|information fusion|feature learning|character recognition|text recognition|computer vision|machine learning|data science|deep learning|image representation|machine learning research|off-line handwriting recognition|machine vision|writer identification|handwriting|computer engineering,
https://openalex.org/W1866095408,The visual word form area,"The visual word form area

A standard model of word reading postulates that visual information is initially processed by occipitotemporal areas contralateral to the stimulated hemifield, from whence it subsequently transferred form (VWF) system, a left inferior temporal region specifically devoted processing letter strings. For stimuli displayed in field, this transfer proceeds right hemisphere through posterior portion corpus callosum. In order characterize spatial and organization these processes, tasks with split-field presentation were performed five control subjects two patients suffering hemialexia following callosal lesions. The subjects' responses studied using behavioural measures functional brain imaging techniques, providing both high resolution (functional MRI, fMRI) (high-density event-related potentials, ERPs). Early was revealed as activations stimulation, located fMRI presumably coincident area V4. negative wave occurring 150-160 ms post-stimulus, also strictly recorded over electrodes. contrast hemifield-dependent effects, VWF system left-hemispheric activation which, subjects, identical for presented or hemifield middle fusiform gyrus. electrical signature consisted unilateral sharp negativity, 180-200 post-stimulus patients, due inability pass across part callosum, activated only field. Similarly, significant influence word/non-word status on ERPs discernible either controls, while affected right-hemifield patients. These findings provide direct support main components classical help specify their timing cerebral substrates.

image analysis, pattern recognition, computer science, linguistics, information visualization, visual language, form finding, image communication, language, image formation, visual communication, visual reasoning, computer vision, visualization, visual perception, image representation, visual science, machine vision",2000,1822,image analysis|pattern recognition|computer science|linguistics|information visualization|visual language|form finding|image communication|language|image formation|visual communication|visual reasoning|computer vision|visualization|visual perception|image representation|visual science|machine vision,
https://openalex.org/W2159017231,Automatic analysis of facial expressions: the state of the art,"Automatic analysis of facial expressions: the state of the art

Humans detect and interpret faces facial expressions in a scene with little or no effort. Still, development of an automated system that accomplishes this task is rather difficult. There are several related problems: detection image segment as face, extraction the expression information, classification (e.g., emotion categories). A performs these operations accurately real time would form big step achieving human-like interaction between man machine. The paper surveys past work solving problems. capability human visual respect to problems discussed, too. It meant serve ultimate goal guide for determining recommendations automatic analyzer.

computer science, facial expressions, computer vision, automatic analysis, facial expression recognition",2000,1729,computer science|facial expressions|computer vision|automatic analysis|facial expression recognition,https://openalex.org/W2799041689
https://openalex.org/W1965305672,Identifying fixations and saccades in eye-tracking protocols,"Identifying fixations and saccades in eye-tracking protocols

The process of fixation identification—separating and labeling fixations saccades in eye-tracking protocols—is an essential part eye-movement data analysis can have a dramatic impact on higher-level analyses. However, algorithms for performing identification are often described informally rarely compared meaningful way. In this paper we propose taxonomy that classifies terms how they utilize spatial temporal information protocols. Using taxonomy, describe five representative different classes the based commonly employed techniques. We then evaluate compare these with respect to number qualitative characteristics. results comparisons offer interesting implications use various future work.

computer science, human-computer interaction, eye tracking, eye-tracking protocols, computer vision, deception detection",2000,1695,computer science|human-computer interaction|eye tracking|eye-tracking protocols|computer vision|deception detection,
https://openalex.org/W2145251738,A new diamond search algorithm for fast block-matching motion estimation,"A new diamond search algorithm for fast block-matching motion estimation

Based on the study of motion vector distribution from several commonly used test image sequences, a new diamond search (DS) algorithm for fast block-matching estimation (BMME) is proposed in this paper. Simulation results demonstrate that DS greatly outperforms well-known three-step (TSS) algorithm. Compared with (NTSS) algorithm, achieves close performance but requires less computation by up to 22% average. Experimental also show better than four-step (4SS) and block-based gradient descent (BBGDS), terms mean-square error required number points.

image analysis, pattern recognition, computer science, information fusion, motion analysis, object tracking, computer vision, machine learning, applied mathematics, graph matching, motion detection, data science, moving object tracking, machine vision, digital image processing",2000,1675,image analysis|pattern recognition|computer science|information fusion|motion analysis|object tracking|computer vision|machine learning|applied mathematics|graph matching|motion detection|data science|moving object tracking|machine vision|digital image processing,
https://openalex.org/W2115213191,A Bayesian computer vision system for modeling human interactions,"A Bayesian computer vision system for modeling human interactions

We describe a real-time computer vision and machine learning system for modeling recognizing human behaviors in visual surveillance task. The deals particularly with detecting when interactions between people occur classifying the type of interaction. Examples interesting interaction include following another person, altering one's path to meet another, so forth. Our combines top-down bottom-up information closed feedback loop, both components employing statistical Bayesian approach. propose compare two different state-based architectures, namely, HMMs CHMMs interactions. Finally, synthetic ""Alife-style"" training is used develop flexible prior models demonstrate ability use these priori accurately classify real no additional tuning or training.

computer science, human-computer interaction, computer vision, bayesian analysis, bayesian network",2000,1605,computer science|human-computer interaction|computer vision|bayesian analysis|bayesian network,https://openalex.org/W2115213191
https://openalex.org/W2124156864,The Unscented Particle Filter,"The Unscented Particle Filter

In this paper, we propose a new particle filter based on sequential importance sampling. The algorithm uses bank of unscented filters to obtain the proposal distribution. This has two very nice properties. Firstly, it makes efficient use latest available information and, secondly, can have heavy tails. As result, find that outperforms standard filtering and other nonlinear methods substantially. experimental finding is in agreement with theoretical convergence proof for algorithm. also includes resampling (possibly) Markov chain Monte Carlo (MCMC) steps.

image analysis, computational imaging, computer science, particle method, spatial filtering, computer vision, biometrics, aerosol science, motion detection, data science, source separation, deep learning, unscented particle filter, applied physics, shift detection, machine vision, digital image processing",2000,1433,image analysis|computational imaging|computer science|particle method|spatial filtering|computer vision|biometrics|aerosol science|motion detection|data science|source separation|deep learning|unscented particle filter|applied physics|shift detection|machine vision|digital image processing,https://openalex.org/W2160337655
https://openalex.org/W2116040950,Active contours without edges,"Active contours without edges

We propose a new model for active contours to detect objects in given image, based on techniques of curve evolution, Mumford-Shah (1989) functional segmentation and level sets. Our can whose boundaries are not necessarily defined by the gradient. minimize an energy which be seen as particular case minimal partition problem. In set formulation, problem becomes ""mean-curvature flow""-like evolving contour, will stop desired boundary. However, stopping term does depend gradient classical contour models, but is instead related image. give numerical algorithm using finite differences. Finally, we present various experimental results some examples snakes methods applicable. Also, initial anywhere interior automatically detected.

image analysis, computational imaging, computer science, edge detection, computer vision, geometry processing, image segmentation, active contours, digital image processing, inpainting, computational geometry",2001,9906,image analysis|computational imaging|computer science|edge detection|computer vision|geometry processing|image segmentation|active contours|digital image processing|inpainting|computational geometry,https://openalex.org/W3132455321
https://openalex.org/W2123921160,From few to many: illumination cone models for face recognition under variable lighting and pose,"From few to many: illumination cone models for face recognition under variable lighting and pose

We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our exploits the fact that set of images an object fixed pose, but all possible illumination conditions, is convex cone space images. Using small number training each face taken with different directions, shape albedo can be reconstructed. In turn, this reconstruction serves as model used to render (or synthesize) novel poses conditions. The pose then sampled and, corresponding approximated by low-dimensional linear subspace whose basis vectors are estimated using model. recognition algorithm assigns test image identity closest cone. Test results show performs almost without error, except on most extreme directions.

image analysis, pattern recognition, computer science, object recognition, computer vision, machine learning, face recognition, illumination modeling, variable lighting, illumination cone models, machine vision",2001,4777,image analysis|pattern recognition|computer science|object recognition|computer vision|machine learning|face recognition|illumination modeling|variable lighting|illumination cone models|machine vision,https://openalex.org/W2129812935
https://openalex.org/W2129112648,Color transfer between images,"Color transfer between images

We use a simple statistical analysis to impose one image's color characteristics on another. can achieve correction by choosing an appropriate source image and apply its characteristic another image.

image analysis, computational imaging, computer science, image communication, color transfer, computer vision, novel imaging method, image manipulation, image representation, image transmission, image similarity, digital image processing",2001,2570,image analysis|computational imaging|computer science|image communication|color transfer|computer vision|novel imaging method|image manipulation|image representation|image transmission|image similarity|digital image processing,https://openalex.org/W2963073614
https://openalex.org/W1999360130,Image quilting for texture synthesis and transfer,"Image quilting for texture synthesis and transfer

We present a simple image-based method of generating novel visual appearance in which new image is synthesized by stitching together small patches existing images. call this process quilting. First, we use quilting as fast and very texture synthesis algorithm produces surprisingly good results for wide range textures. Second, extend the to perform transfer — rendering an object with taken from different object. More generally, demonstrate how can be re-rendered style image. The works directly on images does not require 3D information.

image analysis, computational imaging, computer science, image stitching, synthetic image generation, computer graphic, computer vision, image reconstruction, image representation, texture analysis, image denoising, digital image processing, texture synthesis",2001,2311,image analysis|computational imaging|computer science|image stitching|synthetic image generation|computer graphic|computer vision|image reconstruction|image representation|texture analysis|image denoising|digital image processing|texture synthesis,https://openalex.org/W2292976057|https://openalex.org/W2475287302|https://openalex.org/W2963073614
https://openalex.org/W3214102110,Robust real-time object detection.,"Robust real-time object detection.

This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high rates. There are three key contributions. The first the introduction new image representation called “Integral Image” which allows features used by our detector to be computed very quickly. second learning algorithm, based on AdaBoost, selects small number critical and yields efficient classifiers [4]. third contribution method for combining in “cascade” background regions quickly discarded spending more computation promising object-like regions. A set experiments domain face presented. system performance comparable best previous systems [16, 11, 14, 10, 1]. Implemented conventional desktop, proceeds at 15 frames per second. Author email: fPaul.Viola,Mike.J.Jonesg@compaq.com c Compaq Computer Corporation, 2001 work may not copied or reproduced whole part any commercial purpose. Permission copy without payment fee granted nonprofit educational research purposes provided all such partial copies include following: notice copying permission Cambridge Research Laboratory Corporation Cambridge, Massachusetts; an acknowledgment authors individual contributors work; applicable portions copyright notice. Copying, reproducing, republishing other purpose shall require license with Laboratory. All rights reserved. CRL Technical reports available CRL’s web page http://crl.research.compaq.com. One Center Massachusetts 02142 USA

image analysis, pattern recognition, computer science, object tracking, object recognition, robust feature, computer vision, machine learning, object detection, data science, detection technique, deep learning, machine vision",2001,2218,image analysis|pattern recognition|computer science|object tracking|object recognition|robust feature|computer vision|machine learning|object detection|data science|detection technique|deep learning|machine vision,https://openalex.org/W2548197316|https://openalex.org/W2963037989
https://openalex.org/W2611684114,Computer Vision,"Computer Vision

From the Publisher:
Computer Vision presents necessary theory and techniques for students practitioners who will work in fields where significant information must be extracted automatically from images. It a useful resource book professionals core text both undergraduate beginning graduate computer vision imaging courses.

Features


Topics include image databases an virtual augmented reality addition to classical topics.
Offers complete view of two real-world systems that use vision.
Contains applications industry, medicine, land use, multimedia, graphics.
Includes over 250 exercises programming projects, 48 separately defined algorithms, 360 figures.
The companion website features archive, sample

image analysis, computer science, computer vision, image representation, machine vision, digital image processing, 3d computer vision",2001,1792,image analysis|computer science|computer vision|image representation|machine vision|digital image processing|3d computer vision,
https://openalex.org/W1977758817,Reconstruction and representation of 3D objects with radial basis functions,"Reconstruction and representation of 3D objects with radial basis functions

We use polyharmonic Radial Basis Functions (RBFs) to reconstruct smooth, manifold surfaces from point-cloud data and repair incomplete meshes. An object's surface is defined implicitly as the zero set of an RBF fitted given data. Fast methods for fitting evaluating RBFs allow us model large sets, consisting millions points, by a single — previously impossible task. A greedy algorithm in process reduces number centers required represent results significant compression further computational advantages. The energy-minimisation characterisation splines result ""smoothest"" interpolant. This scale-independent well-suited reconstructing non-uniformly sampled Holes are smoothly filled extrapolated. non-interpolating approximation when noisy. functional representation effect solid model, which means that gradients normals can be determined analytically. helps generate uniform meshes we show has advantages mesh simplification remeshing applications. Results presented real-world rangefinder

computational imaging, computer science, geometric modeling, radial basis function, 3d computer vision, computer vision, geometry, applied mathematics, 3d modeling, 3d reconstruction, computer-aided design, planetary sciences, additive manufacturing, 3d object recognition, numerical simulation, machine vision, digital image processing, computational geometry",2001,1767,computational imaging|computer science|geometric modeling|radial basis function|3d computer vision|computer vision|geometry|applied mathematics|3d modeling|3d reconstruction|computer-aided design|planetary sciences|additive manufacturing|3d object recognition|numerical simulation|machine vision|digital image processing|computational geometry,
https://openalex.org/W2106390385,Recognizing action units for facial expression analysis,"Recognizing action units for facial expression analysis

Most automatic expression analysis systems attempt to recognize a small set of prototypic expressions, such as happiness, anger, surprise, and fear. Such however, occur rather infrequently. Human emotions intentions are more often communicated by changes in one or few discrete facial features. In this paper, we develop an Automatic Face Analysis (AFA) system analyze expressions based on both permanent features (brows, eyes, mouth) transient (deepening furrows) nearly frontal-view face image sequence. The AFA recognizes fine-grained into action units (AUs) the Facial Action Coding System (FACS), instead expressions. Multistate component models proposed for tracking modeling various features, including lips, brows, cheeks, furrows. During tracking, detailed parametric descriptions extracted. With these parameters inputs, group (neutral expression, six upper AUs 10 lower AUs) recognized whether they alone combinations. has achieved average recognition rates 96.4 percent (95.4 if neutral excluded) 96.7 (95.6 with AUs. generalizability been tested using independent databases collected FACS-coded ground-truth different research teams.

pattern recognition, computer science, computer vision, machine learning, feature detection, action units, facial expression analysis, emotion recognition, facial expression recognition",2001,1621,pattern recognition|computer science|computer vision|machine learning|feature detection|action units|facial expression analysis|emotion recognition|facial expression recognition,https://openalex.org/W2799041689
https://openalex.org/W2150969685,The development of the CIE 2000 colour‐difference formula: CIEDE2000,"The development of the CIE 2000 colour‐difference formula: CIEDE2000

Abstract A colour‐difference equation based on CIELAB is developed. It includes not only lightness, chroma, and hue weighting functions, but also an interactive term between chroma differences for improving the performance blue colours a scaling factor * scale gray colours. Four reliable colour discrimination datasets upon object were accumulated combined. The was tested together with other advanced equations using combined dataset each individual dataset. outperformed CMC CIE94 by large margin, predicted better than BFD LCD. has been officially adopted as new CIE equation. © 2001 John Wiley &amp; Sons, Inc. Col Res Appl, 26, 340–350,

computer science, machine learning, screen medium, colorization, color constancy, image analysis, avian biology, colour-difference formula, colorimetry, color reproduction, difference equation, computational imaging, approximation theory, earth science, machine vision, optical properties, color correction, computer vision, applied mathematics",2001,1556,computer science|machine learning|screen medium|colorization|color constancy|image analysis|avian biology|colour-difference formula|colorimetry|color reproduction|difference equation|computational imaging|approximation theory|earth science|machine vision|optical properties|color correction|computer vision|applied mathematics,
https://openalex.org/W2292976057,Image analogies,"Image analogies

This paper describes a new framework for processing images by example, called “image analogies.” The involves two stages: design phase, in which pair of images, with one image purported to be “filtered” version the other, is presented as “training data”; and an application learned filter applied some target order create “analogous” filtered result. Image analogies are based on simple multi-scale autoregression, inspired primarily recent results texture synthesis. By choosing different types source pairs input, supports wide variety filter” effects, including traditional filters, such blurring or embossing; improved synthesis, textures synthesized higher quality than previous approaches; super-resolution, higher-resolution inferred from low-resolution source; transfer, “texturized” arbitrary texture; artistic various drawing painting styles scanned real-world examples; texture-by-numbers, realistic scenes, composed textures, created using interface.

image analogies, computer vision, art, image representation, image similarity, digital image processing",2001,1534,image analogies|computer vision|art|image representation|image similarity|digital image processing,https://openalex.org/W1999360130|https://openalex.org/W2475287302|https://openalex.org/W2962793481|https://openalex.org/W2963073614
https://openalex.org/W1587477963,Curves and surfaces for CAGD : a practical guide,"Curves and surfaces for CAGD : a practical guide

This fifth edition has been fully updated to cover the many advances made in CAGD and curve surface theory since 1997, when fourth appeared. The material streamlined using blossoming approach; applications includes least squares techniques addition traditional interpolation methods. In all other respects, it is, thankfully, same. means you get informal, friendly style unique approach that Curves Surfaces for CAGD: A Practical Guide a true classic.

image analysis, computational imaging, computer science, machine vision, curve fitting, computer vision, curve modeling, deep learning, surface modeling, practical guide, calibration",2001,1437,image analysis|computational imaging|computer science|machine vision|curve fitting|computer vision|curve modeling|deep learning|surface modeling|practical guide|calibration,
https://openalex.org/W2104019579,Unsupervised segmentation of color-texture regions in images and video,"Unsupervised segmentation of color-texture regions in images and video

A method for unsupervised segmentation of color-texture regions in images and video is presented. This method, which we refer to as JSEG, consists two independent steps: color quantization spatial segmentation. In the first step, colors image are quantized several representative classes that can be used differentiate image. The pixels then replaced by their corresponding class labels, thus forming a class-map focus this work on segmentation, where criterion ""good"" using proposed. Applying local windows results ""J-image,"" high low values correspond possible boundaries interiors regions. region growing segment based multiscale J-images. similar approach applied sequences. An additional tracking scheme embedded into process achieve consistent results, even scenes with nonrigid object motion. Experiments show robustness JSEG algorithm real video.

computer science, machine learning, unsupervised segmentation, image analysis, video understanding, cognitive science, image representation, computational imaging, scene understanding, unsupervised machine learning, multimedia information processing, deep learning, machine learning research, machine vision, digital image processing, color-texture regions, computer vision, scene analysis, image sequence analysis",2001,1381,computer science|machine learning|unsupervised segmentation|image analysis|video understanding|cognitive science|image representation|computational imaging|scene understanding|unsupervised machine learning|multimedia information processing|deep learning|machine learning research|machine vision|digital image processing|color-texture regions|computer vision|scene analysis|image sequence analysis,
https://openalex.org/W1550219794,Introduction to modern photogrammetry,"Introduction to modern photogrammetry

This book is designed to give the reader a strong grounding in mathematical basis of photogrammetry while introducing them related fields, such as remote sensing and digital image processing, which are increasingly important photogrammetric research practice.

image analysis, pattern recognition, computer science, computational imaging, optical metrology, biomedical imaging, computer graphic, multi-view geometry, computer vision, biometrics, digital image correlation, data science, image representation, planetary sciences, photometric stereo, machine vision, digital image processing, digital photogrammetry",2001,1001,image analysis|pattern recognition|computer science|computational imaging|optical metrology|biomedical imaging|computer graphic|multi-view geometry|computer vision|biometrics|digital image correlation|data science|image representation|planetary sciences|photometric stereo|machine vision|digital image processing|digital photogrammetry,
https://openalex.org/W2163352848,Multiresolution gray-scale and rotation invariant texture classification with local binary patterns,"Multiresolution gray-scale and rotation invariant texture classification with local binary patterns

Presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns nonparametric discrimination of sample prototype distributions. The method is recognizing that certain patterns, termed ""uniform,"" are fundamental properties image their occurrence histogram proven be powerful feature. We derive generalized operator presentation allows for detecting the ""uniform"" any quantization angular space spatial resolution presents combining multiple operators analysis. proposed robust in terms variations since is, by definition, against monotonic transformation gray scale. Another advantage computational simplicity as can realized with few operations small neighborhood lookup table. Experimental results demonstrate good achieved statistics simple patterns.

image analysis, pattern recognition, computer science, image classification, hierarchical classification, computer vision, local binary patterns, multiresolution gray-scale, object categorization, deep learning, texture analysis, machine vision, digital image processing",2002,14282,image analysis|pattern recognition|computer science|image classification|hierarchical classification|computer vision|local binary patterns|multiresolution gray-scale|object categorization|deep learning|texture analysis|machine vision|digital image processing,
https://openalex.org/W2160337655,A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking,"A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking

Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order model accurately the underlying dynamics a physical system. Moreover, typically crucial process data on-line as arrives, both from point view storage costs well rapid adaptation changing signal characteristics. In this paper, we review optimal suboptimal Bayesian algorithms nonlinear/non-Gaussian tracking problems, with focus on particle filters. Particle filters are sequential Monte Carlo methods based mass (or ""particle"") representations probability densities, which can be applied any state-space generalize traditional Kalman filtering methods. Several variants filter such SIR, ASIR, RPF introduced within generic framework importance sampling (SIS) algorithm. These discussed compared standard EKF through an illustrative example.

image analysis, shift detection, computer science, nonlinear system identification, nonlinear system, object tracking, parameter identification, particle filters, computer vision, machine learning, applied mathematics, bayesian analysis, moving object tracking, statistics, nonlinear dynamic, machine vision, nonlinear science",2002,10947,image analysis|shift detection|computer science|nonlinear system identification|nonlinear system|object tracking|parameter identification|particle filters|computer vision|machine learning|applied mathematics|bayesian analysis|moving object tracking|statistics|nonlinear dynamic|machine vision|nonlinear science,
https://openalex.org/W2067191022,Mean shift: a robust approach toward feature space analysis,"Mean shift: a robust approach toward feature space analysis

A general non-parametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module an old pattern recognition procedure: mean shift. For discrete data, we prove convergence recursive shift procedure nearest stationary point underlying density function and, thus, its utility detecting modes density. relation Nadaraya-Watson estimator from kernel regression robust M-estimators; location also established. Algorithms two low-level vision tasks discontinuity-preserving smoothing image segmentation - are described as applications. In these algorithms, only user-set parameter resolution analysis, either gray-level or color images accepted input. Extensive experimental results illustrate their excellent performance.

pattern recognition, computer science, machine learning, feature space analysis, mean shift, image analysis, information fusion, structure from motion, feature detection, data science, feature (computer vision), computational imaging, robust feature, deep learning, computational statistic, machine learning research, shift detection, machine vision, computer vision, applied mathematics",2002,10781,pattern recognition|computer science|machine learning|feature space analysis|mean shift|image analysis|information fusion|structure from motion|feature detection|data science|feature (computer vision)|computational imaging|robust feature|deep learning|computational statistic|machine learning research|shift detection|machine vision|computer vision|applied mathematics,https://openalex.org/W2548197316
https://openalex.org/W2099244020,Bilateral filtering for gray and color images,"Bilateral filtering for gray and color images

Bilateral filtering smooths images while preserving edges, by means of a nonlinear combination nearby image values. The method is noniterative, local, and simple. It combines gray levels or colors based on both their geometric closeness photometric similarity, prefers near values to distant in domain range. In contrast with filters that operate the three bands color separately, bilateral filter can enforce perceptual metric underlying CIE-Lab space, smooth preserve edges way tuned human perception. Also, standard filtering, produces no phantom along images, reduces where they appear original image.

image analysis, computational imaging, computer science, color correction, bilateral filtering, image enhancement, computer vision, machine learning, feature detection, image restoration, object detection, color images, machine vision, digital image processing",2002,7491,image analysis|computational imaging|computer science|color correction|bilateral filtering|image enhancement|computer vision|machine learning|feature detection|image restoration|object detection|color images|machine vision|digital image processing,https://openalex.org/W2067191022
https://openalex.org/W2098693229,Face recognition using eigenfaces,"Face recognition using eigenfaces

An approach to the detection and identification of human faces is presented, a working, near-real-time face recognition system which tracks subject's head then recognizes person by comparing characteristics those known individuals described. This treats as two-dimensional problem, taking advantage fact that are normally upright thus may be described small set 2-D characteristic views. Face images projected onto feature space ('face space') best encodes variation among images. The defined 'eigenfaces', eigenvectors faces; they do not necessarily correspond isolated features such eyes, ears, noses. framework provides ability learn recognize new in an unsupervised manner.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>

pattern recognition, computer science, face recognition, image analysis, information fusion, feature detection, data science, computational imaging, localization, deep learning, machine learning research, machine vision, digital image processing, face detection, object recognition, feature extraction, computer vision, applied mathematics, image classification",2002,5301,pattern recognition|computer science|face recognition|image analysis|information fusion|feature detection|data science|computational imaging|localization|deep learning|machine learning research|machine vision|digital image processing|face detection|object recognition|feature extraction|computer vision|applied mathematics|image classification,https://openalex.org/W2106390385|https://openalex.org/W2129812935
https://openalex.org/W2159269332,A universal image quality index,"A universal image quality index

We propose a new universal objective image quality index, which is easy to calculate and applicable various processing applications. Instead of using traditional error summation methods, the proposed index designed by modeling any distortion as combination three factors: loss correlation, luminance distortion, contrast distortion. Although mathematically defined no human visual system model explicitly employed, our experiments on types indicate that it performs significantly better than widely used metric mean squared error. Demonstrative images an efficient MATLAB implementation algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality_index/demo.html.

image analysis, pattern recognition, computer science, computational imaging, content-based image retrieval, image coding, image quality assessment, image enhancement, computer vision, deep learning, image representation, image similarity, machine vision, digital image processing",2002,5217,image analysis|pattern recognition|computer science|computational imaging|content-based image retrieval|image coding|image quality assessment|image enhancement|computer vision|deep learning|image representation|image similarity|machine vision|digital image processing,
https://openalex.org/W1508960934,Computer Vision: A Modern Approach,"Computer Vision: A Modern Approach

From the Publisher:
The accessible presentation of this book gives both a general view entire computer vision enterprise and also offers sufficient detail to be able build useful applications. Users learn techniques that have proven by first-hand experience wide range mathematical methods. A CD-ROM with every copy text contains source code for programming practice, color images, illustrative movies. Comprehensive up-to-date, includes essential topics either reflect practical significance or are theoretical importance. Topics discussed in substantial increasing depth. Application surveys describe numerous important application areas such as image based rendering digital libraries. Many algorithms broken down illustrated pseudo code. Appropriate use engineers comprehensive reference enterprise.

image analysis, computer science, computer vision, object detection, image representation, machine vision, digital image processing, 3d computer vision",2002,3719,image analysis|computer science|computer vision|object detection|image representation|machine vision|digital image processing|3d computer vision,https://openalex.org/W2132947399|https://openalex.org/W3132455321
https://openalex.org/W2169551590,Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images,"Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images

In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as ""object"" or ""background"" to provide hard constraints segmentation. Additional soft incorporate both boundary and region information. Graph cuts are used find the globally optimal image. obtained solution gives best balance properties among all segmentations satisfying constraints. topology our is unrestricted segments may consist several isolated parts. Some experimental results presented in context photo/video editing medical image We also demonstrate an interesting Gestalt example. A fast implementation method possible via max-flow algorithm.

computer science, computer vision, n-d images, interactive graph cuts, image segmentation, region segmentation",2002,3532,computer science|computer vision|n-d images|interactive graph cuts|image segmentation|region segmentation,
https://openalex.org/W2121601095,Detecting faces in images: a survey,"Detecting faces in images: a survey

Images containing faces are essential to intelligent vision-based human-computer interaction, and research efforts in face processing include recognition, tracking, pose estimation expression recognition. However, many reported methods assume that the an image or sequence have been identified localized. To build fully automated systems analyze information contained images, robust efficient detection algorithms required. Given a single image, goal of is identify all regions which contain face, regardless its 3D position, orientation lighting conditions. Such problem challenging because non-rigid high degree variability size, shape, color texture. Numerous techniques developed detect purpose this paper categorize evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics benchmarking. After analyzing identifying their limitations, we conclude with several promising directions for future research.

image analysis, pattern recognition, computer science, computational imaging, information fusion, medical image computing, computer vision, machine learning, multimedia retrieval, cognitive science, object detection, data science, facial recognition system, vision recognition, machine vision, digital image processing, facial expression recognition, face detection",2002,3433,image analysis|pattern recognition|computer science|computational imaging|information fusion|medical image computing|computer vision|machine learning|multimedia retrieval|cognitive science|object detection|data science|facial recognition system|vision recognition|machine vision|digital image processing|facial expression recognition|face detection,
https://openalex.org/W2159128898,Real-time tracking of non-rigid objects using mean shift,"Real-time tracking of non-rigid objects using mean shift

A new method for real time tracking of non-rigid objects seen from a moving camera is proposed. The central computational module based on the mean shift iterations and finds most probable target position in current frame. dissimilarity between model (its color distribution) candidates expressed by metric derived Bhattacharyya coefficient. theoretical analysis approach shows that it relates to Bayesian framework while providing practical, fast efficient solution. capability tracker handle partial occlusions, significant clutter, scale variations, demonstrated several image sequences.

image analysis, pattern recognition, computer science, information fusion, motion analysis, object tracking, computer vision, machine learning, motion detection, object detection, data science, moving object tracking, real-time tracking, mean shift, machine vision, digital image processing, non-rigid objects",2002,2876,image analysis|pattern recognition|computer science|information fusion|motion analysis|object tracking|computer vision|machine learning|motion detection|object detection|data science|moving object tracking|real-time tracking|mean shift|machine vision|digital image processing|non-rigid objects,https://openalex.org/W2067191022|https://openalex.org/W2167089254
https://openalex.org/W2125127226,Comprehensive database for facial expression analysis,"Comprehensive database for facial expression analysis

Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, generalizability these various remains unknown. We describe problem space for analysis, which includes level description, transitions among expressions, eliciting conditions, reliability and validity training test data, individual differences subjects, head orientation scene complexity image characteristics, relation to non-verbal behavior. then present CMU-Pittsburgh AU-Coded Face Expression Image Database, currently 2105 digitized sequences from 182 adult subjects varying ethnicity, performing multiple tokens primary FACS action units. This database is comprehensive testbed date comparative studies

image analysis, pattern recognition, computer science, affective computing, computer vision, machine learning, comprehensive database, facial expression analysis, data science, deep learning, machine learning research, large-scale datasets, emotion recognition, database, face detection, facial expression recognition",2002,2495,image analysis|pattern recognition|computer science|affective computing|computer vision|machine learning|comprehensive database|facial expression analysis|data science|deep learning|machine learning research|large-scale datasets|emotion recognition|database|face detection|facial expression recognition,https://openalex.org/W2106390385
https://openalex.org/W2124351082,Training support vector machines: an application to face detection,"Training support vector machines: an application to face detection

We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found solving linearly constrained quadratic programming problem. This optimization problem challenging because form completely dense memory requirements grow with square number data points. present decomposition algorithm guarantees global optimality, used to train SVM's over very large sets. main idea behind iterative solution sub-problems evaluation optimality conditions which both generate improved values, also establish stopping criteria algorithm. experimental results our implementation SVM, demonstrate feasibility approach on face detection involves set 50,000

image analysis, pattern recognition, computer science, support vector machine, computer vision, machine learning, object detection, machine vision, face detection",2002,2465,image analysis|pattern recognition|computer science|support vector machine|computer vision|machine learning|object detection|machine vision|face detection,https://openalex.org/W2217896605|https://openalex.org/W3214102110|https://openalex.org/W2121601095
https://openalex.org/W2132680427,The curvelet transform for image denoising,"The curvelet transform for image denoising

We describe approximate digital implementations of two new mathematical transforms, namely, the ridgelet transform and curvelet transform. Our offer exact reconstruction, stability against perturbations, ease implementation, low computational complexity. A central tool is Fourier-domain computation an Radon introduce a very simple interpolation in Fourier space which takes Cartesian samples yields on rectopolar grid, pseudo-polar sampling set based concentric squares geometry. Despite crudeness our interpolation, visual performance surprisingly good. applies to special overcomplete wavelet pyramid whose wavelets have compact support frequency domain. uses as component step, implements subbands using filter bank a; trous filters. philosophy throughout that transforms should be overcomplete, rather than critically sampled. apply these denoising some standard images embedded white noise. In tests reported here, thresholding coefficients competitive with ""state art"" techniques wavelets, including decimated or undecimated also tree-based Bayesian posterior mean methods. Moreover, reconstructions exhibit higher perceptual quality wavelet-based reconstructions, offering visually sharper and, particular, recovery edges faint linear curvilinear features. Existing theory for suggests approaches can outperform methods certain image reconstruction problems. The empirical results here are encouraging agreement.

digital image processing, computer vision, computational imaging, curvelet transform, computer science, image denoising",2002,2202,digital image processing|computer vision|computational imaging|curvelet transform|computer science|image denoising,https://openalex.org/W1976709621
https://openalex.org/W2106115875,Coding facial expressions with Gabor wavelets,"Coding facial expressions with Gabor wavelets

A method for extracting information about facial expressions from images is presented. Facial expression are coded using a multi-orientation multi-resolution set of Gabor filters which topographically ordered and aligned approximately with the face. The similarity space derived this representation compared one semantic ratings by human observers. results show that it possible to construct classifier coding as input stage. shows significant degree psychological plausibility, design feature may be important human-computer interfaces.

image analysis, pattern recognition, computer science, computational imaging, information fusion, gabor wavelets, scene interpretation, facial expressions, affective computing, computer vision, machine learning, multimodal signal processing, facial recognition system, wavelet, emotion recognition, machine vision, facial expression recognition",2002,1993,image analysis|pattern recognition|computer science|computational imaging|information fusion|gabor wavelets|scene interpretation|facial expressions|affective computing|computer vision|machine learning|multimodal signal processing|facial recognition system|wavelet|emotion recognition|machine vision|facial expression recognition,https://openalex.org/W2159017231|https://openalex.org/W2799041689
https://openalex.org/W3118608800,Learning Multiple Layers of Features from Tiny Images,"Learning Multiple Layers of Features from Tiny Images

In this work we describe how to train a multi-layer generative model of natural images. We use dataset millions tiny colour images, described in the next section. This has been attempted by several groups but without success. The models on which focus are RBMs (Restricted Boltzmann Machines) and DBNs (Deep Belief Networks). These learn interesting-looking filters, show more useful classifier than raw pixels. labeled subset that have collected call CIFAR-10 dataset.

image analysis, computational imaging, computer science, feature learning, computer vision, machine learning, tiny images, feature fusion, multiple layers, data science, knowledge discovery, deep learning, image representation, few-shot learning, machine vision, digital image processing, multiple instance learning",2009,21388,image analysis|computational imaging|computer science|feature learning|computer vision|machine learning|tiny images|feature fusion|multiple layers|data science|knowledge discovery|deep learning|image representation|few-shot learning|machine vision|digital image processing|multiple instance learning,https://openalex.org/W2102605133|https://openalex.org/W2963446712|https://openalex.org/W3094502228|https://openalex.org/W3005680577|https://openalex.org/W3023371261|https://openalex.org/W4214493665|https://openalex.org/W3157506437|https://openalex.org/W4206706211
https://openalex.org/W2129812935,Robust Face Recognition via Sparse Representation,"Robust Face Recognition via Sparse Representation

We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well occlusion disguise. cast recognition one classifying among multiple linear regression models argue that new theory sparse signal representation offers key to addressing this problem. Based on a computed by l{1}-minimization, we propose general classification algorithm for (image-based) object recognition. This framework provides insights into two crucial issues in face recognition: feature extraction robustness occlusion. For extraction, show if sparsity is properly harnessed, choice features no longer critical. What critical, however, whether number sufficiently large correctly computed. Unconventional such downsampled images random projections perform just conventional Eigenfaces Laplacianfaces, long dimension space surpasses certain threshold, predicted representation. can handle errors due corruption uniformly exploiting fact these are often respect standard (pixel) basis. The helps predict how much choose training maximize conduct extensive experiments publicly available databases verify efficacy proposed corroborate above claims.

image analysis, pattern recognition, computer science, computational imaging, object recognition, feature learning, feature extraction, computer vision, machine learning, feature detection, data science, sparse representation, deep learning, image representation, machine vision, robust face recognition",2009,9264,image analysis|pattern recognition|computer science|computational imaging|object recognition|feature learning|feature extraction|computer vision|machine learning|feature detection|data science|sparse representation|deep learning|image representation|machine vision|robust face recognition,
https://openalex.org/W2024165284,Tensor Decompositions and Applications,"Tensor Decompositions and Applications

This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A is a multidimensional or N-way array. Decompositions tensors (i.e., arrays with $N \geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, analysis, data mining, neuroscience, graph elsewhere. Two particular decompositions can be considered to extensions the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes as sum rank-one tensors, Tucker decomposition form principal component analysis. There are many other including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, PARATUCK2 well nonnegative variants all above. The Toolbox, Tensor Multilinear Engine examples software packages for working tensors.

mathematics, computational imaging, computer science, dimensionality reduction, linear algebra, operator theory, parameter identification, tensor decompositions, matrix analysis, computer vision, low-rank approximation, applied mathematics, numerical linear algebra, spectral theory, machine learning research, feature construction, computational geometry",2009,8929,mathematics|computational imaging|computer science|dimensionality reduction|linear algebra|operator theory|parameter identification|tensor decompositions|matrix analysis|computer vision|low-rank approximation|applied mathematics|numerical linear algebra|spectral theory|machine learning research|feature construction|computational geometry,
https://openalex.org/W2160821342,Fast Point Feature Histograms (FPFH) for 3D registration,"Fast Point Feature Histograms (FPFH) for 3D registration

In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi-dimensional features which describe the local geometry around a point p for 3D cloud datasets. this paper, modify their mathematical expressions and perform rigorous analysis on robustness complexity problem of registration overlapping views. More concretely, present several optimizations that reduce computation times drastically by either caching previously computed values or revising theoretical formulations. The latter results in new type features, called Fast (FPFH), retain most discriminative power PFH. Moreover, propose an algorithm online FPFH realtime applications. To validate demonstrate efficiency sample consensus based method bringing two datasets into convergence basin non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment).

pattern recognition, computer science, multi-view geometry, machine learning, point feature histograms, image analysis, information fusion, data science, 3d imaging, image registration, computational imaging, systems engineering, 3d object recognition, additive manufacturing, machine learning research, machine vision, digital image processing, computer vision, 3d pose estimation",2009,2983,pattern recognition|computer science|multi-view geometry|machine learning|point feature histograms|image analysis|information fusion|data science|3d imaging|image registration|computational imaging|systems engineering|3d object recognition|additive manufacturing|machine learning research|machine vision|digital image processing|computer vision|3d pose estimation,https://openalex.org/W2560609797|https://openalex.org/W3012494314
https://openalex.org/W1976709621,From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images,"From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images

A full-rank matrix ${\bf A}\in \mathbb{R}^{n\times m}$ with $n<m$ generates an underdetermined system of linear equations Ax} = {\bf b}$ having infinitely many solutions. Suppose we seek the sparsest solution, i.e., one fewest nonzero entries. Can it ever be unique? If so, when? As optimization sparsity is combinatorial in nature, are there efficient methods for finding solution? These questions have been answered positively and constructively recent years, exposing a wide variety surprising phenomena, particular existence easily verifiable conditions under which optimally sparse solutions can found by concrete, effective computational methods. Such theoretical results inspire bold perspective on some important practical problems signal image processing. Several well-known processing cast as demanding undetermined systems equations. previously seemed, to many, intractable, but considerable evidence that these often Hence, advances energized research such problems—to striking effect. In this paper review systems, empirical modeling signals images, applications inverse compression This work lies at intersection applied mathematics, arose initially from wavelets harmonic analysis communities. The aim introduce few key notions connected sparsity, targeting newcomers interested either mathematical aspects area or its applications.

image analysis, computational imaging, nonlinear signal processing, sparse solutions, signal processing, speech processing, computer vision, systems biology, applied mathematics, systems modeling, sparse representation, image representation, temporal complexity, signal reconstruction",2009,2187,image analysis|computational imaging|nonlinear signal processing|sparse solutions|signal processing|speech processing|computer vision|systems biology|applied mathematics|systems modeling|sparse representation|image representation|temporal complexity|signal reconstruction,
https://openalex.org/W2546302380,What is the best multi-stage architecture for object recognition?,"What is the best multi-stage architecture for object recognition?

In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, non-linear transformation, and some sort pooling layer. Most systems use only one stage in which the filters hard-wired, or two where both learned supervised unsupervised mode. This paper addresses three questions: 1. How does non-linearities that follow banks influence accuracy? 2. learning an manner improve performance over random hardwired filters? 3. Is there any advantage to using architecture with extraction, rather than one? We show include rectification local contrast normalization is single most important ingredient for good accuracy on benchmarks. yield better one. surprisingly, we two-stage system can almost 63% rate Caltech-101, provided proper layers used. Finally, refinement, achieves state-of-the-art NORB dataset (5.6%) pre-training followed by refinement produces Caltech-101 (> 65%), lowest known error undistorted, unprocessed MNIST (0.53%).

computer science, machine learning, image analysis, information fusion, convolutional neural network, robot learning, object detection, image representation, computational imaging, neural architecture search, systems engineering, deep learning, 3d object recognition, machine vision, best multi-stage architecture, object recognition, computer vision, architecture, neural network (machine learning)",2009,2057,computer science|machine learning|image analysis|information fusion|convolutional neural network|robot learning|object detection|image representation|computational imaging|neural architecture search|systems engineering|deep learning|3d object recognition|machine vision|best multi-stage architecture|object recognition|computer vision|architecture|neural network (machine learning),https://openalex.org/W2155541015|https://openalex.org/W2963881378
https://openalex.org/W2142224912,Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems,"Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems

This paper studies gradient-based schemes for image denoising and deblurring problems based on the discretized total variation (TV) minimization model with constraints. We derive a fast algorithm constrained TV-based deburring problem. To achieve this task, we combine an acceleration of well known dual approach to problem novel monotone version iterative shrinkage/thresholding (FISTA) have recently introduced. The resulting shares remarkable simplicity together proven global rate convergence which is significantly better than currently gradient projections-based methods. Our results are applicable both anisotropic isotropic TV functionals. Initial numerical demonstrate viability efficiency proposed algorithms box

image analysis, pattern recognition, computer science, computational imaging, gradient-based algorithms, variational analysis, algorithmic development, image restoration, computer vision, machine learning, applied mathematics, data science, deblurring, machine learning research, deblurring problems, image denoising, machine vision, digital image processing",2009,1864,image analysis|pattern recognition|computer science|computational imaging|gradient-based algorithms|variational analysis|algorithmic development|image restoration|computer vision|machine learning|applied mathematics|data science|deblurring|machine learning research|deblurring problems|image denoising|machine vision|digital image processing,
https://openalex.org/W2534320940,Super-resolution from a single image,"Super-resolution from a single image

Methods for super-resolution can be broadly classified into two families of methods: (i) The classical multi-image (combining images obtained at subpixel misalignments), and (ii) Example-Based (learning correspondence between low high resolution image patches from a database). In this paper we propose unified framework combining these methods. We further show how combined approach applied to obtain super as little single (with no database or prior examples). Our is based on the observation that in natural tend redundantly recur many times inside image, both within same scale, well across different scales. Recurrence scale (at misalignments) gives rise super-resolution, whereas recurrence scales example-based super-resolution. attempts recover each pixel its best possible increase patch redundancy

image analysis, computational imaging, computer science, super-resolution imaging, image stitching, computer vision, high resolution, single-image super-resolution, single image, image representation, image resolution, machine vision, digital image processing",2009,1808,image analysis|computational imaging|computer science|super-resolution imaging|image stitching|computer vision|high resolution|single-image super-resolution|single image|image representation|image resolution|machine vision|digital image processing,https://openalex.org/W1885185971|https://openalex.org/W2242218935|https://openalex.org/W2476548250|https://openalex.org/W2963470893|https://openalex.org/W2963372104|https://openalex.org/W3013529009
https://openalex.org/W2167089254,Visual tracking with online Multiple Instance Learning,"Visual tracking with online Multiple Instance Learning

In this paper, we address the problem of learning an adaptive appearance model for object tracking. particular, a class tracking techniques called ""tracking by detection"" have been shown to give promising results at real-time speeds. These methods train discriminative classifier in online manner separate from background. This bootstraps itself using current tracker state extract positive and negative examples frame. Slight inaccuracies can therefore lead incorrectly labeled training examples, which degrades cause further drift. paper show that Multiple Instance Learning (MIL) instead traditional supervised avoids these problems, more robust with fewer parameter tweaks. We present novel MIL algorithm achieves superior performance.

multi-task learning, image analysis, computer science, computer vision, machine learning, deep learning, multiple instance learning, machine vision",2009,1749,multi-task learning|image analysis|computer science|computer vision|machine learning|deep learning|multiple instance learning|machine vision,
https://openalex.org/W1579624063,Global Multi‐Resolution Topography synthesis,"Global Multi‐Resolution Topography synthesis

Seafloor bathymetric data acquired with modern swath echo sounders provide coverage for only a small fraction of the global seabed yet are high value studies dynamic processes seafloor volcanism, tectonics, mass wasting, and sediment transport that create shape undersea landscape. A new method compilation bathymetry preserves native resolution sonars is presented. The Global Multi‐Resolution Topography synthesis consists hierarchy tiles digital elevations shaded relief imagery spanning nine magnification doublings from pole to ( http://www.marine‐geo.org/portals/gmrt ). updated accessible as surveys contributed, edited, added tiles. Access via Web services WMS‐enabled client applications such GeoMapApp®, Virtual Ocean, NASA World Wind®, Google Earth®.

image analysis, computational imaging, computer science, digital earth, computer graphic, global optimization, computer vision, geography, image reconstruction, multi-resolution method, 3d reconstruction, deep learning, multi-resolution modeling, volume rendering, earth science, remote sensing, machine vision, digital image processing",2009,1734,image analysis|computational imaging|computer science|digital earth|computer graphic|global optimization|computer vision|geography|image reconstruction|multi-resolution method|3d reconstruction|deep learning|multi-resolution modeling|volume rendering|earth science|remote sensing|machine vision|digital image processing,
https://openalex.org/W2536599074,Non-local sparse models for image restoration,"Non-local sparse models for image restoration

We propose in this paper to unify two different approaches image restoration: On the one hand, learning a basis set (dictionary) adapted sparse signal descriptions has proven be very effective reconstruction and classification tasks. other explicitly exploiting self-similarities of natural images led successful non-local means approach restoration. simultaneous coding as framework for combining these manner. This is achieved by jointly decomposing groups similar signals on subsets learned dictionary. Experimental results denoising demosaicking tasks with synthetic real noise show that proposed method outperforms state art, making it possible effectively restore raw from digital cameras at reasonable speed memory cost.

image analysis, computational imaging, computer science, compressive sensing, non-local sparse models, image restoration, computer vision, machine learning, sparse neural network, image reconstruction, sparse representation, image representation, machine learning research, digital image processing",2009,1641,image analysis|computational imaging|computer science|compressive sensing|non-local sparse models|image restoration|computer vision|machine learning|sparse neural network|image reconstruction|sparse representation|image representation|machine learning research|digital image processing,https://openalex.org/W2508457857|https://openalex.org/W3167568784
https://openalex.org/W2548197316,An HOG-LBP human detector with partial occlusion handling,"An HOG-LBP human detector with partial occlusion handling

By combining Histograms of Oriented Gradients (HOG) and Local Binary Pattern (LBP) as the feature set, we propose a novel human detection approach capable handling partial occlusion. Two kinds detectors, i.e., global detector for whole scanning windows part detectors local regions, are learned from training data using linear SVM. For each ambiguous window, construct an occlusion likelihood map by response block HOG to detector. The is then segmented Mean-shift approach. portion window with majority negative inferred occluded region. If indicated high in certain applied on unoccluded regions achieve final classification current window. With help augmented HOG-LBP global-part method, rate 91.3% FPPW= 10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">−6</sup> , 94.7% xmlns:xlink=""http://www.w3.org/1999/xlink"">−5</sup> 97.9% xmlns:xlink=""http://www.w3.org/1999/xlink"">−4</sup> INRIA dataset, which, our best knowledge, performance dataset. method further validated synthesized constructed Pascal

image analysis, human pose estimation, computer science, partial occlusion handling, computer vision, object detection, detection technique, hog-lbp human detector, human identification, machine vision, early detection",2009,1637,image analysis|human pose estimation|computer science|partial occlusion handling|computer vision|object detection|detection technique|hog-lbp human detector|human identification|machine vision|early detection,
https://openalex.org/W2132947399,Make3D: Learning 3D Scene Structure from a Single Still Image,"Make3D: Learning 3D Scene Structure from a Single Still Image

We consider the problem of estimating detailed 3D structure from a single still image an unstructured environment. Our goal is to create models that are both quantitatively accurate as well visually pleasing. For each small homogeneous patch in image, we use Markov Random Field (MRF) infer set ""plane parameters"" capture location and orientation patch. The MRF, trained via supervised learning, depth cues relationships between different parts image. Other than assuming environment made up number planes, our model makes no explicit assumptions about scene; this enables algorithm much more does prior art also give richer experience flythroughs created using image-based rendering, even for scenes with significant nonvertical structure. Using approach, have qualitatively correct 64.9 percent 588 images downloaded Internet. extended produce large-scale few images.

image analysis, computational imaging, computer science, scene interpretation, scene modeling, scene understanding, scene structure, computer vision, machine learning, scene analysis, deep learning, machine vision, digital image processing, 3d computer vision",2009,1602,image analysis|computational imaging|computer science|scene interpretation|scene modeling|scene understanding|scene structure|computer vision|machine learning|scene analysis|deep learning|machine vision|digital image processing|3d computer vision,https://openalex.org/W2171740948|https://openalex.org/W3081167590
https://openalex.org/W2152161678,Recognizing indoor scenes,"Recognizing indoor scenes

Indoor scene recognition is a challenging open problem in high level vision. Most models that work well for outdoor scenes perform poorly the indoor domain. The main difficulty while some (e.g. corridors) can be characterized by global spatial properties, others (e.g, bookstores) are better objects they contain. More generally, to address we need model exploit local and discriminative information. In this paper propose prototype based successfully combine both sources of To test our approach created dataset 67 categories (the largest available) covering wide range domains. results show significantly outperform state art classifier task.

image analysis, computer science, computer vision, localization, digital image processing, vision recognition, object recognition, optical image recognition, computational imaging, multimedia retrieval, scene understanding, indoor scenes, image representation, data science, deep learning, scene analysis, pattern recognition, machine vision",2009,1390,image analysis|computer science|computer vision|localization|digital image processing|vision recognition|object recognition|optical image recognition|computational imaging|multimedia retrieval|scene understanding|indoor scenes|image representation|data science|deep learning|scene analysis|pattern recognition|machine vision,https://openalex.org/W2062118960|https://openalex.org/W2134670479
https://openalex.org/W2052094314,ASIFT: A New Framework for Fully Affine Invariant Image Comparison,"ASIFT: A New Framework for Fully Affine Invariant Image Comparison

If a physical object has smooth or piecewise boundary, its images obtained by cameras in varying positions undergo apparent deformations. These deformations are locally well approximated affine transforms of the image plane. In consequence solid recognition problem often been led back to computation invariant local features. Such features could be normalization methods, but no fully method exists for time being. Even scale invariance is dealt with rigorously only scale-invariant feature transform (SIFT) method. By simulating zooms out and normalizing translation rotation, SIFT four six parameters an transform. The proposed this paper, affine-SIFT (ASIFT), simulates all views obtainable two camera axis orientation parameters, namely, latitude longitude angles, left over Then it covers other using itself. resulting will mathematically proved invariant. Against any prognosis, depending on feasible dramatic computational load. A two-resolution scheme further reduces ASIFT complexity about twice that SIFT. new notion, transition tilt, measuring amount distortion from one view another, introduced. While absolute tilt frontal slanted exceeding 6 rare, much higher tilts common when compared (see Figure hightransitiontiltsillustration). attainable measured each comparison permits reliably identify have undergone large magnitude, up 36 higher. This fact substantiated many experiments which show significantly outperforms state-of-the-art methods SIFT, maximally stable extremal region (MSER), Harris-affine, Hessian-affine.

image analysis, pattern recognition, computer science, computational imaging, information fusion, content-based image retrieval, computer vision, deep learning, image representation, image similarity, machine vision, digital image processing",2009,1380,image analysis|pattern recognition|computer science|computational imaging|information fusion|content-based image retrieval|computer vision|deep learning|image representation|image similarity|machine vision|digital image processing,
https://openalex.org/W2102605133,Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation,"Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation

Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable algorithm improves mean average precision (mAP) by more than 30% relative to previous best result 2012 -- achieving mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) bottom-up region proposals order localize segment objects (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed domain-specific fine-tuning, yields significant performance boost. Since CNNs, call our method R-CNN: Regions CNN features. We also present experiments provide insight into what network learns, revealing rich hierarchy Source code complete system available at http://www.cs.berkeley.edu/~rbg/rcnn.

pattern recognition, computer science, machine learning, fuzzy set, image analysis, information fusion, accurate object detection, feature detection, cognitive science, object detection, data science, object categorization, computational imaging, localization, deep learning, machine vision, rich feature hierarchies, semantic segmentation, object recognition, computer vision, scene analysis",2014,24628,pattern recognition|computer science|machine learning|fuzzy set|image analysis|information fusion|accurate object detection|feature detection|cognitive science|object detection|data science|object categorization|computational imaging|localization|deep learning|machine vision|rich feature hierarchies|semantic segmentation|object recognition|computer vision|scene analysis,https://openalex.org/W2016053056|https://openalex.org/W2950094539|https://openalex.org/W2062118960|https://openalex.org/W1923697677|https://openalex.org/W7746136|https://openalex.org/W2963173190|https://openalex.org/W1903029394|https://openalex.org/W2953106684|https://openalex.org/W1677182931|https://openalex.org/W2613718673|https://openalex.org/W2109255472|https://openalex.org/W1522734439|https://openalex.org/W1905882502|https://openalex.org/W1745334888|https://openalex.org/W764651262|https://openalex.org/W2963037989|https://openalex.org/W2183341477|https://openalex.org/W2963420272|https://openalex.org/W639708223|https://openalex.org/W2963351448|https://openalex.org/W2565639579|https://openalex.org/W2962858109|https://openalex.org/W2963524571|https://openalex.org/W3018757597|https://openalex.org/W3035524453|https://openalex.org/W2884561390|https://openalex.org/W3023371261|https://openalex.org/W3210586215|https://openalex.org/W4312443924|https://openalex.org/W2799041689|https://openalex.org/W3136761610
https://openalex.org/W2145287260,DeepFace: Closing the Gap to Human-Level Performance in Face Verification,"DeepFace: Closing the Gap to Human-Level Performance in Face Verification

In modern face recognition, the conventional pipeline consists of four stages: detect => align represent classify. We revisit both alignment step and representation by employing explicit 3D modeling in order to apply a piecewise affine transformation, derive from nine-layer deep neural network. This network involves more than 120 million parameters using several locally connected layers without weight sharing, rather standard convolutional layers. Thus we trained it on largest facial dataset to-date, an identity labeled images belonging 4, 000 identities. The learned representations coupling accurate model-based with large database generalize remarkably well faces unconstrained environments, even simple classifier. Our method reaches accuracy 97.35% Labeled Faces Wild (LFW) dataset, reducing error current state art 27%, closely approaching human-level performance.

image analysis, pattern recognition, computer science, information fusion, adversarial machine learning, face verification, human-level performance, computer vision, biometrics, cognitive science, data science, computational intelligence, deep learning, deepfakes, human image synthesis, facial recognition system, machine vision, face detection",2014,5810,image analysis|pattern recognition|computer science|information fusion|adversarial machine learning|face verification|human-level performance|computer vision|biometrics|cognitive science|data science|computational intelligence|deep learning|deepfakes|human image synthesis|facial recognition system|machine vision|face detection,https://openalex.org/W2062118960|https://openalex.org/W1677182931|https://openalex.org/W2109255472|https://openalex.org/W1834627138|https://openalex.org/W2325939864
https://openalex.org/W2016053056,Large-Scale Video Classification with Convolutional Neural Networks,"Large-Scale Video Classification with Convolutional Neural Networks

Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation CNNs on large-scale video classification using new dataset 1 million YouTube videos belonging to 487 classes. We study multiple approaches extending the connectivity CNN in time domain take advantage local spatio-temporal information and suggest multiresolution, foveated architecture promising way speeding up training. Our best networks display significant performance improvements compared strong feature-based baselines (55.3% 63.9%), but only surprisingly modest improvement single-frame (59.3% 60.9%). further generalization our model retraining top layers UCF-101 Action Recognition observe baseline (63.3% from 43.9%).

neural network (machine learning), computer science, machine learning research, deep learning, pattern recognition, machine vision, multimedia information processing, multimedia retrieval, scene understanding, computational imaging, machine learning, data science, video interpretation, video retrieval, computer vision, video understanding, cognitive science, large-scale video classification, convolutional neural network",2014,5630,neural network (machine learning)|computer science|machine learning research|deep learning|pattern recognition|machine vision|multimedia information processing|multimedia retrieval|scene understanding|computational imaging|machine learning|data science|video interpretation|video retrieval|computer vision|video understanding|cognitive science|large-scale video classification|convolutional neural network,https://openalex.org/W2156303437|https://openalex.org/W1522734439|https://openalex.org/W1947481528|https://openalex.org/W2183341477|https://openalex.org/W2963524571|https://openalex.org/W4214612132
https://openalex.org/W2156303437,Two-Stream Convolutional Networks for Action Recognition in Videos,"Two-Stream Convolutional Networks for Action Recognition in Videos

We investigate architectures of discriminatively trained deep Convolutional Networks (ConvNets) for action recognition in video. The challenge is to capture the complementary information on appearance from still frames and motion between frames. also aim generalise best performing hand-crafted features within a data-driven learning framework. Our contribution three-fold. First, we propose two-stream ConvNet architecture which incorporates spatial temporal networks. Second, demonstrate that multi-frame dense optical flow able achieve very good performance spite limited training data. Finally, show multi-task learning, applied two different classification datasets, can be used increase amount data improve both. evaluated standard video actions benchmarks UCF-101 HMDB-51, where it competitive with state art. It exceeds by large margin previous attempts use nets classification.

digital image processing, computer science, deep learning, video content analysis, information fusion, machine vision, multimedia information processing, video observation, computational imaging, two-stream convolutional networks, machine learning, data science, action recognition, video retrieval, computer vision, video understanding, cognitive science, convolutional neural network, image analysis",2014,4829,digital image processing|computer science|deep learning|video content analysis|information fusion|machine vision|multimedia information processing|video observation|computational imaging|two-stream convolutional networks|machine learning|data science|action recognition|video retrieval|computer vision|video understanding|cognitive science|convolutional neural network|image analysis,https://openalex.org/W1522734439|https://openalex.org/W1947481528|https://openalex.org/W1745334888|https://openalex.org/W2963524571|https://openalex.org/W3023371261|https://openalex.org/W4214612132|https://openalex.org/W4312443924
https://openalex.org/W2015159529,scikit-image: image processing in Python,"scikit-image: image processing in Python

scikit-image is an image processing library that implements algorithms and utilities for use in research, education industry applications. It released under the liberal Modified BSD open source license, provides a well-documented API Python programming language, developed by active, international team of collaborators. In this paper we highlight advantages to achieve goals library, showcase several real-world applications scikit-image. More information can be found on project homepage, http://scikit-image.org.

digital image processing, image processing, computer science, pattern recognition, machine vision, multimedia information processing, feature extraction, image representation, image restoration, computational imaging, image manipulation, machine learning, computational photography, image classification, data science, scientific computing, computer vision, computer engineering, image analysis",2014,4531,digital image processing|image processing|computer science|pattern recognition|machine vision|multimedia information processing|feature extraction|image representation|image restoration|computational imaging|image manipulation|machine learning|computational photography|image classification|data science|scientific computing|computer vision|computer engineering|image analysis,
https://openalex.org/W2950094539,Caffe: Convolutional Architecture for Fast Feature Embedding,"Caffe: Convolutional Architecture for Fast Feature Embedding

Caffe provides multimedia scientists and practitioners with a clean modifiable framework for state-of-the-art deep learning algorithms collection of reference models. The is BSD-licensed C++ library Python MATLAB bindings training deploying general-purpose convolutional neural networks other models efficiently on commodity architectures. fits industry internet-scale media needs by CUDA GPU computation, processing over 40 million images day single K40 or Titan ($\approx$ 2.5 ms per image). By separating model representation from actual implementation, allows experimentation seamless switching among platforms ease development deployment prototyping machines to cloud environments. maintained developed the Berkeley Vision Learning Center (BVLC) help an active community contributors GitHub. It powers ongoing research projects, large-scale industrial applications, startup prototypes in vision, speech, multimedia.

pattern recognition, computer science, feature learning, fast feature, machine learning, feature construction, fuzzy set, image analysis, convolutional neural network, feature detection, cognitive science, data science, convolutional architecture, image representation, feature (computer vision), computational imaging, deep learning, machine learning research, machine vision, computer vision",2014,4037,pattern recognition|computer science|feature learning|fast feature|machine learning|feature construction|fuzzy set|image analysis|convolutional neural network|feature detection|cognitive science|data science|convolutional architecture|image representation|feature (computer vision)|computational imaging|deep learning|machine learning research|machine vision|computer vision,https://openalex.org/W1923697677|https://openalex.org/W2953106684|https://openalex.org/W2962914239|https://openalex.org/W2612445135
https://openalex.org/W2062118960,CNN Features Off-the-Shelf: An Astounding Baseline for Recognition,"CNN Features Off-the-Shelf: An Astounding Baseline for Recognition

Recent results indicate that the generic descriptors extracted from convolutional neural networks are very powerful. This paper adds to mounting evidence this is indeed case. We report on a series of experiments conducted for different recognition tasks using publicly available code and model OverFeat network which was trained perform object classification ILSVRC13. use features as image representation tackle diverse range classification, scene recognition, fine grained attribute detection retrieval applied set datasets. selected these datasets they gradually move further away original task data solve. Astonishingly, we consistent superior compared highly tuned state-of-the-art systems in all visual various For instance it consistently outperforms low memory footprint methods except sculptures dataset. The achieved linear SVM classifier (or L2 distance case retrieval) feature size 4096 layer net. representations modified simple augmentation techniques e.g. jittering. strongly suggest obtained deep learning with nets should be primary candidate most tasks.

image analysis, computer science, machine learning, digital image processing, feature detection, information fusion, object recognition, object categorization, computational imaging, machine learning research, feature (computer vision), image representation, data science, deep learning, convolutional neural network, pattern recognition, cognitive science, machine vision",2014,3805,image analysis|computer science|machine learning|digital image processing|feature detection|information fusion|object recognition|object categorization|computational imaging|machine learning research|feature (computer vision)|image representation|data science|deep learning|convolutional neural network|pattern recognition|cognitive science|machine vision,https://openalex.org/W2016053056|https://openalex.org/W2134670479|https://openalex.org/W2109255472|https://openalex.org/W1834627138|https://openalex.org/W2799041689
https://openalex.org/W2155541015,DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,"DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition

We evaluate whether features extracted from the activation of a deep convolutional network trained in fully supervised fashion on large, fixed set object recognition tasks can be repurposed to novel generic tasks. Our may differ significantly originally and there insufficient labeled or unlabeled data conventionally train adapt architecture new investigate visualize semantic clustering with respect variety such tasks, including scene recognition, domain adaptation, fine-grained challenges. compare efficacy relying various levels define feature, report results that outperform state-of-the-art several important vision are releasing DeCAF, an open-source implementation these features, along all associated parameters enable researchers able conduct experimentation representations across range visual concept learning paradigms.

pattern recognition, computer science, feature learning, machine learning, visual science, generic visual recognition, image analysis, information fusion, convolutional neural network, cognitive science, data science, image representation, vision recognition, feature (computer vision), computational imaging, deep learning, machine vision, visual question answering, computer vision",2014,3290,pattern recognition|computer science|feature learning|machine learning|visual science|generic visual recognition|image analysis|information fusion|convolutional neural network|cognitive science|data science|image representation|vision recognition|feature (computer vision)|computational imaging|deep learning|machine vision|visual question answering|computer vision,https://openalex.org/W2102605133|https://openalex.org/W2950094539|https://openalex.org/W2062118960|https://openalex.org/W2134670479|https://openalex.org/W1903029394|https://openalex.org/W2109255472|https://openalex.org/W1522734439|https://openalex.org/W1834627138|https://openalex.org/W2963037989|https://openalex.org/W2475287302|https://openalex.org/W2963420272|https://openalex.org/W2395611524|https://openalex.org/W3005680577
https://openalex.org/W1923697677,Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs,"Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs

Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs probabilistic graphical models for addressing task pixel-level (also called ""semantic segmentation""). We show that responses at final layer are not sufficiently localized accurate segmentation. is due to very invariance properties make good tasks. overcome this poor localization property deep networks by combining DCNN with a fully connected Conditional Random Field (CRF). Qualitatively, our ""DeepLab"" system able localize segment boundaries accuracy which beyond previous methods. Quantitatively, method sets new state-of-art PASCAL VOC-2012 semantic segmentation task, reaching 71.6% IOU test set. how these results can be obtained efficiently: Careful network re-purposing novel application 'hole' algorithm wavelet community allow dense computation neural net 8 frames per second on modern GPU.

image analysis, computational imaging, computer science, convolutional neural network, scene interpretation, deep convolutional nets, scene understanding, computer vision, machine learning, scene analysis, deep learning, image representation, machine learning research, image segmentation, semantic image segmentation, digital image processing",2014,3177,image analysis|computational imaging|computer science|convolutional neural network|scene interpretation|deep convolutional nets|scene understanding|computer vision|machine learning|scene analysis|deep learning|image representation|machine learning research|image segmentation|semantic image segmentation|digital image processing,https://openalex.org/W1745334888|https://openalex.org/W2475287302|https://openalex.org/W2963073614|https://openalex.org/W2963881378|https://openalex.org/W2560023338|https://openalex.org/W2395611524|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3170841864|https://openalex.org/W4285531802|https://openalex.org/W4214893857
https://openalex.org/W2963542991,"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks","OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks

Abstract: We present an integrated framework for using Convolutional Networks classification, localization and detection. show how a multiscale sliding window approach can be efficiently implemented within ConvNet. also introduce novel deep learning to by predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order increase detection confidence. that different tasks learned simultaneously single shared network. This is the winner of task ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) obtained very competitive results classifications tasks. In post-competition work, we establish new state art task. Finally, release feature extractor from our best model called OverFeat.

image analysis, pattern recognition, computer science, convolutional networks, object recognition, integrated recognition, computer vision, machine learning, feature detection, object detection, localization, deep learning, shift detection",2014,2672,image analysis|pattern recognition|computer science|convolutional networks|object recognition|integrated recognition|computer vision|machine learning|feature detection|object detection|localization|deep learning|shift detection,https://openalex.org/W2016053056|https://openalex.org/W2950094539|https://openalex.org/W2062118960|https://openalex.org/W2963173190|https://openalex.org/W1903029394|https://openalex.org/W2953106684|https://openalex.org/W1677182931|https://openalex.org/W2613718673|https://openalex.org/W2109255472|https://openalex.org/W2325939864|https://openalex.org/W2963037989|https://openalex.org/W639708223|https://openalex.org/W2963351448|https://openalex.org/W2565639579|https://openalex.org/W2395611524|https://openalex.org/W3034971973|https://openalex.org/W2884561390|https://openalex.org/W3014641072
https://openalex.org/W7746136,Edge Boxes: Locating Object Proposals from Edges,"Edge Boxes: Locating Object Proposals from Edges

The use of object proposals is an effective recent approach for increasing the computational efficiency detection. We propose a novel method generating bounding box using edges. Edges provide sparse yet informative representation image. Our main observation that number contours are wholly contained in indicative likelihood containing object. simple objectness score measures edges exist minus those members overlap box's boundary. Using efficient data structures, millions candidate boxes can be evaluated fraction second, returning ranked set few thousand top-scoring proposals. standard metrics, we show results significantly more accurate than current state-of-the-art while being faster to compute. In particular, given just 1000 achieve over 96% recall at threshold 0.5 and 75% challenging 0.7. runs 0.25 seconds additionally demonstrate near real-time variant with only minor loss accuracy.

image analysis, pattern recognition, computer science, edge detection, edge boxes, object tracking, computer vision, scene analysis, object detection, edge computing, localization, moving object tracking, object proposals, machine vision",2014,2607,image analysis|pattern recognition|computer science|edge detection|edge boxes|object tracking|computer vision|scene analysis|object detection|edge computing|localization|moving object tracking|object proposals|machine vision,https://openalex.org/W2953106684|https://openalex.org/W2613718673|https://openalex.org/W2109255472|https://openalex.org/W1834627138|https://openalex.org/W1745334888|https://openalex.org/W2963037989|https://openalex.org/W639708223|https://openalex.org/W2963351448|https://openalex.org/W2963881378|https://openalex.org/W2884561390
https://openalex.org/W2963173190,Return of the Devil in the Details: Delving Deep into Convolutional Nets,"Return of the Devil in the Details: Delving Deep into Convolutional Nets

The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest community these methods. Nevertheless, it is still unclear how different CNN methods compare with each other previous state-of-the-art shallow representations such as Bag-of-Visual-Words Improved Fisher Vector. This paper conducts a rigorous evaluation new techniques, exploring deep architectures comparing them common ground, identifying disclosing important implementation details. We identify several useful properties CNN-based representations, including fact that dimensionality output layer can be reduced without having an adverse effect performance. also aspects successfully shared. In particular, we show data augmentation techniques commonly applied to methods, result analogous performance boost. Source code models reproduce experiments made publicly available.

scene understanding, computer vision, neural computation, neuroscience, neural network (machine learning), adversarial machine learning, cognitive science, computer science, convolutional neural network, machine learning, machine learning research, convolutional nets, deep learning, feature construction",2014,2607,scene understanding|computer vision|neural computation|neuroscience|neural network (machine learning)|adversarial machine learning|cognitive science|computer science|convolutional neural network|machine learning|machine learning research|convolutional nets|deep learning|feature construction,https://openalex.org/W2156303437|https://openalex.org/W1677182931|https://openalex.org/W2325939864
https://openalex.org/W2170964688,AliView: a fast and lightweight alignment viewer and editor for large datasets,"AliView: a fast and lightweight alignment viewer and editor for large datasets

Abstract Summary: AliView is an alignment viewer and editor designed to meet the requirements of next-generation sequencing era phylogenetic datasets. handles alignments unlimited size in formats most commonly used, i.e. FASTA, Phylip, Nexus, Clustal MSF. The intuitive graphical interface makes it easy inspect, sort, delete, merge realign sequences as part manual filtering process large also works easy-to-use for small well Availability implementation: released open-source software under GNU General Public License, version 3.0 (GPLv3), available at GitHub ( www.github.com/AliView ). program cross-platform extensively tested on Linux, Mac OS X Windows systems. Downloads help are http://ormbunkar.se/aliview Contact: anders.larsson@ebc.uu.se Supplementary information: data Bioinformatics online.

image analysis, computer science, information fusion, large datasets, data integration, lightweight alignment viewer, large-scale datasets, computer vision, clustering, machine learning, data re-identification, data science, data set, statistics, machine learning research, benchmark datasets, sequence alignment, machine vision",2014,2600,image analysis|computer science|information fusion|large datasets|data integration|lightweight alignment viewer|large-scale datasets|computer vision|clustering|machine learning|data re-identification|data science|data set|statistics|machine learning research|benchmark datasets|sequence alignment|machine vision,
https://openalex.org/W2134670479,Learning Deep Features for Scene Recognition using Places Database,"Learning Deep Features for Scene Recognition using Places Database

Scene recognition is one of the hallmark tasks computer vision, allowing definition a context for object recognition. Whereas tremendous recent progress in due to availability large datasets like ImageNet and rise Convolutional Neural Networks (CNNs) learning high-level features, performance at scene has not attained same level success. This may be because current deep features trained from are competitive enough such tasks. Here, we introduce new scene-centric database called Places with over 7 million labeled pictures scenes. We propose methods compare density diversity image show that as dense other more diversity. Using CNN, learn tasks, establish state-of-the-art results on several datasets. A visualization CNN layers' responses allows us differences internal representations object-centric networks.

pattern recognition, computer science, feature learning, machine learning, deep features, image analysis, places database, information fusion, scene modeling, data science, image representation, computational imaging, scene understanding, deep learning, machine learning research, shift detection, machine vision, scene recognition, computer vision, geography, image database",2014,2547,pattern recognition|computer science|feature learning|machine learning|deep features|image analysis|places database|information fusion|scene modeling|data science|image representation|computational imaging|scene understanding|deep learning|machine learning research|shift detection|machine vision|scene recognition|computer vision|geography|image database,https://openalex.org/W1522734439|https://openalex.org/W3036224891|https://openalex.org/W3159481202|https://openalex.org/W3023371261|https://openalex.org/W4313156423
https://openalex.org/W2101032778,Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments,"Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments

We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems evaluating next generation pose estimation models algorithms. Besides increasing size datasets in current state-of-the-art several orders magnitude, we also aim to complement such with diverse set motions poses encountered as part typical activities (taking photos, talking on phone, posing, greeting, eating, etc.), additional synchronized image, motion capture, time flight (depth) data, body scans all subject actors involved. provide controlled mixed reality evaluation scenarios where are animated using capture inserted correct geometry, complex real environments, viewed moving cameras, occlusion. Finally, large-scale statistical detailed baselines dataset illustrating its diversity scope improvement future work research community. Our experiments show that our best model can leverage full obtain 20% compared scale largest existing public this problem. Yet potential leveraging higher capacity, more large is substantially vaster should stimulate research. The together code associated learning models, features, visualization tools, well server, available online at http://vision.imar.ro/human3.6m.

computer science, predictive methods, machine learning, natural environments, remote sensing, image analysis, information fusion, large-scale datasets, environmental science, localization, 3d object recognition, earth science, machine vision, geoscientific model development, digital earth, 3d computer vision, human pose estimation, computer vision, human geography, 3d pose estimation",2014,2534,computer science|predictive methods|machine learning|natural environments|remote sensing|image analysis|information fusion|large-scale datasets|environmental science|localization|3d object recognition|earth science|machine vision|geoscientific model development|digital earth|3d computer vision|human pose estimation|computer vision|human geography|3d pose estimation,
https://openalex.org/W2087681821,One millisecond face alignment with an ensemble of regression trees,"One millisecond face alignment with an ensemble of regression trees

This paper addresses the problem of Face Alignment for a single image. We show how an ensemble regression trees can be used to estimate face's landmark positions directly from sparse subset pixel intensities, achieving super-realtime performance with high quality predictions. present general framework based on gradient boosting learning that optimizes sum square error loss and naturally handles missing or partially labelled data. using appropriate priors exploiting structure image data helps efficient feature selection. Different regularization strategies its importance combat overfitting are also investigated. In addition, we analyse effect quantity training accuracy predictions explore augmentation synthesized

image analysis, pattern recognition, computer science, millisecond face alignment, computer vision, regression trees, facial recognition system, machine vision, face detection, facial expression recognition",2014,2469,image analysis|pattern recognition|computer science|millisecond face alignment|computer vision|regression trees|facial recognition system|machine vision|face detection|facial expression recognition,
https://openalex.org/W2171740948,Depth Map Prediction from a Single Image using a Multi-Scale Deep Network,"Depth Map Prediction from a Single Image using a Multi-Scale Deep Network

Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices estimation, finding relations from single image less straightforward, requiring integration both global and information various cues. Moreover, task inherently ambiguous, with large source uncertainty coming overall scale. In this paper, we present new method that addresses by employing two deep network stacks: one makes coarse prediction based on entire image, another refines locally. We also apply scale-invariant error to help measure rather than By leveraging raw datasets as sources training data, our achieves state-of-the-art results NYU Depth KITTI, matches detailed boundaries without need superpixelation.

computer science, multi-image fusion, information fusion, convolutional neural network, depth map prediction, digital image processing, single image, 3d vision, depth map, data science, deep learning, machine learning, computer vision, computational imaging, machine vision, image analysis, localization, multi-scale deep network, earth science, image representation, geometric learning",2014,2365,computer science|multi-image fusion|information fusion|convolutional neural network|depth map prediction|digital image processing|single image|3d vision|depth map|data science|deep learning|machine learning|computer vision|computational imaging|machine vision|image analysis|localization|multi-scale deep network|earth science|image representation|geometric learning,https://openalex.org/W1903029394|https://openalex.org/W764651262|https://openalex.org/W2963881378|https://openalex.org/W2395611524|https://openalex.org/W3081167590
https://openalex.org/W1903029394,Fully convolutional networks for semantic segmentation,"Fully convolutional networks for semantic segmentation

Convolutional networks are powerful visual models that yield hierarchies of features. We show convolutional by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ""fully convolutional"" take input arbitrary size and produce correspondingly-sized output with efficient inference learning. define detail space fully networks, explain their application spatially dense prediction tasks, draw connections prior models. adapt contemporary classification (AlexNet [20], VGG net [31], GoogLeNet [32]) into transfer learned representations fine-tuning [3] segmentation task. then a skip architecture combines information from deep, coarse layer appearance shallow, fine accurate detailed segmentations. network achieves PASCAL VOC (20% relative improvement 62.2% mean IU on 2012), NYUDv2, SIFT Flow, while takes less than one fifth second for typical image.

pattern recognition, computer science, machine learning, image segmentation, image analysis, information fusion, convolutional neural network, convolutional networks, cognitive science, data science, computational imaging, scene understanding, deep learning, machine learning research, machine vision, semantic segmentation, medical image computing, feature extraction, computer vision, scene analysis",2015,28292,pattern recognition|computer science|machine learning|image segmentation|image analysis|information fusion|convolutional neural network|convolutional networks|cognitive science|data science|computational imaging|scene understanding|deep learning|machine learning research|machine vision|semantic segmentation|medical image computing|feature extraction|computer vision|scene analysis,https://openalex.org/W2953106684|https://openalex.org/W2613718673|https://openalex.org/W1745334888|https://openalex.org/W764651262|https://openalex.org/W2183341477|https://openalex.org/W2962914239|https://openalex.org/W2476548250|https://openalex.org/W2475287302|https://openalex.org/W2963420272|https://openalex.org/W2963446712|https://openalex.org/W639708223|https://openalex.org/W2963351448|https://openalex.org/W2565639579|https://openalex.org/W2962793481|https://openalex.org/W2963073614|https://openalex.org/W2963881378|https://openalex.org/W2962858109|https://openalex.org/W2560023338|https://openalex.org/W2395611524|https://openalex.org/W3018757597|https://openalex.org/W3035524453|https://openalex.org/W2884561390|https://openalex.org/W2996290406|https://openalex.org/W3015788359|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W3023371261|https://openalex.org/W4214893857|https://openalex.org/W4312815172|https://openalex.org/W2938260698|https://openalex.org/W3130754787
https://openalex.org/W2953106684,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

State-of-the-art object detection networks depend on region proposal algorithms to hypothesize locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these networks, exposing computation as a bottleneck. In this work, we introduce Region Proposal Network (RPN) that shares full-image convolutional features with network, thus enabling nearly cost-free proposals. An RPN is fully network simultaneously predicts bounds objectness scores at each position. The trained end-to-end generate high-quality proposals, which are used by for detection. We further merge into single sharing their features---using recently popular terminology neural 'attention' mechanisms, component tells unified where look. For very deep VGG-16 model, our system has frame rate 5fps (including all steps) GPU, while achieving state-of-the-art accuracy PASCAL VOC 2007, 2012, MS COCO datasets only 300 proposals per image. ILSVRC 2015 competitions, Faster foundations 1st-place winning entries in several tracks. Code been made publicly available.

vehicular technology, computer science, deep learning, pattern recognition, information fusion, machine vision, object detection, scene understanding, object recognition, region proposal networks, computational imaging, machine learning, localization, data science, computer vision, cognitive science, object tracking, scene analysis, image analysis",2015,18973,vehicular technology|computer science|deep learning|pattern recognition|information fusion|machine vision|object detection|scene understanding|object recognition|region proposal networks|computational imaging|machine learning|localization|data science|computer vision|cognitive science|object tracking|scene analysis|image analysis,https://openalex.org/W2963037989|https://openalex.org/W2963351448|https://openalex.org/W2565639579|https://openalex.org/W2612445135|https://openalex.org/W2963524571|https://openalex.org/W3035524453|https://openalex.org/W3034971973|https://openalex.org/W3035574168|https://openalex.org/W2962730651|https://openalex.org/W3131500599|https://openalex.org/W2898200825|https://openalex.org/W3210586215|https://openalex.org/W4312815172
https://openalex.org/W1677182931,Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier networks image classification from two aspects. First, propose a Parametric Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, derive robust initialization method particularly considers nonlinearities. This enables us to train extremely deep models directly scratch investigate deeper or wider network architectures. Based on learnable advanced initialization, achieve 4.94% top-5 test error ImageNet 2012 dataset. is 26% relative improvement over ILSVRC 2014 winner (GoogLeNet, 6.66% [33]). To our knowledge, result first surpass reported human-level performance (5.1%, [26])

digital image processing, sparse neural network, imagenet classification, computer science, machine learning research, deep learning, machine vision, adversarial machine learning, image representation, human-level performance, human image synthesis, computational imaging, geometric learning, machine learning, data science, computer vision, cognitive science, computational intelligence, convolutional neural network, image analysis",2015,14420,digital image processing|sparse neural network|imagenet classification|computer science|machine learning research|deep learning|machine vision|adversarial machine learning|image representation|human-level performance|human image synthesis|computational imaging|geometric learning|machine learning|data science|computer vision|cognitive science|computational intelligence|convolutional neural network|image analysis,https://openalex.org/W2183341477|https://openalex.org/W2962914239|https://openalex.org/W2242218935|https://openalex.org/W3101998545|https://openalex.org/W2401231614|https://openalex.org/W2963446712|https://openalex.org/W2963881378|https://openalex.org/W2963470893|https://openalex.org/W2508457857|https://openalex.org/W3018757597|https://openalex.org/W3035574324|https://openalex.org/W3035574168|https://openalex.org/W2799041689
https://openalex.org/W2613718673,Faster R-CNN: towards real-time object detection with region proposal networks,"Faster R-CNN: towards real-time object detection with region proposal networks

State-of-the-art object detection networks depend on region proposal algorithms to hypothesize locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these networks, exposing computation as a bottleneck. In this work, we introduce Region Proposal Network (RPN) that shares full-image convolutional features with network, thus enabling nearly cost-free proposals. An RPN is fully-convolutional network simultaneously predicts bounds objectness scores at each position. RPNs are trained end-to-end generate high-quality proposals, which used by for detection. With simple alternating optimization, can be share features. For very deep VGG-16 model [19], our system has frame rate 5fps (including all steps) GPU, while achieving state-of-the-art accuracy PASCAL VOC 2007 (73.2% mAP) 2012 (70.4% using 300 proposals per image. Code available https://github.com/ShaoqingRen/faster_rcnn.

computer science, information fusion, vehicular technology, scene analysis, cognitive science, data science, deep learning, pattern recognition, machine learning, computer vision, object recognition, computational imaging, object detection, scene understanding, object tracking, region proposal networks, machine vision, image analysis, localization",2015,13298,computer science|information fusion|vehicular technology|scene analysis|cognitive science|data science|deep learning|pattern recognition|machine learning|computer vision|object recognition|computational imaging|object detection|scene understanding|object tracking|region proposal networks|machine vision|image analysis|localization,https://openalex.org/W2963351448|https://openalex.org/W2565639579|https://openalex.org/W2963524571|https://openalex.org/W3018757597|https://openalex.org/W3035524453|https://openalex.org/W3034971973|https://openalex.org/W2884561390|https://openalex.org/W3035574168|https://openalex.org/W3122239467|https://openalex.org/W2962730651|https://openalex.org/W3131500599|https://openalex.org/W3023371261|https://openalex.org/W2898200825|https://openalex.org/W2938260698
https://openalex.org/W2109255472,Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition

Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224 × 224) input image. This requirement is ""artificial"" and may reduce the recognition accuracy for images or sub-images of an arbitrary size/scale. In this work, we equip with another pooling strategy, ""spatial pyramid pooling"", to eliminate above requirement. The new network structure, called SPP-net, can generate fixed-length representation regardless image Pyramid also robust object deformations. With these advantages, SPP-net should in general improve all CNN-based classification methods. On ImageNet 2012 dataset, demonstrate that boosts variety CNN architectures despite their different designs. Pascal VOC 2007 Caltech101 datasets, achieves state-of-the-art results using single full-image no fine-tuning. power significant detection. Using compute feature maps from entire only once, then pool features regions (sub-images) representations training detectors. method avoids repeatedly computing features. processing test images, our 24-102 faster than R-CNN method, while achieving better comparable on 2007. Large Scale Visual Recognition Challenge (ILSVRC) 2014, methods rank #2 detection #3 among 38 teams. manuscript introduces improvement made competition.

computer science, machine learning, visual recognition, image analysis, information fusion, convolutional neural network, feature detection, cognitive science, data science, object categorization, image representation, vision recognition, computational imaging, deep convolutional networks, deep learning, spatial pyramid pooling, machine learning research, machine vision, digital image processing, computer vision, geometric learning",2015,8381,computer science|machine learning|visual recognition|image analysis|information fusion|convolutional neural network|feature detection|cognitive science|data science|object categorization|image representation|vision recognition|computational imaging|deep convolutional networks|deep learning|spatial pyramid pooling|machine learning research|machine vision|digital image processing|computer vision|geometric learning,https://openalex.org/W2963037989|https://openalex.org/W639708223|https://openalex.org/W2560023338|https://openalex.org/W3018757597|https://openalex.org/W3210586215
https://openalex.org/W1522734439,Learning Spatiotemporal Features with 3D Convolutional Networks,"Learning Spatiotemporal Features with 3D Convolutional Networks

We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets more suitable compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures and 3) learned features, namely C3D (Convolutional 3D), simple linear classifier outperform state-of-the-art methods 4 different benchmarks comparable current other 2 benchmarks. In addition, features compact: achieving 52.8% accuracy UCF101 dataset only 10 dimensions also very efficient compute due fast inference of ConvNets. Finally, they conceptually easy train use.

image analysis, computer science, convolutional neural network, structure from motion, convolutional networks, spatiotemporal features, computer vision, machine learning, spatio-temporal model, data science, deep learning, big spatiotemporal data analytics, machine vision, spatialtemporal reasoning",2015,7330,image analysis|computer science|convolutional neural network|structure from motion|convolutional networks|spatiotemporal features|computer vision|machine learning|spatio-temporal model|data science|deep learning|big spatiotemporal data analytics|machine vision|spatialtemporal reasoning,https://openalex.org/W2476548250|https://openalex.org/W2963524571|https://openalex.org/W3023371261|https://openalex.org/W4214612132|https://openalex.org/W2799041689|https://openalex.org/W4312560592
https://openalex.org/W2963840672,Multi-Scale Context Aggregation by Dilated Convolutions,"Multi-Scale Context Aggregation by Dilated Convolutions

State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed image classification. However, dense prediction and classification structurally different. In this work, we develop a new network module is specifically prediction. The presented uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. architecture the fact support exponential expansion receptive field loss resolution or coverage. We show context increases accuracy state-of-the-art systems. addition, examine adaptation simplifying adapted can increase accuracy.

dilated convolutions, scene understanding, computer vision, machine vision, multi-scale context aggregation, natural language processing, computer science, convolutional neural network, machine learning, deep learning, data science, scene analysis, image analysis",2015,6319,dilated convolutions|scene understanding|computer vision|machine vision|multi-scale context aggregation|natural language processing|computer science|convolutional neural network|machine learning|deep learning|data science|scene analysis|image analysis,https://openalex.org/W2963881378|https://openalex.org/W2560023338|https://openalex.org/W2395611524|https://openalex.org/W3132455321|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W4285531802|https://openalex.org/W4214893857
https://openalex.org/W1834627138,Deep Learning Face Attributes in the Wild,"Deep Learning Face Attributes in the Wild

Predicting face attributes in the wild is challenging due to complex variations. We propose a novel deep learning framework for attribute prediction wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with tags, but pre-trained differently. by massive general object categories localization, while ANet identities prediction. This not only outperforms state-of-the-art large margin, also reveals valuable facts on representation. (1) shows how performances of localization (LNet) (ANet) can be improved different pre-training strategies. (2) that although filters image-level their response maps over entire images have strong indication locations. fact enables training annotations, without bounding boxes or landmarks, required all recognition works. (3) demonstrates high-level hidden neurons automatically discover semantic concepts after identities, such significantly enriched fine-tuning tags. Each well explained sparse linear combination these concepts.

image analysis, pattern recognition, computer science, computational imaging, scene interpretation, computer vision, machine learning, face attributes, data science, computational intelligence, deep learning, human image synthesis, style transfer, facial recognition system, machine vision, face detection",2015,6114,image analysis|pattern recognition|computer science|computational imaging|scene interpretation|computer vision|machine learning|face attributes|data science|computational intelligence|deep learning|human image synthesis|style transfer|facial recognition system|machine vision|face detection,https://openalex.org/W3101998545|https://openalex.org/W3013529009
https://openalex.org/W1514535095,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We how can train this a deterministic manner using standard backpropagation techniques stochastically maximizing variational lower bound. also show through visualization is able learn fix its gaze on salient objects while generating corresponding words output sequence. validate use with state-of-the-art performance three benchmark datasets: Flickr9k, Flickr30k MS COCO.

vision language model, computer science, machine learning research, deep learning, language generation, information fusion, natural language processing, machine translation, natural language generation, scene understanding, scene interpretation, visual perception, data science, computer vision, retrieval augmented generation, cognitive science, visual attention, image communication, image analysis",2015,5247,vision language model|computer science|machine learning research|deep learning|language generation|information fusion|natural language processing|machine translation|natural language generation|scene understanding|scene interpretation|visual perception|data science|computer vision|retrieval augmented generation|cognitive science|visual attention|image communication|image analysis,https://openalex.org/W3012494314|https://openalex.org/W3212386989|https://openalex.org/W2938260698
https://openalex.org/W3194700752,"You Only Look Once: Unified, Real-Time Object Detection","You Only Look Once: Unified, Real-Time Object Detection

We present YOLO, a new approach to object detection. Prior work on detection repurposes classifiers perform Instead, we frame as regression problem spatially separated bounding boxes and associated class probabilities. A single neural network predicts probabilities directly from full images in one evaluation. Since the whole pipeline is network, it can be optimized end-to-end performance. Our unified architecture extremely fast. base YOLO model processes real-time at 45 frames per second. smaller version of Fast an astounding 155 second while still achieving double mAP other detectors. Compared state-of-the-art systems, makes more localization errors but far less likely predict false detections where nothing exists. Finally, learns very general representations objects. It outperforms all methods, including DPM R-CNN, by wide margin when generalizing natural artwork both Picasso Dataset People-Art Dataset.

scene understanding, computer vision, pattern recognition, computational imaging, information fusion, machine vision, motion detection, computer science, object detection, machine learning, feature detection, image representation, real-time object detection, deep learning, data science, object tracking, image analysis",2015,4907,scene understanding|computer vision|pattern recognition|computational imaging|information fusion|machine vision|motion detection|computer science|object detection|machine learning|feature detection|image representation|real-time object detection|deep learning|data science|object tracking|image analysis,
https://openalex.org/W1947481528,Long-term recurrent convolutional networks for visual recognition and description,"Long-term recurrent convolutional networks for visual recognition and description

Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or ""temporally deep"", effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent architecture suitable large-scale learning is end-to-end trainable, demonstrate the value of these benchmark video recognition tasks, description retrieval problems, narration challenges. In contrast to current assume fixed spatio-temporal receptive field simple temporal averaging sequential processing, ""doubly deep"" in that they can be compositional spatial ""layers"". Such may advantages when target concepts complex and/or training data limited. Learning long-term dependencies possible nonlinearities incorporated into network state updates. Long-term RNN appealing directly map variable-length inputs (e.g., frames) variable length outputs natural language text) model dynamics; yet optimized with backpropagation. Our connected modern convnet jointly trained simultaneously learn dynamics perceptual representations. results show such distinct over state-of-the-art generation separately defined optimized.

pattern recognition, computer science, machine learning, visual recognition, image analysis, information fusion, convolutional neural network, recurrent neural network, cognitive science, data science, image representation, vision recognition, computational imaging, scene understanding, deep learning, machine learning research, machine vision, computer vision, image classification",2015,4855,pattern recognition|computer science|machine learning|visual recognition|image analysis|information fusion|convolutional neural network|recurrent neural network|cognitive science|data science|image representation|vision recognition|computational imaging|scene understanding|deep learning|machine learning research|machine vision|computer vision|image classification,https://openalex.org/W1522734439|https://openalex.org/W1514535095|https://openalex.org/W1905882502|https://openalex.org/W1956340063|https://openalex.org/W2963524571|https://openalex.org/W3023371261|https://openalex.org/W2799041689
https://openalex.org/W2950178297,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Show, Attend and Tell: Neural Image Caption Generation with Visual Attention

Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We how can train this a deterministic manner using standard backpropagation techniques stochastically maximizing variational lower bound. also show through visualization is able learn fix its gaze on salient objects while generating corresponding words output sequence. validate use with state-of-the-art performance three benchmark datasets: Flickr8k, Flickr30k MS COCO.

image communication, computer science, information fusion, scene interpretation, cognitive science, visual perception, vision language model, data science, deep learning, computer vision, language generation, machine learning research, scene understanding, natural language processing, retrieval augmented generation, image analysis, visual attention, natural language generation, machine translation",2015,4667,image communication|computer science|information fusion|scene interpretation|cognitive science|visual perception|vision language model|data science|deep learning|computer vision|language generation|machine learning research|scene understanding|natural language processing|retrieval augmented generation|image analysis|visual attention|natural language generation|machine translation,https://openalex.org/W3012494314|https://openalex.org/W2938260698
https://openalex.org/W2325939864,Deep Face Recognition,"Deep Face Recognition

The goal of this paper is face recognition – from either a single photograph or set faces tracked in video. Recent progress area has been due to two factors: (i) end learning for the task using convolutional neural network (CNN), and (ii) availability very large scale training datasets. We make contributions: first, we show how dataset (2.6M images, over 2.6K people) can be assembled by combination automation human loop, discuss trade off between data purity time; second, traverse through complexities deep present methods procedures achieve comparable state art results on standard LFW YTF benchmarks.

pattern recognition, computer science, feature learning, machine learning, image analysis, information fusion, convolutional neural network, feature detection, cognitive science, data science, image representation, computational imaging, deep learning, machine learning research, deep face recognition, machine vision, object recognition, feature extraction, computer vision",2015,4637,pattern recognition|computer science|feature learning|machine learning|image analysis|information fusion|convolutional neural network|feature detection|cognitive science|data science|image representation|computational imaging|deep learning|machine learning research|deep face recognition|machine vision|object recognition|feature extraction|computer vision,https://openalex.org/W2799041689
https://openalex.org/W1905882502,Deep visual-semantic alignments for generating image descriptions,"Deep visual-semantic alignments for generating image descriptions

We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets sentence to learn about the inter-modal correspondences between visual data. alignment is based on novel combination Convolutional Neural Networks over image regions, bidirectional Recurrent sentences, structured objective aligns two modalities through multimodal embedding. then describe Multimodal Network architecture uses inferred alignments generate demonstrate our produces state art results in retrieval experiments Flickr8K, Flickr30K MSCOCO datasets. show generated significantly outperform baselines both full new dataset region-level annotations.

image analysis, pattern recognition, computer science, computational imaging, deep visual-semantic alignments, image descriptions, scene interpretation, scene understanding, vision language model, computer vision, machine learning, feature detection, deep learning, image representation, machine vision, visual question answering",2015,4409,image analysis|pattern recognition|computer science|computational imaging|deep visual-semantic alignments|image descriptions|scene interpretation|scene understanding|vision language model|computer vision|machine learning|feature detection|deep learning|image representation|machine vision|visual question answering,https://openalex.org/W1514535095|https://openalex.org/W2950178297|https://openalex.org/W1956340063|https://openalex.org/W2962858109
https://openalex.org/W1920022804,3D ShapeNets: A deep representation for volumetric shapes,"3D ShapeNets: A deep representation for volumetric shapes

3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of good generic representation. With recent availability inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it becoming increasingly important have powerful representation loop. Apart from category recognition, recovering full shapes view-based maps also critical part visual understanding. To this end, we propose represent geometric as probability distribution binary variables on voxel grid, using Convolutional Deep Belief Network. Our model, ShapeNets, learns complex across different object categories and arbitrary poses raw CAD data, discovers hierarchical compositional automatically. It naturally supports joint recognition completion maps, enables active through view planning. train our deep learning construct ModelNet - large-scale model dataset. Extensive experiments show that significant performance improvement over the-state-of-the-arts variety tasks.

computer science, geometric modeling, 3d modeling, image analysis, deep representation, 3d reconstruction, volume rendering, volumetric shapes, computational geometry, computational imaging, computer-aided design, additive manufacturing, numerical simulation, machine vision, shape modeling, computer vision, applied mathematics, geometric learning, applied physics, geometry",2015,3850,computer science|geometric modeling|3d modeling|image analysis|deep representation|3d reconstruction|volume rendering|volumetric shapes|computational geometry|computational imaging|computer-aided design|additive manufacturing|numerical simulation|machine vision|shape modeling|computer vision|applied mathematics|geometric learning|applied physics|geometry,https://openalex.org/W2560609797|https://openalex.org/W3023371261
https://openalex.org/W2204750386,Scalable Person Re-identification: A Benchmark,"Scalable Person Re-identification: A Benchmark

This paper contributes a new high quality dataset for person re-identification, named ""Market-1501"". Generally, current datasets: 1) are limited in scale, 2) consist of hand-drawn bboxes, which unavailable under realistic settings, 3) have only one ground truth and query image each identity (close environment). To tackle these problems, the proposed Market-1501 is featured three aspects. First, it contains over 32,000 annotated plus distractor set 500K images, making largest re-id to date. Second, images produced using Deformable Part Model (DPM) as pedestrian detector. Third, our collected an open system, where has multiple camera. As minor contribution, inspired by recent advances large-scale search, this proposes unsupervised Bag-of-Words descriptor. We view re-identification special task search. In experiment, we show that descriptor yields competitive accuracy on VIPeR, CUHK03, datasets, scalable 500k dataset.

image analysis, pattern recognition, computer science, information fusion, scalable person re-identification, domain adaptation, computer vision, machine learning, data re-identification, cognitive science, identification method, data science, similarity search, deep learning, machine learning research, human identification, machine vision",2015,3544,image analysis|pattern recognition|computer science|information fusion|scalable person re-identification|domain adaptation|computer vision|machine learning|data re-identification|cognitive science|identification method|data science|similarity search|deep learning|machine learning research|human identification|machine vision,
https://openalex.org/W1745334888,Learning Deconvolution Network for Semantic Segmentation,"Learning Deconvolution Network for Semantic Segmentation

We propose a novel semantic segmentation algorithm by learning deep deconvolution network. learn the network on top of convolutional layers adopted from VGG 16-layer net. The is composed and unpooling layers, which identify pixelwise class labels predict masks. apply trained to each proposal in an input image, construct final map combining results all proposals simple manner. proposed mitigates limitations existing methods based fully networks integrating proposal-wise prediction, our method typically identifies detailed structures handles objects multiple scales naturally. Our demonstrates outstanding performance PASCAL VOC 2012 dataset, we achieve best accuracy (72.5%) among without using Microsoft COCO dataset through ensemble with

computer science, information fusion, scene analysis, scene interpretation, cognitive science, digital image processing, image segmentation, semantic segmentation, data science, deep learning, feature extraction, deconvolution network, machine learning, computer vision, biomedical imaging, computational imaging, machine learning research, scene understanding, machine vision, image analysis",2015,3525,computer science|information fusion|scene analysis|scene interpretation|cognitive science|digital image processing|image segmentation|semantic segmentation|data science|deep learning|feature extraction|deconvolution network|machine learning|computer vision|biomedical imaging|computational imaging|machine learning research|scene understanding|machine vision|image analysis,https://openalex.org/W2962914239|https://openalex.org/W2963881378|https://openalex.org/W2560023338|https://openalex.org/W2395611524|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W4285531802
https://openalex.org/W764651262,FlowNet: Learning Optical Flow with Convolutional Networks,"FlowNet: Learning Optical Flow with Convolutional Networks

Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not among the tasks CNNs succeeded at. In this paper we construct which are capable solving optical problem as supervised learning task. We propose and compare two architectures: generic architecture another one including layer that correlates feature vectors at different image locations. Since existing ground truth data sets sufficiently large train CNN, generate synthetic Flying Chairs dataset. show trained unrealistic still generalize well datasets such Sintel KITTI, achieving competitive accuracy frame rates 5 10 fps.

image analysis, computer science, computer vision, machine learning, image classification, convolutional networks, scene understanding, convolutional neural network, data science, deep learning, learning optical flow, machine vision",2015,3416,image analysis|computer science|computer vision|machine learning|image classification|convolutional networks|scene understanding|convolutional neural network|data science|deep learning|learning optical flow|machine vision,https://openalex.org/W2395611524|https://openalex.org/W3023371261
https://openalex.org/W1956340063,CIDEr: Consensus-based image description evaluation,"CIDEr: Consensus-based image description evaluation

Automatically describing an image with a sentence is long-standing challenge in computer vision and natural language processing. Due to recent progress object detection, attribute classification, action recognition, etc., there renewed interest this area. However, evaluating the quality of descriptions has proven be challenging. We propose novel paradigm for that uses human consensus. This consists three main parts: new triplet-based method collecting annotations measure consensus, automated metric captures two datasets: PASCAL-50S ABSTRACT-50S contain 50 sentences each image. Our simple judgment consensus better than existing metrics across generated by various sources. also evaluate five state-of-the-art description approaches using protocol provide benchmark future comparisons. A version CIDEr named CIDEr-D available as part MS COCO evaluation server enable systematic benchmarking.

image analysis, computational imaging, computer science, machine vision, content-based image retrieval, image retrieval, computer vision, semantic evaluation, knowledge discovery, deep learning, image representation, information retrieval, digital image processing",2015,3387,image analysis|computational imaging|computer science|machine vision|content-based image retrieval|image retrieval|computer vision|semantic evaluation|knowledge discovery|deep learning|image representation|information retrieval|digital image processing,https://openalex.org/W1905882502|https://openalex.org/W1956340063
https://openalex.org/W2963037989,"You Only Look Once: Unified, Real-Time Object Detection","You Only Look Once: Unified, Real-Time Object Detection

We present YOLO, a new approach to object detection. Prior work on detection repurposes classifiers perform Instead, we frame as regression problem spatially separated bounding boxes and associated class probabilities. A single neural network predicts probabilities directly from full images in one evaluation. Since the whole pipeline is network, it can be optimized end-to-end performance. Our unified architecture extremely fast. base YOLO model processes real-time at 45 frames per second. smaller version of Fast an astounding 155 second while still achieving double mAP other detectors. Compared state-of-the-art systems, makes more localization errors but less likely predict false positives background. Finally, learns very general representations objects. It outperforms methods, including DPM R-CNN, when generalizing natural domains like artwork.

image analysis, pattern recognition, computer science, computational imaging, information fusion, object tracking, scene understanding, computer vision, machine learning, feature detection, motion detection, object detection, real-time object detection, data science, deep learning, image representation, machine vision",2016,26723,image analysis|pattern recognition|computer science|computational imaging|information fusion|object tracking|scene understanding|computer vision|machine learning|feature detection|motion detection|object detection|real-time object detection|data science|deep learning|image representation|machine vision,https://openalex.org/W2963351448|https://openalex.org/W3018757597|https://openalex.org/W2884561390|https://openalex.org/W3023371261|https://openalex.org/W3210586215|https://openalex.org/W3136761610
https://openalex.org/W2183341477,Rethinking the Inception Architecture for Computer Vision,"Rethinking the Inception Architecture for Computer Vision

Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety tasks. Since 2014 very deep convolutional started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend translate immediate quality tasks (as long as enough labeled data is provided training), efficiency low parameter count still enabling factors use cases such mobile big-data scenarios. Here we exploring ways scale up that aim utilizing added computation efficiently possible by suitably factorized convolutions aggressive regularization. We benchmark our methods on ILSVRC 2012 classification challenge validation set demonstrate over art: 21:2% top-1 5:6% top-5 error single frame evaluation using network with 5 billion multiply-adds per inference less than 25 million parameters. With an ensemble 4 models multi-crop evaluation, report 3:5% 17:3% 3:6% official test set.

image analysis, computational imaging, computer science, artificial intelligence, object recognition, scene understanding, computer vision, machine learning, feature detection, inception architecture, object detection, deep learning, image representation, vision recognition, machine vision",2016,23508,image analysis|computational imaging|computer science|artificial intelligence|object recognition|scene understanding|computer vision|machine learning|feature detection|inception architecture|object detection|deep learning|image representation|vision recognition|machine vision,https://openalex.org/W2963446712|https://openalex.org/W2612445135|https://openalex.org/W3018757597|https://openalex.org/W3035160371|https://openalex.org/W3131500599|https://openalex.org/W3121523901|https://openalex.org/W3167976421|https://openalex.org/W4214612132|https://openalex.org/W3157506437|https://openalex.org/W4312443924|https://openalex.org/W4313156423|https://openalex.org/W2799041689
https://openalex.org/W1885185971,Image Super-Resolution Using Deep Convolutional Networks,"Image Super-Resolution Using Deep Convolutional Networks

We propose a deep learning method for single image super-resolution (SR). Our directly learns an end-to-end mapping between the low/high-resolution images. The is represented as convolutional neural network (CNN) that takes low-resolution input and outputs high-resolution one. further show traditional sparse-coding-based SR methods can also be viewed network. But unlike handle each component separately, our jointly optimizes all layers. CNN has lightweight structure, yet demonstrates state-of-the-art restoration quality, achieves fast speed practical on-line usage. explore different structures parameter settings to achieve trade-offs performance speed. Moreover, we extend cope with three color channels simultaneously, better overall reconstruction quality.

computer science, high resolution, machine learning, image analysis, convolutional neural network, biomedical imaging, data science, image representation, computational imaging, deep convolutional networks, deep learning, image super-resolution, image resolution, machine vision, single-image super-resolution, digital image processing, medical image computing, super-resolution imaging, computer vision",2016,7136,computer science|high resolution|machine learning|image analysis|convolutional neural network|biomedical imaging|data science|image representation|computational imaging|deep convolutional networks|deep learning|image super-resolution|image resolution|machine vision|single-image super-resolution|digital image processing|medical image computing|super-resolution imaging|computer vision,https://openalex.org/W2242218935|https://openalex.org/W2476548250|https://openalex.org/W2963470893|https://openalex.org/W3013529009|https://openalex.org/W3155072588|https://openalex.org/W3167568784
https://openalex.org/W2962914239,V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation,"V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation

Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able process 2D images while data used in clinical practice consists of 3D volumes. In this work we propose an approach segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end MRI volumes depicting prostate, learns predict for whole volume at once. We introduce novel objective function, that optimise during training, Dice coefficient. way can deal with situations where there strong imbalance between number foreground background voxels. To cope limited annotated available augment applying random non-linear transformations histogram matching. show our experimental evaluation achieves good performances challenging test requiring fraction processing time needed by other previous methods.

image segmentation, computed tomography, digital image processing, medical image computing, computer vision, radiology, biomedical imaging, computational imaging, machine vision, medical image analysis, biomedical engineering, medical imaging, computer science, digital medicine, convolutional neural network, machine learning, deep learning, 3d imaging",2016,6563,image segmentation|computed tomography|digital image processing|medical image computing|computer vision|radiology|biomedical imaging|computational imaging|machine vision|medical image analysis|biomedical engineering|medical imaging|computer science|digital medicine|convolutional neural network|machine learning|deep learning|3d imaging,https://openalex.org/W2996290406|https://openalex.org/W3132455321|https://openalex.org/W4212875960|https://openalex.org/W4312815172
https://openalex.org/W2174661749,"Radiomics: Images Are More than Pictures, They Are Data","Radiomics: Images Are More than Pictures, They Are Data

In the past decade, field of medical image analysis has grown exponentially, with an increased number pattern recognition tools and increase in data set sizes. These advances have facilitated development processes for high-throughput extraction quantitative features that result conversion images into mineable subsequent these decision support; this practice is termed radiomics. This contrast to traditional treating as pictures intended solely visual interpretation. Radiomic contain first-, second-, higher-order statistics. are combined other patient mined sophisticated bioinformatics develop models may potentially improve diagnostic, prognostic, predictive accuracy. Because radiomics analyses be conducted standard care images, it conceivable digital will eventually become routine practice. report describes process radiomics, its challenges, potential power facilitate better clinical making, particularly patients cancer.

image analysis, pattern recognition, computer science, computational imaging, content-based image retrieval, radiomics, image communication, computer vision, data and information visualization, image sequence analysis, data science, image representation, radiographic imaging, machine learning research, machine vision, digital image processing, image database",2016,5990,image analysis|pattern recognition|computer science|computational imaging|content-based image retrieval|radiomics|image communication|computer vision|data and information visualization|image sequence analysis|data science|image representation|radiographic imaging|machine learning research|machine vision|digital image processing|image database,
https://openalex.org/W2242218935,Accurate Image Super-Resolution Using Very Deep Convolutional Networks,"Accurate Image Super-Resolution Using Very Deep Convolutional Networks

We present a highly accurate single-image superresolution (SR) method. Our method uses very deep convolutional network inspired by VGG-net used for ImageNet classification [19]. find increasing our depth shows significant improvement in accuracy. final model 20 weight layers. By cascading small filters many times structure, contextual information over large image regions is exploited an efficient way. With networks, however, convergence speed becomes critical issue during training. propose simple yet effective training procedure. learn residuals only and use extremely high learning rates (104 higher than SRCNN [6]) enabled adjustable gradient clipping. proposed performs better existing methods accuracy visual improvements results are easily noticeable.

image analysis, computational imaging, computer science, convolutional neural network, deep convolutional networks, super-resolution imaging, biomedical imaging, computer vision, machine learning, high resolution, single-image super-resolution, data science, deep learning, image representation, image resolution, machine vision, digital image processing, accurate image super-resolution",2016,5855,image analysis|computational imaging|computer science|convolutional neural network|deep convolutional networks|super-resolution imaging|biomedical imaging|computer vision|machine learning|high resolution|single-image super-resolution|data science|deep learning|image representation|image resolution|machine vision|digital image processing|accurate image super-resolution,https://openalex.org/W2508457857|https://openalex.org/W2963372104|https://openalex.org/W3013529009|https://openalex.org/W3121661546|https://openalex.org/W3155072588
https://openalex.org/W2476548250,Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network,"Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network

Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input is upscaled to high (HR) space using a filter, commonly bicubic interpolation, before reconstruction. This means that super-resolution (SR) operation performed HR space. We demonstrate this sub-optimal adds complexity. paper, we present first convolutional network (CNN) capable real-time SR 1080p videos K2 GPU. To achieve this, propose novel CNN architecture where feature maps are extracted LR addition, introduce an efficient sub-pixel convolution layer which learns array upscaling filters upscale final into output. By doing so, effectively replace handcrafted filter pipeline with more complex specifically trained each map, whilst also reducing complexity overall operation. evaluate proposed approach images from publicly available datasets show it performs significantly better (+0.15dB Images +0.39dB Videos) order magnitude faster than previous CNN-based methods.

image resolution, digital image processing, computer vision, biomedical imaging, super-resolution imaging, computational imaging, machine vision, real-time single image, single-image super-resolution, video super-resolution, computer science, convolutional neural network, machine learning, deep learning, data science, computer graphic, image analysis",2016,5071,image resolution|digital image processing|computer vision|biomedical imaging|super-resolution imaging|computational imaging|machine vision|real-time single image|single-image super-resolution|video super-resolution|computer science|convolutional neural network|machine learning|deep learning|data science|computer graphic|image analysis,https://openalex.org/W2963470893|https://openalex.org/W2963372104|https://openalex.org/W3013529009
https://openalex.org/W3101998545,Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks,"Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks

Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a cascaded multi-task framework which exploits the inherent correlation between them boost up their performance. particular, our adopts structure with three stages of carefully designed convolutional networks predict face landmark location coarse-to-fine manner. addition, process, new online hard sample mining strategy improve automatically without manual selection. Our method achieves superior accuracy over state-of-the-art techniques FDDB WIDER FACE benchmark for detection, AFLW alignment, while keeps real time

pattern recognition, computer science, machine learning, joint face detection, image analysis, information fusion, convolutional neural network, convolutional networks, multi-task learning, feature detection, cognitive science, object detection, data science, computational imaging, deep learning, machine learning research, machine vision, face detection, computer vision",2016,4888,pattern recognition|computer science|machine learning|joint face detection|image analysis|information fusion|convolutional neural network|convolutional networks|multi-task learning|feature detection|cognitive science|object detection|data science|computational imaging|deep learning|machine learning research|machine vision|face detection|computer vision,https://openalex.org/W3035574168|https://openalex.org/W2799041689
https://openalex.org/W2475287302,Image Style Transfer Using Convolutional Neural Networks,"Image Style Transfer Using Convolutional Neural Networks

Rendering the semantic content of an image in different styles is a difficult processing task. Arguably, major limiting factor for previous approaches has been lack representations that explicitly represent information and, thus, allow to separate from style. Here we use derived Convolutional Neural Networks optimised object recognition, which make high level explicit. We introduce A Algorithm Artistic Style can and recombine style natural images. The algorithm allows us produce new images perceptual quality combine arbitrary photograph with appearance numerous wellknown artworks. Our results provide insights into deep learned by demonstrate their potential synthesis manipulation.

computer science, machine learning, image manipulation, synthetic image generation, image analysis, convolutional neural network, cognitive science, style transfer, image representation, computational imaging, transfer learning, image communication, deep learning, image style transfer, machine vision, digital image processing, computer vision, multimedia retrieval, art",2016,4876,computer science|machine learning|image manipulation|synthetic image generation|image analysis|convolutional neural network|cognitive science|style transfer|image representation|computational imaging|transfer learning|image communication|deep learning|image style transfer|machine vision|digital image processing|computer vision|multimedia retrieval|art,https://openalex.org/W2962793481|https://openalex.org/W2963073614|https://openalex.org/W2963470893|https://openalex.org/W3013529009
https://openalex.org/W2963420272,Context Encoders: Feature Learning by Inpainting,"Context Encoders: Feature Learning by Inpainting

We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders - a convolutional neural network trained to generate the contents of arbitrary image region conditioned on its surroundings. In order succeed at this task, context encoders need both understand content entire image, as well produce plausible hypothesis for missing part(s). When training encoders, have experimented standard pixel-wise reconstruction loss, plus adversarial loss. The latter produces much sharper results because it can better handle multiple modes in output. found that encoder learns representation captures not just appearance but also semantics structures. quantitatively demonstrate effectiveness our learned features CNN pre-training classification, detection, and segmentation tasks. Furthermore, be used semantic inpainting tasks, either stand-alone or initialization non-parametric methods.

inpainting, context encoders, computer vision, pattern recognition, automatic classification, machine vision, feature (computer vision), cognitive science, scene interpretation, computer science, feature learning, machine learning, image representation, deep learning, data science, machine learning research, image analysis",2016,4301,inpainting|context encoders|computer vision|pattern recognition|automatic classification|machine vision|feature (computer vision)|cognitive science|scene interpretation|computer science|feature learning|machine learning|image representation|deep learning|data science|machine learning research|image analysis,https://openalex.org/W2962793481|https://openalex.org/W2963073614|https://openalex.org/W3035524453|https://openalex.org/W3036224891|https://openalex.org/W3023371261|https://openalex.org/W4313156423|https://openalex.org/W2938260698
https://openalex.org/W2471962767,Structure-from-Motion Revisited,"Structure-from-Motion Revisited

Incremental Structure-from-Motion is a prevalent strategy for 3D reconstruction from unordered image collections. While incremental systems have tremendously advanced in all regards, robustness, accuracy, completeness, and scalability remain the key problems towards building truly general-purpose pipeline. We propose new SfM technique that improves upon state of art to make further step this ultimate goal. The full pipeline released public as an open-source implementation.

image analysis, computational imaging, computer science, kinematics, structure from motion, biomedical imaging, multi-view geometry, computer vision, kinesiology, applied mathematics, physically based animation, 3d reconstruction, deep learning, numerical simulation, machine vision, motion synthesis, computational geometry",2016,3260,image analysis|computational imaging|computer science|kinematics|structure from motion|biomedical imaging|multi-view geometry|computer vision|kinesiology|applied mathematics|physically based animation|3d reconstruction|deep learning|numerical simulation|machine vision|motion synthesis|computational geometry,
https://openalex.org/W2401231614,Wide Residual Networks,"Wide Residual Networks

Deep residual networks were shown to be able scale up thousands of layers and still have improving performance. However, each fraction a percent improved accuracy costs nearly doubling the number layers, so training very deep has problem diminishing feature reuse, which makes these slow train. To tackle problems, in this paper we conduct detailed experimental study on architecture ResNet blocks, based propose novel where decrease depth increase width networks. We call resulting network structures wide (WRNs) show that are far superior over their commonly used thin counterparts. For example, demonstrate even simple 16-layer-deep outperforms efficiency all previous networks, including thousand-layer-deep achieving new state-of-the-art results CIFAR, SVHN, COCO, significant improvements ImageNet. Our code models available at https://github.com/szagoruyko/wide-residual-networks

sparse neural network, computational optimization, neural network (machine learning), computer science, deep reinforcement learning, machine learning research, deep learning, pattern recognition, mathematical optimization, information fusion, neuronal network, machine learning, data science, systems engineering, computer vision, computational intelligence, graph neural network, convolutional neural network, wide residual networks",2016,3207,sparse neural network|computational optimization|neural network (machine learning)|computer science|deep reinforcement learning|machine learning research|deep learning|pattern recognition|mathematical optimization|information fusion|neuronal network|machine learning|data science|systems engineering|computer vision|computational intelligence|graph neural network|convolutional neural network|wide residual networks,https://openalex.org/W2963446712|https://openalex.org/W2565639579|https://openalex.org/W3121523901
https://openalex.org/W2964137095,Wide Residual Networks,"Wide Residual Networks

Deep residual networks were shown to be able scale up thousands of layers and still have improving performance. However, each fraction a percent improved accuracy costs nearly doubling the number layers, so training very deep has problem diminishing feature reuse, which makes these slow train. To tackle problems, in this paper we conduct detailed experimental study on architecture ResNet blocks, based propose novel where decrease depth increase width networks. We call resulting network structures wide (WRNs) show that are far superior over their commonly used thin counterparts. For example, demonstrate even simple 16-layer-deep outperforms efficiency all previous networks, including thousand-layer-deep achieving new state-of-the-art results CIFAR, SVHN, COCO, significant improvements ImageNet. Our code models available at https URL

neuronal network, computer science, systems engineering, deep reinforcement learning, information fusion, sparse neural network, convolutional neural network, data science, deep learning, pattern recognition, neural network (machine learning), machine learning, computer vision, graph neural network, computational optimization, machine learning research, wide residual networks, computational intelligence, mathematical optimization",2016,3150,neuronal network|computer science|systems engineering|deep reinforcement learning|information fusion|sparse neural network|convolutional neural network|data science|deep learning|pattern recognition|neural network (machine learning)|machine learning|computer vision|graph neural network|computational optimization|machine learning research|wide residual networks|computational intelligence|mathematical optimization,https://openalex.org/W2565639579|https://openalex.org/W3138516171|https://openalex.org/W3121523901|https://openalex.org/W3130754787
https://openalex.org/W2963446712,Densely Connected Convolutional Networks,"Densely Connected Convolutional Networks

Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close the input those output. In this paper, we embrace observation introduce Dense Convolutional Network (DenseNet), which connects each layer every other in a feed-forward fashion. Whereas traditional with L have connections-one its subsequent layer-our network L(L+1)/2 direct connections. For layer, feature-maps of all preceding are used as inputs, own inputs into layers. DenseNets several compelling advantages: alleviate vanishing-gradient problem, strengthen feature propagation, encourage reuse, reduce number parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, ImageNet). obtain significant improvements over state-of-the-art most them, whilst requiring less memory computation achieve high performance. Code pre-trained models available at https://github.com/liuzhuang13/DenseNet.

computer science, convolutional neural network, convolutional networks, computer vision, machine learning, deep learning, large-scale datasets, neural network (machine learning), machine vision",2017,33093,computer science|convolutional neural network|convolutional networks|computer vision|machine learning|deep learning|large-scale datasets|neural network (machine learning)|machine vision,https://openalex.org/W3018757597|https://openalex.org/W2996290406|https://openalex.org/W3035160371|https://openalex.org/W3138516171|https://openalex.org/W2962730651|https://openalex.org/W3014641072|https://openalex.org/W3131500599|https://openalex.org/W3023371261|https://openalex.org/W3121523901|https://openalex.org/W3013529009|https://openalex.org/W3167976421|https://openalex.org/W3210586215|https://openalex.org/W4312443924|https://openalex.org/W4312560592|https://openalex.org/W3081167590|https://openalex.org/W3130754787|https://openalex.org/W4226178544
https://openalex.org/W639708223,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks

State-of-the-art object detection networks depend on region proposal algorithms to hypothesize locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these networks, exposing computation as a bottleneck. In this work, we introduce Region Proposal Network(RPN) that shares full-image convolutional features with network, thus enabling nearly cost-free proposals. An RPN is fully network simultaneously predicts bounds objectness scores at each position. The trained end-to-end generate high-quality proposals, which are used by for detection. We further merge into single sharing their features-using recently popular terminology neural 'attention' mechanisms, component tells unified where look. For very deep VGG-16 model [3], our system has frame rate 5 fps (including all steps) GPU, while achieving state-of-the-art accuracy PASCAL VOC 2007, 2012, MS COCO datasets only 300 proposals per image. ILSVRC 2015 competitions, Faster foundations 1st-place winning entries in several tracks. Code been made publicly available.

pattern recognition, computer science, machine learning, image analysis, information fusion, cognitive science, object detection, data science, computational imaging, scene understanding, localization, deep learning, machine vision, region proposal networks, object tracking, object recognition, computer vision, scene analysis, vehicular technology",2017,26058,pattern recognition|computer science|machine learning|image analysis|information fusion|cognitive science|object detection|data science|computational imaging|scene understanding|localization|deep learning|machine vision|region proposal networks|object tracking|object recognition|computer vision|scene analysis|vehicular technology,https://openalex.org/W2963037989|https://openalex.org/W639708223|https://openalex.org/W2395611524|https://openalex.org/W3014641072|https://openalex.org/W4312443924|https://openalex.org/W4206706211|https://openalex.org/W3136761610
https://openalex.org/W2963351448,Focal Loss for Dense Object Detection,"Focal Loss for Dense Object Detection

The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where classifier is applied sparse set of candidate locations. In contrast, one-stage that over regular, dense sampling possible locations have the potential be faster and simpler, but trailed thus far. this paper, we investigate why case. We discover extreme foreground-background class imbalance encountered during training central cause. propose address reshaping standard cross entropy loss such it down-weights assigned well-classified examples. Our novel Focal Loss focuses hard examples prevents vast number easy negatives from overwhelming detector training. To evaluate effectiveness our loss, design train simple call RetinaNet. results show when trained with focal RetinaNet able match speed previous while surpassing all existing state-of-the-art detectors.

digital image processing, computer science, deep learning, pattern recognition, focal loss, information fusion, machine vision, object detection, image representation, scene understanding, object recognition, computational imaging, dense object detection, localization, data science, computer vision, cognitive science, multi-view geometry, object tracking, image analysis",2017,17900,digital image processing|computer science|deep learning|pattern recognition|focal loss|information fusion|machine vision|object detection|image representation|scene understanding|object recognition|computational imaging|dense object detection|localization|data science|computer vision|cognitive science|multi-view geometry|object tracking|image analysis,https://openalex.org/W3018757597|https://openalex.org/W3034971973|https://openalex.org/W3122239467|https://openalex.org/W3015788359|https://openalex.org/W3014641072|https://openalex.org/W3131500599|https://openalex.org/W3210586215|https://openalex.org/W4312815172|https://openalex.org/W3136761610
https://openalex.org/W2565639579,Feature Pyramid Networks for Object Detection,"Feature Pyramid Networks for Object Detection

Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided recent object detectors that based on deep convolutional networks, partially because they slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of networks construct feature with marginal extra cost. A top-down architecture lateral connections is developed building high-level semantic maps all This architecture, called Pyramid Network (FPN), shows significant improvement as generic extractor several applications. Using Faster R-CNN system, our method achieves state-of-the-art single-model results COCO detection benchmark without bells whistles, surpassing existing entries including those from 2016 challenge winners. addition, can run 5 FPS GPU thus practical accurate solution multi-scale detection. Code will be made publicly available.

image analysis, pattern recognition, computer science, feature pyramid networks, computer vision, machine learning, feature detection, object detection, object categorization, deep learning, image representation, feature construction, machine vision, feature (computer vision)",2017,17369,image analysis|pattern recognition|computer science|feature pyramid networks|computer vision|machine learning|feature detection|object detection|object categorization|deep learning|image representation|feature construction|machine vision|feature (computer vision),https://openalex.org/W2963351448|https://openalex.org/W3018757597|https://openalex.org/W3035524453|https://openalex.org/W3034971973|https://openalex.org/W2884561390|https://openalex.org/W2996290406|https://openalex.org/W3122239467|https://openalex.org/W3138516171|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W3023371261|https://openalex.org/W3210586215|https://openalex.org/W4313156423|https://openalex.org/W4312815172|https://openalex.org/W2965898445|https://openalex.org/W3136761610
https://openalex.org/W2962793481,Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks

Image-to-image translation is a class of vision and graphics problems where the goal to learn mapping between an input image output using training set aligned pairs. However, for many tasks, paired data will not be available. We present approach learning translate from source domain X target Y in absence examples. Our G : → such that distribution images G(X) indistinguishable adversarial loss. Because this highly under-constrained, we couple it with inverse F introduce cycle consistency loss push F(G(X)) ≈ (and vice versa). Qualitative results are presented on several tasks does exist, including collection style transfer, object transfiguration, season photo enhancement, etc. Quantitative comparisons against prior methods demonstrate superiority our approach.

digital image processing, biomedical imaging, unpaired image-to-image translation, computer science, deep learning, machine translation, adversarial machine learning, cycle-consistent adversarial networks, image representation, human image synthesis, computational imaging, machine learning, domain adaptation, generative adversarial network, data science, computer vision, cognitive science, synthetic image generation, image communication, image analysis",2017,16550,digital image processing|biomedical imaging|unpaired image-to-image translation|computer science|deep learning|machine translation|adversarial machine learning|cycle-consistent adversarial networks|image representation|human image synthesis|computational imaging|machine learning|domain adaptation|generative adversarial network|data science|computer vision|cognitive science|synthetic image generation|image communication|image analysis,https://openalex.org/W3023371261|https://openalex.org/W3013529009|https://openalex.org/W3121661546|https://openalex.org/W3212516020
https://openalex.org/W2963073614,Image-to-Image Translation with Conditional Adversarial Networks,"Image-to-Image Translation with Conditional Adversarial Networks

We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These not only learn the mapping from input image output image, but also loss function train this mapping. This makes it possible apply same generic approach problems that traditionally would require very different formulations. demonstrate is effective at synthesizing photos label maps, reconstructing objects edge and colorizing images, among other tasks. Moreover, since release of pi×2pi× software associated with paper, hundreds twitter users have posted their own artistic experiments using our system. As community, we no longer hand-engineer functions, work suggests can achieve reasonable results without handengineering functions either.

computer science, conditional adversarial networks, machine learning, multimodal translation, synthetic image generation, image analysis, data science, image representation, machine translation, image-to-image translation, computational imaging, adversarial machine learning, image communication, deep learning, machine learning research, digital image processing, computer vision, neural machine translation, human image synthesis",2017,15243,computer science|conditional adversarial networks|machine learning|multimodal translation|synthetic image generation|image analysis|data science|image representation|machine translation|image-to-image translation|computational imaging|adversarial machine learning|image communication|deep learning|machine learning research|digital image processing|computer vision|neural machine translation|human image synthesis,https://openalex.org/W3023371261|https://openalex.org/W3212516020
https://openalex.org/W2963881378,SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation

We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable engine consists of an encoder network, corresponding decoder followed by classification layer. The the is topologically identical to 13 layers in VGG16 [1] . role map low resolution feature maps full input classification. novelty SegNet lies manner which upsamples its lower map(s). Specifically, uses pooling indices computed max-pooling step perform non-linear upsampling. eliminates need learning upsample. upsampled are sparse then convolved with filters produce dense maps. compare our proposed widely adopted FCN [2] also well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. comparison reveals memory versus accuracy trade-off involved achieving good performance. was primarily motivated scene understanding applications. Hence, it designed be efficient both terms computational time during inference. It significantly smaller number parameters than other competing architectures can trained end-to-end using stochastic gradient descent. performed controlled benchmark on road scenes SUN RGB-D indoor tasks. These quantitative assessments show that provides performance competitive inference most memory-wise as compared provide Caffe implementation web demo at http://mi.eng.cam.ac.uk/projects/segnet/.

image analysis, pattern recognition, computer science, computational imaging, medical image computing, autoencoders, convolutional neural network, scene understanding, computer vision, machine learning, scene analysis, cognitive science, data science, deep learning, image representation, machine learning research, image segmentation, machine vision",2017,14516,image analysis|pattern recognition|computer science|computational imaging|medical image computing|autoencoders|convolutional neural network|scene understanding|computer vision|machine learning|scene analysis|cognitive science|data science|deep learning|image representation|machine learning research|image segmentation|machine vision,https://openalex.org/W2560023338|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3170841864|https://openalex.org/W4285531802|https://openalex.org/W4214893857
https://openalex.org/W2962858109,Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization,"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization

We propose a technique for producing `visual explanations' decisions from large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients any target concept (say logits `dog' or even caption), flowing into final convolutional layer to produce coarse localization map highlighting important regions in image predicting concept. Unlike previous approaches, Grad- CAM is applicable wide variety CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) used structured outputs captioning), (3) tasks multi-modal inputs visual question answering) reinforcement learning, without architectural changes re-training. combine Grad-CAM existing fine-grained visualizations create high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it classification, captioning, answering (VQA) including ResNet-based architectures. In context classification our (a) lend insights failure modes these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform methods on ILSVRC-15 weakly-supervised task, (c) are faithful underlying model, (d) help achieve model generalization by identifying dataset bias. For captioning VQA, show non-attention based can localize inputs. Finally, we design conduct human studies measure if explanations users establish appropriate trust deep networks helps untrained successfully discern `stronger' network `weaker' one when both make identical predictions. code available at https: //github.com/ramprs/grad-cam/ along demo CloudCV [2] video youtu.be/COjUB9Izk6E.

computer science, vision language model, machine learning, visual science, image analysis, information visualization, visualization, feature detection, cognitive science, data science, computational imaging, scene understanding, deep networks, localization, deep learning, machine vision, visual explanations, visual question answering, interactive visualization, computer vision, gradient-based localization",2017,12443,computer science|vision language model|machine learning|visual science|image analysis|information visualization|visualization|feature detection|cognitive science|data science|computational imaging|scene understanding|deep networks|localization|deep learning|machine vision|visual explanations|visual question answering|interactive visualization|computer vision|gradient-based localization,
https://openalex.org/W2560023338,Pyramid Scene Parsing Network,"Pyramid Scene Parsing Network

Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based aggregation through our pyramid pooling module together with proposed scene network (PSPNet). Our prior representation effective to produce good quality results on task, while PSPNet provides a superior framework pixel-level prediction. The approach achieves state-of-the-art performance various datasets. It came first in ImageNet challenge 2016, PASCAL VOC 2012 benchmark Cityscapes benchmark. A single yields new record mIoU accuracy 85.4% 80.2% Cityscapes.

image analysis, pattern recognition, computer science, computational imaging, structure from motion, scene interpretation, scene understanding, multi-view geometry, computer vision, scene analysis, multimedia retrieval, data science, systems engineering, deep learning, image representation, machine learning research, machine vision, digital image processing",2017,11099,image analysis|pattern recognition|computer science|computational imaging|structure from motion|scene interpretation|scene understanding|multi-view geometry|computer vision|scene analysis|multimedia retrieval|data science|systems engineering|deep learning|image representation|machine learning research|machine vision|digital image processing,https://openalex.org/W3015788359|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W3023371261|https://openalex.org/W3013529009|https://openalex.org/W3167976421|https://openalex.org/W4285531802|https://openalex.org/W4214893857|https://openalex.org/W3212386989|https://openalex.org/W4312815172
https://openalex.org/W4297775537,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications

We present a class of efficient models called MobileNets for mobile and embedded vision applications. are based on streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. introduce two simple global hyper-parameters efficiently trade off between latency accuracy. These allow the model builder choose right sized their application constraints problem. extensive experiments resource accuracy tradeoffs show strong performance compared other popular ImageNet classification. then demonstrate effectiveness across wide range applications use cases including object detection, finegrain classification, face attributes large scale geo-localization.

computer vision, mobile sensing, machine vision, neural network (machine learning), motion detection, cognitive science, computational intelligence, computer science, convolutional neural network, machine learning, feature detection, machine learning research, object detection, deep learning, data science, mobile vision applications, image analysis",2017,10304,computer vision|mobile sensing|machine vision|neural network (machine learning)|motion detection|cognitive science|computational intelligence|computer science|convolutional neural network|machine learning|feature detection|machine learning research|object detection|deep learning|data science|mobile vision applications|image analysis,https://openalex.org/W3121523901|https://openalex.org/W3013529009|https://openalex.org/W4214493665|https://openalex.org/W3210586215|https://openalex.org/W4312443924
https://openalex.org/W2963470893,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network

Despite the breakthroughs in accuracy and speed of single image super-resolution using faster deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover finer texture details when super-resolve at large upscaling factors? The behavior optimization-based methods is principally driven by choice objective function. Recent work has focused on minimizing mean squared reconstruction error. resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency perceptually unsatisfying sense that fail to match fidelity expected higher resolution. In this paper, present SRGAN, a generative adversarial network (GAN) for (SR). To our knowledge, it first framework capable inferring photo-realistic natural images 4x factors. achieve this, propose perceptual loss function which consists an content loss. pushes solution manifold discriminator trained differentiate between super-resolved original images. addition, use motivated similarity instead pixel space. Our deep residual able textures from heavily downsampled public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains quality SRGAN. MOS scores obtained with SRGAN closer those high-resolution than any state-of-the-art method.

image analysis, computational imaging, computer science, super-resolution imaging, computer vision, machine learning, generative adversarial network, generative ai, single-image super-resolution, deep learning, image representation, synthetic image generation, machine vision, digital image processing",2017,9742,image analysis|computational imaging|computer science|super-resolution imaging|computer vision|machine learning|generative adversarial network|generative ai|single-image super-resolution|deep learning|image representation|synthetic image generation|machine vision|digital image processing,https://openalex.org/W2962793481|https://openalex.org/W2963073614|https://openalex.org/W2963372104|https://openalex.org/W3023371261|https://openalex.org/W3013529009|https://openalex.org/W3121661546|https://openalex.org/W3155072588|https://openalex.org/W3212516020
https://openalex.org/W2612445135,MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications

We present a class of efficient models called MobileNets for mobile and embedded vision applications. are based on streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. introduce two simple global hyper-parameters efficiently trade off between latency accuracy. These allow the model builder choose right sized their application constraints problem. extensive experiments resource accuracy tradeoffs show strong performance compared other popular ImageNet classification. then demonstrate effectiveness across wide range applications use cases including object detection, finegrain classification, face attributes large scale geo-localization.

image analysis, computer science, computer vision, machine learning, computational intelligence, motion detection, feature detection, mobile sensing, object detection, neural network (machine learning), machine learning research, convolutional neural network, data science, deep learning, mobile vision applications, machine vision, cognitive science",2017,8181,image analysis|computer science|computer vision|machine learning|computational intelligence|motion detection|feature detection|mobile sensing|object detection|neural network (machine learning)|machine learning research|convolutional neural network|data science|deep learning|mobile vision applications|machine vision|cognitive science,https://openalex.org/W3018757597|https://openalex.org/W3121523901|https://openalex.org/W3013529009|https://openalex.org/W3167976421|https://openalex.org/W3157506437
https://openalex.org/W2560609797,PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation,"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation

Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such regular 3D voxel grids or collections images. This, however, renders unnecessarily voluminous and causes issues. In this paper, we design a novel neural network that directly consumes point clouds, which well respects the permutation invariance points in input. Our network, named PointNet, provides unified architecture for applications ranging from object classification, part segmentation, scene semantic parsing. Though simple, PointNet highly efficient effective. Empirically, it shows strong performance on par even better than state art. Theoretically, provide analysis towards understanding what has learnt why robust with respect input perturbation corruption.

pattern recognition, computer science, point sets, multi-view geometry, machine learning, visual science, image analysis, information fusion, scene modeling, cognitive science, data science, computational imaging, localization, deep learning, 3d object recognition, machine vision, 3d computer vision, 3d vision, computer vision",2017,7870,pattern recognition|computer science|point sets|multi-view geometry|machine learning|visual science|image analysis|information fusion|scene modeling|cognitive science|data science|computational imaging|localization|deep learning|3d object recognition|machine vision|3d computer vision|3d vision|computer vision,https://openalex.org/W3012494314|https://openalex.org/W3023371261
https://openalex.org/W2395611524,Fully Convolutional Networks for Semantic Segmentation,"Fully Convolutional Networks for Semantic Segmentation

Convolutional networks are powerful visual models that yield hierarchies of features. We show convolutional by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build ""fully convolutional"" take input arbitrary size and produce correspondingly-sized output with efficient inference learning. define detail space fully networks, explain their application spatially dense prediction tasks, draw connections prior models. adapt contemporary classification (AlexNet, VGG net, GoogLeNet) into transfer learned representations fine-tuning segmentation task. then a skip architecture combines information from deep, coarse layer appearance shallow, fine accurate detailed segmentations. achieve improved PASCAL VOC (30% relative improvement 67.2% mean IU 2012), NYUDv2, SIFT Flow, PASCAL-Context, while takes one tenth second for typical image.

pattern recognition, computer science, machine learning, image segmentation, image analysis, information fusion, convolutional neural network, convolutional networks, cognitive science, data science, computational imaging, scene understanding, deep learning, machine learning research, machine vision, semantic segmentation, medical image computing, feature extraction, computer vision, scene analysis",2017,7476,pattern recognition|computer science|machine learning|image segmentation|image analysis|information fusion|convolutional neural network|convolutional networks|cognitive science|data science|computational imaging|scene understanding|deep learning|machine learning research|machine vision|semantic segmentation|medical image computing|feature extraction|computer vision|scene analysis,https://openalex.org/W2996290406|https://openalex.org/W3014641072|https://openalex.org/W4214893857
https://openalex.org/W2963524571,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset

The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures light the new Kinetics Human Action Video dataset. two orders magnitude more data, with 400 human classes over clips per class, is collected from realistic, challenging YouTube videos. We provide an analysis how fare task this dataset much improves smaller benchmark after pre-training Kinetics. also introduce a Two-Stream Inflated 3D ConvNet (I3D) that based 2D inflation: filters pooling kernels very deep image ConvNets are expanded into 3D, making possible learn seamless spatio-temporal feature extractors while leveraging successful ImageNet architecture designs even their parameters. show that, Kinetics, I3D models considerably improve upon classification, reaching 80.2% HMDB-51 97.9% UCF-101.

computer science, robot learning, information fusion, multimedia retrieval, action recognition, cognitive science, structure from motion, digital image processing, data science, pattern recognition, quo vadis, video understanding, machine learning, computer vision, machine vision, activity recognition, image analysis, motion detection, motion analysis, image representation",2017,6836,computer science|robot learning|information fusion|multimedia retrieval|action recognition|cognitive science|structure from motion|digital image processing|data science|pattern recognition|quo vadis|video understanding|machine learning|computer vision|machine vision|activity recognition|image analysis|motion detection|motion analysis|image representation,https://openalex.org/W4214612132|https://openalex.org/W4312560592
https://openalex.org/W2508457857,Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising,"Beyond a Gaussian Denoiser: Residual Learning of Deep CNN for Image Denoising

Discriminative model learning for image denoising has been recently attracting considerable attentions due to its favorable performance. In this paper, we take one step forward by investigating the construction of feed-forward convolutional neural networks (DnCNNs) embrace progress in very deep architecture, algorithm, and regularization method into denoising. Specifically, residual batch normalization are utilized speed up training process as well boost Different from existing discriminative models which usually train a specific additive white Gaussian noise (AWGN) at certain level, our DnCNN is able handle with unknown level (i.e., blind denoising). With strategy, implicitly removes latent clean hidden layers. This property motivates us single tackle several general tasks such denoising, super-resolution JPEG deblocking. Our extensive experiments demonstrate that can not only exhibit high effectiveness tasks, but also be efficiently implemented benefiting GPU computing.

image analysis, computational imaging, computer science, convolutional neural network, image restoration, computer vision, machine learning, deep cnn, gaussian denoiser, deep learning, deblurring, machine learning research, residual learning, image denoising, automatic classification, machine vision, digital image processing",2017,6359,image analysis|computational imaging|computer science|convolutional neural network|image restoration|computer vision|machine learning|deep cnn|gaussian denoiser|deep learning|deblurring|machine learning research|residual learning|image denoising|automatic classification|machine vision|digital image processing,https://openalex.org/W3121661546|https://openalex.org/W3167568784
https://openalex.org/W2630837129,Rethinking Atrous Convolution for Semantic Image Segmentation,"Rethinking Atrous Convolution for Semantic Image Segmentation

In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well control the resolution of feature responses computed by Deep Convolutional Neural Networks, in application semantic image segmentation. To handle problem segmenting objects at multiple scales, design modules which employ convolution cascade or parallel capture multi-scale context adopting rates. Furthermore, propose augment our previously proposed Atrous Spatial Pyramid Pooling module, probes convolutional features with image-level encoding global and further boost performance. We also elaborate on implementation details share experience training system. The `DeepLabv3' system significantly improves over previous DeepLab versions without DenseCRF post-processing attains comparable performance other state-of-art models PASCAL VOC 2012 segmentation benchmark.

pattern recognition, computer science, machine learning, atrous convolution, image segmentation, information fusion, convolutional neural network, biomedical imaging, cognitive science, data science, image representation, computational imaging, scene understanding, deep learning, semantic image segmentation, machine vision, digital image processing, scene interpretation, computer vision, scene analysis",2017,6262,pattern recognition|computer science|machine learning|atrous convolution|image segmentation|information fusion|convolutional neural network|biomedical imaging|cognitive science|data science|image representation|computational imaging|scene understanding|deep learning|semantic image segmentation|machine vision|digital image processing|scene interpretation|computer vision|scene analysis,https://openalex.org/W3015788359|https://openalex.org/W3014641072|https://openalex.org/W3132455321|https://openalex.org/W3170841864|https://openalex.org/W4285531802|https://openalex.org/W4214893857|https://openalex.org/W4312815172
https://openalex.org/W2963121255,PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space,"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space

Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, design does not capture local structures induced the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability complex scenes. In work, we introduce hierarchical neural network that applies recursively nested partitioning of input set. By exploiting distances, our able learn features with increasing contextual scales. With further observation sets are usually sampled varying densities, which results greatly decreased performance for networks trained uniform propose novel set layers adaptively combine from multiple Experiments show called PointNet++ efficiently robustly. particular, significantly better than state-of-the-art have been obtained challenging benchmarks 3D clouds.

computer science, hierarchical classification, point sets, feature learning, manifold learning, data science, deep learning, pattern recognition, neural network (machine learning), machine learning, computational imaging, machine learning research, feature (computer vision), deep hierarchical feature, machine vision, metric space, image analysis, computational intelligence, geometric learning, applied mathematics",2017,5922,computer science|hierarchical classification|point sets|feature learning|manifold learning|data science|deep learning|pattern recognition|neural network (machine learning)|machine learning|computational imaging|machine learning research|feature (computer vision)|deep hierarchical feature|machine vision|metric space|image analysis|computational intelligence|geometric learning|applied mathematics,https://openalex.org/W3012494314
https://openalex.org/W2559085405,Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields,"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields

We present an approach to efficiently detect the 2D pose of multiple people in image. The uses a nonparametric representation, which we refer as Part Affinity Fields (PAFs), learn associate body parts with individuals architecture encodes global context, allowing greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective number is designed jointly part locations and their association via two branches same sequential prediction process. Our method placed first inaugural COCO 2016 keypoints challenge, significantly exceeds previous state-of-the-art result on MPII Multi-Person benchmark, both performance efficiency.

image analysis, pattern recognition, computer science, human pose estimation, multi-view geometry, computer vision, machine learning, motion capture, pose estimation, deep learning, 3d pose estimation, machine vision, part affinity fields",2017,5622,image analysis|pattern recognition|computer science|human pose estimation|multi-view geometry|computer vision|machine learning|motion capture|pose estimation|deep learning|3d pose estimation|machine vision|part affinity fields,https://openalex.org/W2962730651|https://openalex.org/W3014641072
https://openalex.org/W2963372104,Enhanced Deep Residual Networks for Single Image Super-Resolution,"Enhanced Deep Residual Networks for Single Image Super-Resolution

Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. this paper, we develop an enhanced network (EDSR) performance exceeding those current state-of-the-art SR methods. The significant improvement our model is due to optimization by removing unnecessary modules in conventional networks. further expanding size while stabilize training procedure. We also propose a new multi-scale system (MDSR) and method, which can reconstruct high-resolution images different upscaling factors single model. proposed methods show superior over benchmark datasets prove its excellence winning NTIRE2017 Super-Resolution Challenge[26].

image resolution, digital image processing, computer vision, biomedical imaging, super-resolution imaging, computational imaging, machine vision, sparse neural network, single-image super-resolution, computer science, machine learning, high resolution, image representation, deep learning, data science, image analysis",2017,5055,image resolution|digital image processing|computer vision|biomedical imaging|super-resolution imaging|computational imaging|machine vision|sparse neural network|single-image super-resolution|computer science|machine learning|high resolution|image representation|deep learning|data science|image analysis,https://openalex.org/W3013529009|https://openalex.org/W3167568784
https://openalex.org/W3094502228,An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale

While the Transformer architecture has become de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used replace certain components of networks while keeping their overall structure place. We show that this reliance on CNNs not necessary and a pure transformer directly sequences image patches can perform very well classification tasks. When pre-trained large amounts data transferred multiple mid-sized small recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision (ViT) attains excellent results compared state-of-the-art requiring substantially fewer computational resources train.

image analysis, pattern recognition, computer science, image classification, image search, natural language processing, image communication, image retrieval, text recognition, computer vision, machine learning, feature detection, data science, multimedia information processing, image representation, machine learning research, machine vision, image recognition",2020,9670,image analysis|pattern recognition|computer science|image classification|image search|natural language processing|image communication|image retrieval|text recognition|computer vision|machine learning|feature detection|data science|multimedia information processing|image representation|machine learning research|machine vision|image recognition,https://openalex.org/W3138516171|https://openalex.org/W3131500599|https://openalex.org/W3170841864|https://openalex.org/W3159481202|https://openalex.org/W3121523901|https://openalex.org/W4214493665|https://openalex.org/W4214612132|https://openalex.org/W3210586215|https://openalex.org/W4285531802|https://openalex.org/W4214893857|https://openalex.org/W4313156423|https://openalex.org/W4212875960|https://openalex.org/W4312560592|https://openalex.org/W4312815172
https://openalex.org/W3018757597,YOLOv4: Optimal Speed and Accuracy of Object Detection,"YOLOv4: Optimal Speed and Accuracy of Object Detection

There are a huge number of features which said to improve Convolutional Neural Network (CNN) accuracy. Practical testing combinations such on large datasets, and theoretical justification the result, is required. Some operate certain models exclusively for problems exclusively, or only small-scale datasets; while some features, as batch-normalization residual-connections, applicable majority models, tasks, datasets. We assume that universal include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) Mish-activation. use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, DropBlock regularization, CIoU loss, combine them achieve state-of-the-art results: 43.5% AP (65.7% AP50) MS COCO dataset at realtime speed ~65 FPS Tesla V100. Source code https://github.com/AlexeyAB/darknet

image analysis, pattern recognition, computer science, object tracking, object recognition, real-time operation, computer vision, machine learning, object detection, data science, moving object tracking, deep learning, optimal speed, machine vision",2020,7532,image analysis|pattern recognition|computer science|object tracking|object recognition|real-time operation|computer vision|machine learning|object detection|data science|moving object tracking|deep learning|optimal speed|machine vision,https://openalex.org/W3138516171|https://openalex.org/W3210586215
https://openalex.org/W3035524453,Momentum Contrast for Unsupervised Visual Representation Learning,"Momentum Contrast for Unsupervised Visual Representation Learning

We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build dynamic with queue and moving-averaged encoder. This enables building large consistent on-the-fly that facilitates MoCo provides competitive results under the common linear protocol ImageNet classification. More importantly, representations learned by transfer well to downstream tasks. can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks PASCAL VOC, COCO, other datasets, sometimes surpassing it margins. suggests gap between has been largely closed many vision

computer science, vision language model, machine learning, visual science, image analysis, visual reasoning, visualization, cognitive science, data science, image representation, computational imaging, scene understanding, unsupervised machine learning, deep learning, machine learning research, machine vision, momentum contrast, visual question answering, computer vision",2020,7118,computer science|vision language model|machine learning|visual science|image analysis|visual reasoning|visualization|cognitive science|data science|image representation|computational imaging|scene understanding|unsupervised machine learning|deep learning|machine learning research|machine vision|momentum contrast|visual question answering|computer vision,https://openalex.org/W3094502228|https://openalex.org/W3159481202|https://openalex.org/W3023371261|https://openalex.org/W4313156423|https://openalex.org/W4206706211
https://openalex.org/W3005680577,A Simple Framework for Contrastive Learning of Visual Representations,"A Simple Framework for Contrastive Learning of Visual Representations

This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed self-supervised algorithms without requiring specialized architectures or memory bank. In order to understand what enables the prediction tasks learn useful representations, we systematically study major components our framework. show that (1) composition data augmentations plays critical role in defining effective predictive tasks, (2) introducing learnable nonlinear transformation between representation and loss substantially improves quality learned (3) benefits from larger batch sizes more training steps compared supervised learning. By combining these findings, are able considerably outperform previous methods semi-supervised on ImageNet. A linear classifier trained representations by SimCLR achieves 76.5% top-1 accuracy, which is 7% relative improvement over state-of-the-art, matching performance ResNet-50. When fine-tuned only 1% labels, achieve 85.8% top-5 outperforming AlexNet with 100X fewer labels.

simple framework, visual modeling, computer vision, machine vision, contrastive learning, visual perception, visual reasoning, cognitive science, computer science, machine learning, image representation, deep learning, data science, visual representations, visual data mining, image analysis",2020,5329,simple framework|visual modeling|computer vision|machine vision|contrastive learning|visual perception|visual reasoning|cognitive science|computer science|machine learning|image representation|deep learning|data science|visual representations|visual data mining|image analysis,https://openalex.org/W3035524453|https://openalex.org/W3036224891|https://openalex.org/W3159481202|https://openalex.org/W3023371261|https://openalex.org/W4313156423|https://openalex.org/W4206706211
https://openalex.org/W3034971973,EfficientDet: Scalable and Efficient Object Detection,"EfficientDet: Scalable and Efficient Object Detection

Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, a weighted bi-directional feature pyramid (BiFPN), which allows easy fast multi-scale fusion; Second, compound scaling method that uniformly scales the resolution, depth, width all backbone, network, box/class prediction networks at same time. Based on these EfficientNet backbones, have developed new family of detectors, called EfficientDet, consistently achieve much better than prior art across wide spectrum resource constraints. particular, with single-model single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP COCO test-dev 52M parameters 325B FLOPs1, being 4x - 9x smaller using 13x 42x fewer FLOPs previous detector. Code is available https://github.com/google/ automl/tree/master/efficientdet.

image analysis, pattern recognition, computer science, object recognition, feature extraction, computer vision, machine learning, feature detection, motion detection, object detection, data science, deep learning, efficient object detection, machine vision",2020,4437,image analysis|pattern recognition|computer science|object recognition|feature extraction|computer vision|machine learning|feature detection|motion detection|object detection|data science|deep learning|efficient object detection|machine vision,https://openalex.org/W3122239467|https://openalex.org/W3138516171|https://openalex.org/W3210586215|https://openalex.org/W4312815172
https://openalex.org/W2884561390,Focal Loss for Dense Object Detection,"Focal Loss for Dense Object Detection

The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where classifier is applied sparse set of candidate locations. In contrast, one-stage that over regular, dense sampling possible locations have the potential be faster and simpler, but trailed thus far. this paper, we investigate why case. We discover extreme foreground-background class imbalance encountered during training central cause. propose address reshaping standard cross entropy loss such it down-weights assigned well-classified examples. Our novel Focal Loss focuses hard examples prevents vast number easy negatives from overwhelming detector training. To evaluate effectiveness our loss, design train simple call RetinaNet. results show when trained with focal RetinaNet able match speed previous while surpassing all existing state-of-the-art detectors. Code at: https://github.com/facebookresearch/Detectron.

pattern recognition, computer science, multi-view geometry, image analysis, focal loss, information fusion, cognitive science, object detection, data science, image representation, computational imaging, scene understanding, localization, deep learning, machine vision, digital image processing, object tracking, object recognition, computer vision, dense object detection",2020,4390,pattern recognition|computer science|multi-view geometry|image analysis|focal loss|information fusion|cognitive science|object detection|data science|image representation|computational imaging|scene understanding|localization|deep learning|machine vision|digital image processing|object tracking|object recognition|computer vision|dense object detection,https://openalex.org/W3023371261
https://openalex.org/W3035574324,Analyzing and Improving the Image Quality of StyleGAN,"Analyzing and Improving the Image Quality of StyleGAN

The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, propose changes both model training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, regularize encourage good conditioning mapping from latent codes images. addition improving quality, this path length regularizer additional benefit that becomes significantly easier invert. This makes it possible reliably attribute a generated particular network. furthermore visualize how well utilizes output resolution, identify capacity problem, motivating us train larger models for quality improvements. Overall, our improved redefines state art modeling, terms existing distribution metrics as perceived quality.

image analysis, pattern recognition, computer science, computational imaging, image compression, image communication, computer graphic, image enhancement, computer vision, generative adversarial network, digital imaging, image quality assessment, multimedia information processing, image representation, digital image processing, image quality",2020,3571,image analysis|pattern recognition|computer science|computational imaging|image compression|image communication|computer graphic|image enhancement|computer vision|generative adversarial network|digital imaging|image quality assessment|multimedia information processing|image representation|digital image processing|image quality,
https://openalex.org/W3035574168,nuScenes: A Multimodal Dataset for Autonomous Driving,"nuScenes: A Multimodal Dataset for Autonomous Driving

Robust detection and tracking of objects is crucial for the deployment autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, segmentation agents environment. Most vehicles, however, carry a combination cameras range sensors lidar radar. As machine learning methods become more prevalent, there need to train evaluate on containing sensor data along with images. In this work we present nuTonomy scenes (nuScenes), first dataset full suite: 6 cameras, 5 radars 1 lidar, all 360 degree field view. nuScenes comprises 1000 scenes, each 20s long fully annotated 3D bounding boxes 23 classes 8 attributes. It has 7x many annotations 100x images pioneering KITTI dataset. We define novel metrics. also provide careful analysis well baselines image tracking. Data, kit information are available online.

autonomous driving, computer science, multimodal dataset, real-time data, real world data, computer vision, selfdriving car, machine learning, data science, machine learning research, large-scale datasets, multimodal learning, machine vision",2020,2838,autonomous driving|computer science|multimodal dataset|real-time data|real world data|computer vision|selfdriving car|machine learning|data science|machine learning research|large-scale datasets|multimodal learning|machine vision,
https://openalex.org/W2996290406,UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation,"UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation

The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these have two limitations: (1) optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble varying depths; (2) skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps encoder decoder sub-networks. To overcome limitations, we propose UNet++, a new neural semantic instance segmentation, by alleviating unknown network with efficient U-Nets depths, which partially share co-learn simultaneously using deep supervision; redesigning to aggregate features scales sub-networks, leading highly flexible scheme; (3) devising pruning scheme accelerate inference speed UNet++. We evaluated UNet++ six different datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance (MRI), electron microscopy (EM), demonstrating that consistently outperforms baseline task across datasets backbone architectures; enhances quality varying-size objects-an improvement over fixed-depth U-Net; Mask RCNN++ (Mask R-CNN design) original segmentation; (4) pruned achieve significant speedup while showing modest performance degradation. Our implementation pre-trained available https://github.com/MrGiovanni/UNetPlusPlus.

computational imaging, pattern recognition, computer science, computer vision, multiscale features, redesigning skip connections, image segmentation, digital image processing",2020,2150,computational imaging|pattern recognition|computer science|computer vision|multiscale features|redesigning skip connections|image segmentation|digital image processing,https://openalex.org/W3130754787
https://openalex.org/W3035160371,Self-Training With Noisy Student Improves ImageNet Classification,"Self-Training With Noisy Student Improves ImageNet Classification

We present a simple self-training method that achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A from 61.0% to 83.7%, reduces ImageNet-C mean corruption error 45.7 28.3, and ImageNet-P flip rate 27.8 12.2. To achieve this result, we first train an EfficientNet ImageNet images use as teacher generate pseudo labels 300M unlabeled then larger student combination of iterate process by putting back teacher. During generation labels, not noised so are accurate possible. However, during learning student, inject noise such dropout, stochastic depth data augmentation via RandAugment generalizes

image analysis, computational imaging, computer science, imagenet classification, feature extraction, computer vision, machine learning, self-supervised learning, cognitive science, data science, deep learning, image representation, autonomous learning, neural network (machine learning), image classification, noisy student",2020,1665,image analysis|computational imaging|computer science|imagenet classification|feature extraction|computer vision|machine learning|self-supervised learning|cognitive science|data science|deep learning|image representation|autonomous learning|neural network (machine learning)|image classification|noisy student,https://openalex.org/W3094502228|https://openalex.org/W3159481202|https://openalex.org/W3157506437
https://openalex.org/W3099700870,Dense Passage Retrieval for Open-Domain Question Answering,"Dense Passage Retrieval for Open-Domain Question Answering

Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.

retrieval technique, question answering, image analysis, computer science, computer vision, knowledge discovery, clustering, open-domain question, dense passage retrieval, multimedia retrieval, machine learning research, information retrieval, nlp task, natural language processing, data science, interactive information retrieval, machine vision, content similarity detection",2020,1545,retrieval technique|question answering|image analysis|computer science|computer vision|knowledge discovery|clustering|open-domain question|dense passage retrieval|multimedia retrieval|machine learning research|information retrieval|nlp task|natural language processing|data science|interactive information retrieval|machine vision|content similarity detection,
https://openalex.org/W3122239467,Deformable DETR: Deformable Transformers for End-to-End Object Detection,"Deformable DETR: Deformable Transformers for End-to-End Object Detection

DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due limitation of Transformer attention modules processing image maps. To mitigate these issues, we Deformable DETR, whose only attend a small set key sampling points around reference. can achieve better performance than (especially on objects) with 10 times less training epochs. Extensive experiments COCO benchmark demonstrate effectiveness our approach. Code is released at https://github.com/fundamentalvision/Deformable-DETR.

computer vision, deformable transformers, end-to-end object detection, deformable detr, computer science, object detection",2020,1345,computer vision|deformable transformers|end-to-end object detection|deformable detr|computer science|object detection,https://openalex.org/W3138516171|https://openalex.org/W3131500599|https://openalex.org/W3121523901|https://openalex.org/W4214493665|https://openalex.org/W3210586215|https://openalex.org/W4206706211|https://openalex.org/W3212386989|https://openalex.org/W4212875960|https://openalex.org/W4312815172
https://openalex.org/W3036224891,Unsupervised Learning of Visual Features by Contrasting Cluster Assignments,"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably recent achievements of contrastive learning methods. These methods typically work online and rely on a large number explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an algorithm, SwAV, that takes advantage without requiring to compute comparisons. Specifically, our method simultaneously clusters data while enforcing consistency between cluster assignments produced for different augmentations (or views) same image, instead comparing features directly as in learning. Simply put, use swapped prediction mechanism where predict assignment view from representation another view. Our can be trained small batches scale unlimited amounts data. Compared previous methods, more memory efficient since it does not require bank or special momentum network. addition, also new augmentation strategy, multi-crop, uses mix views resolutions place two full-resolution views, increasing requirements much. We validate findings by achieving 75.3% top-1 accuracy ImageNet ResNet-50, well surpassing pretraining all considered transfer tasks.

pattern recognition, computer science, feature learning, machine learning, image analysis, cognitive science, data science, image representation, unsupervised learning, feature (computer vision), computational imaging, scene understanding, unsupervised machine learning, visual features, cluster assignments, deep learning, machine learning research, machine vision, visual data mining, computer vision, knowledge discovery",2020,1331,pattern recognition|computer science|feature learning|machine learning|image analysis|cognitive science|data science|image representation|unsupervised learning|feature (computer vision)|computational imaging|scene understanding|unsupervised machine learning|visual features|cluster assignments|deep learning|machine learning research|machine vision|visual data mining|computer vision|knowledge discovery,https://openalex.org/W3159481202
https://openalex.org/W3015788359,UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation,"UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation

Recently, a growing interest has been seen in deep learning-based semantic segmentation. UNet, which is one of learning networks with an encoder-decoder architecture, widely used medical image Combining multi-scale features important factors for accurate UNet++ was developed as modified Unet by designing architecture nested and dense skip connections. However, it does not explore sufficient information from full scales there still large room improvement. In this paper, we propose novel UNet 3+, takes advantage full-scale connections supervisions. The incorporate low-level details high-level semantics feature maps different scales; while the supervision learns hierarchical representations aggregated maps. proposed method especially benefiting organs that appear at varying scales. addition to accuracy improvements, 3+ can reduce network parameters improve computation efficiency. We further hybrid loss function devise classification-guided module enhance organ boundary over-segmentation non-organ image, yielding more segmentation results. effectiveness demonstrated on two datasets. code available at: github.com/ZJUGiveLab/UNet-Version.

full duplex, computer science, full-scale connected unet, image segmentation, medical image segmentation, image analysis, medical imaging, biomedical imaging, bioimage analysis, neuroscience, computational imaging, computer-aided diagnosis, medical image analysis, biomedical engineering, deep learning, machine vision, digital image processing, medical image computing, computer vision, biomedical informatics",2020,1313,full duplex|computer science|full-scale connected unet|image segmentation|medical image segmentation|image analysis|medical imaging|biomedical imaging|bioimage analysis|neuroscience|computational imaging|computer-aided diagnosis|medical image analysis|biomedical engineering|deep learning|machine vision|digital image processing|medical image computing|computer vision|biomedical informatics,
https://openalex.org/W3034978746,A Simple Framework for Contrastive Learning of Visual Representations,"A Simple Framework for Contrastive Learning of Visual Representations

This paper presents SimCLR: a simple framework for contrastive learning of
visual representations. We simplify recently proposed contrastive
self-supervised algorithms without requiring specialized architectures
or memory bank. In order to understand what enables the contrastive
prediction tasks learn useful representations, we systematically study the
major components of our framework. show that (1) composition data
augmentations plays critical role in defining effective predictive tasks, (2)
introducing learnable nonlinear transformation between representation and
the loss substantially improves quality learned
representations, and (3) benefits from larger batch sizes
and more training steps compared supervised learning. By combining these
findings, are able considerably outperform previous methods for
self-supervised semi-supervised on ImageNet. A linear classifier
trained self-supervised representations learned by SimCLR achieves 76.5%
top-1 accuracy, which is 7% relative improvement over previous
state-of-the-art, matching performance ResNet-50. When
fine-tuned only 1% labels, achieve 85.8% top-5 accuracy,
outperforming AlexNet with 100X fewer labels.

image analysis, computer science, computer vision, machine learning, visual reasoning, simple framework, visual perception, visual modeling, visual representations, visual data mining, image representation, data science, deep learning, machine vision, contrastive learning, cognitive science",2020,1278,image analysis|computer science|computer vision|machine learning|visual reasoning|simple framework|visual perception|visual modeling|visual representations|visual data mining|image representation|data science|deep learning|machine vision|contrastive learning|cognitive science,https://openalex.org/W3094502228
https://openalex.org/W3012494314,RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds,"RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds

We study the problem of efficient semantic segmentation for large-scale 3D point clouds. By relying on expensive sampling techniques or computationally heavy pre/post-processing steps, most existing approaches are only able to be trained and operate over small-scale In this paper, we introduce RandLA-Net, an lightweight neural architecture directly infer per-point semantics The key our approach is use random instead more complex selection approaches. Although remarkably computation memory efficient, can discard features by chance. To overcome this, a novel local feature aggregation module progressively increase receptive field each point, thereby effectively preserving geometric details. Extensive experiments show that RandLA-Net process 1 million points in single pass with up 200x faster than Moreover, clearly surpasses state-of-the-art two benchmarks Semantic3D SemanticKITTI.

pattern recognition, computer science, multi-view geometry, machine learning, planetary sciences, image analysis, information fusion, point cloud, scene modeling, large-scale point clouds, data science, computational imaging, scene understanding, efficient semantic segmentation, localization, deep learning, machine learning research, machine vision, point cloud processing, computer vision",2020,1126,pattern recognition|computer science|multi-view geometry|machine learning|planetary sciences|image analysis|information fusion|point cloud|scene modeling|large-scale point clouds|data science|computational imaging|scene understanding|efficient semantic segmentation|localization|deep learning|machine learning research|machine vision|point cloud processing|computer vision,
https://openalex.org/W3138516171,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows

This paper presents a new vision Transformer, called Swin that capably serves as general-purpose backbone for computer vision. Challenges in adapting Transformer from language to arise differences between the two domains, such large variations scale of visual entities and high resolution pixels images compared words text. To address these differences, we propose hierarchical whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation non-overlapping local windows while also allowing cross-window connection. architecture has flexibility model at various scales linear computational complexity respect image size. These qualities make it compatible broad range tasks, including classification (87.3 top-1 accuracy on ImageNet-1K) dense prediction tasks object detection (58.7 box AP 51.1 mask COCO test-dev) semantic segmentation (53.5 mIoU ADE20K val). Its performance surpasses previous state-of-the-art margin +2.7 +2.6 COCO, +3.2 ADE20K, demonstrating potential Transformer-based models backbones. design window approach prove beneficial all-MLP architectures. code are publicly available https://github.com/microsoft/Swin-Transformer.

computer vision, swin transformer, computer science, image manipulation, hierarchical vision transformer",2021,11529,computer vision|swin transformer|computer science|image manipulation|hierarchical vision transformer,https://openalex.org/W3210586215|https://openalex.org/W4214893857|https://openalex.org/W4312443924|https://openalex.org/W3212386989|https://openalex.org/W4212875960|https://openalex.org/W4312560592|https://openalex.org/W4312815172
https://openalex.org/W2962730651,OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields,"OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields

Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people images and videos. In this work, we present realtime approach detect the multiple image. The proposed method uses nonparametric representation, which refer as Part Affinity Fields (PAFs), learn associate body parts with individuals This bottom-up system achieves high accuracy performance, regardless number previous PAFs part location were refined simultaneously across training stages. We demonstrate that PAF-only refinement rather than both PAF results substantial increase runtime performance accuracy. also first combined foot keypoint detector, based on internal annotated dataset publicly released. show detector not only reduces inference time compared running them sequentially, but maintains each individually. work has culminated release OpenPose, open-source for detection, including body, foot, hand, facial keypoints.

image analysis, human pose estimation, computer science, multi-view geometry, computer vision, machine learning, pose estimation, gesture recognition, 3d pose estimation, machine vision, part affinity fields",2021,3042,image analysis|human pose estimation|computer science|multi-view geometry|computer vision|machine learning|pose estimation|gesture recognition|3d pose estimation|machine vision|part affinity fields,
https://openalex.org/W3014641072,Deep High-Resolution Representation Learning for Visual Recognition,"Deep High-Resolution Representation Learning for Visual Recognition

High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image a low-resolution representation through subnetwork that is formed by connecting high-to-low resolution convolutions <i>in series</i> (e.g., ResNet, VGGNet), then recover high-resolution from encoded representation. Instead, our proposed network, named High-Resolution Network (HRNet), maintains whole process. There two key characteristics: (i) Connect convolution streams parallel</i> (ii) repeatedly exchange information across resolutions. The benefit resulting semantically richer spatially more precise. We show superiority of HRNet in wide range applications, including detection, suggesting stronger backbone computer problems. All codes available at <uri>https://github.com/HRNet</uri>.

computer science, high resolution, machine learning, visual science, visual recognition, image analysis, feature detection, cognitive science, data science, image representation, vision recognition, computational imaging, deep high-resolution representation, deep learning, machine vision, digital image processing, feature extraction, computer vision, image classification",2021,2367,computer science|high resolution|machine learning|visual science|visual recognition|image analysis|feature detection|cognitive science|data science|image representation|vision recognition|computational imaging|deep high-resolution representation|deep learning|machine vision|digital image processing|feature extraction|computer vision|image classification,https://openalex.org/W3138516171
https://openalex.org/W3132455321,Image Segmentation Using Deep Learning: A Survey,"Image Segmentation Using Deep Learning: A Survey

Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical analysis, robotic perception, video surveillance, augmented reality, compression, among others, numerous algorithms are found the literature. Against this backdrop, broad success of deep learning (DL) has prompted development new approaches leveraging DL models. We provide comprehensive review recent literature, covering spectrum pioneering efforts semantic instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale pyramid-based approaches, recurrent visual attention models, generative models adversarial settings. investigate relationships, strengths, challenges these DL-based examine widely used datasets, compare performances, discuss promising research directions.

pattern recognition, computer science, scene understanding, computer vision, machine learning, deep learning, image segmentation, digital image processing",2021,2095,pattern recognition|computer science|scene understanding|computer vision|machine learning|deep learning|image segmentation|digital image processing,https://openalex.org/W3132455321|https://openalex.org/W4214893857
https://openalex.org/W3131500599,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,"Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions

Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed image classification specifically, we introduce Pyramid (PVT), which overcomes difficulties of porting to various PVT has several merits compared current state arts. (1) Different from ViT typically yields low-resolution outputs and incurs high computational memory costs, not only can be trained on partitions an achieve output resolution, is important prediction, but also uses progressive shrinking pyramid reduce computations large feature maps. (2) inherits advantages both CNN Transformer, making it unified vision tasks without convolutions, where used as direct replacement backbones. (3) We validate through extensive experiments, showing boosts performance downstream tasks, including object detection, instance semantic segmentation. For example, with comparable number parameters, PVT+RetinaNet achieves 40.4 AP COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute (see Figure 2). hope could, serre alternative useful pixel-level predictions facilitate future research.

computer vision, computational imaging, neural computation, dense prediction, versatile backbone, computer science, convolutional neural network, machine learning, image representation, deep learning, pyramid vision transformer, machine learning research, image analysis",2021,2089,computer vision|computational imaging|neural computation|dense prediction|versatile backbone|computer science|convolutional neural network|machine learning|image representation|deep learning|pyramid vision transformer|machine learning research|image analysis,https://openalex.org/W3138516171|https://openalex.org/W4214493665|https://openalex.org/W4214612132|https://openalex.org/W3157506437|https://openalex.org/W4206706211|https://openalex.org/W3212386989|https://openalex.org/W4212875960
https://openalex.org/W3170841864,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,"Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers

Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts larger receptive fields. Since context modeling is critical for segmentation, latest efforts have been focused on increasing field, through either dilated/atrous convolutions or inserting attention modules. However, based FCN architecture remains unchanged. In this paper, we aim to provide alternative perspective by treating as sequence-to-sequence prediction task. Specifically, deploy pure transformer (i.e., without convolution reduction) encode image sequence of patches. With global modeled in every layer transformer, can be combined simple decoder powerful model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state art ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) competitive results Cityscapes. Particularly, achieve first position highly test server leaderboard day submission.

computer science, natural language processing, sequence-to-sequence perspective, computer vision, machine learning, deep learning, image segmentation, semantic segmentation",2021,1934,computer science|natural language processing|sequence-to-sequence perspective|computer vision|machine learning|deep learning|image segmentation|semantic segmentation,https://openalex.org/W3138516171|https://openalex.org/W4214612132|https://openalex.org/W4214893857|https://openalex.org/W4206706211|https://openalex.org/W3212386989|https://openalex.org/W4212875960|https://openalex.org/W4312815172
https://openalex.org/W3159481202,Emerging Properties in Self-Supervised Vision Transformers,"Emerging Properties in Self-Supervised Vision Transformers

In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared convolutional networks (convnets). Beyond the fact adapting methods architecture works particularly well, make following observations: first, ViT features contain explicit information about semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor convnets. Second, these are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet a small ViT. Our study underlines importance momentum encoder [26], multi-crop training [9], and use patches ViTs. We implement our findings into simple method, called DINO, interpret form self-distillation no labels. show synergy between DINO ViTs by achieving 80.1% in linear evaluation ViT-Base.

computational imaging, computer science, self-supervised vision transformers, computer vision, machine learning, self-supervised learning, deep learning, neural network (machine learning), vision recognition, machine vision, vision robotics",2021,1623,computational imaging|computer science|self-supervised vision transformers|computer vision|machine learning|self-supervised learning|deep learning|neural network (machine learning)|vision recognition|machine vision|vision robotics,https://openalex.org/W4313156423|https://openalex.org/W3212386989|https://openalex.org/W4212875960
https://openalex.org/W3023371261,Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,"Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey

Large-scale labeled data are generally required to train deep neural networks in order obtain better performance visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset unsupervised methods, self-supervised methods proposed learn general image video features unlabeled without using any human-annotated labels. This paper provides an review learning-based videos. First, the motivation, pipeline, terminologies this field described. Then common network architectures that used summarized. Next, schema evaluation metrics reviewed followed by commonly datasets images, videos, audios, 3D data, well existing methods. Finally, quantitative comparisons on benchmark summarized discussed both learning. At last, is concluded lists set promising future directions

computer science, feature learning, machine learning, visual science, image analysis, supervised learning, feature detection, cognitive science, data science, image representation, feature (computer vision), self-supervised visual feature, computational imaging, deep learning, machine learning research, machine vision, computer vision, self-supervised learning, deep neural networks, neural network (machine learning)",2021,1317,computer science|feature learning|machine learning|visual science|image analysis|supervised learning|feature detection|cognitive science|data science|image representation|feature (computer vision)|self-supervised visual feature|computational imaging|deep learning|machine learning research|machine vision|computer vision|self-supervised learning|deep neural networks|neural network (machine learning),https://openalex.org/W3132455321|https://openalex.org/W4206706211|https://openalex.org/W2938260698
https://openalex.org/W3121523901,Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet,"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet

Transformers, which are popular for language modeling, have been explored solving vision tasks recently, e.g., the Vision Transformer (ViT) image classification. The ViT model splits each into a sequence of tokens with fixed length and then applies multiple layers to their global relation However, achieves inferior performance CNNs when trained from scratch on midsize dataset like ImageNet. We find it is because: 1) simple tokenization input images fails important local structure such as edges lines among neighboring pixels, leading low training sample efficiency; 2) redundant attention backbone design leads limited feature richness computation budgets samples. To overcome limitations, we propose new Tokens-To-Token (T2T-VTT), incorporates layer-wise Tokens-to-Token (T2T) transformation progressively structurize by recursively aggregating Tokens one Token (Tokens-to-Token), that represented surrounding can be modeled reduced; an efficient deep-narrow transformer motivated CNN architecture after empirical study. Notably, T2T-ViT reduces parameter count MACs vanilla half, while achieving more than 3.0% improvement It also outperforms ResNets comparable MobileNets directly For example, size ResNet50 (21.5M parameters) achieve 83.3% top1 accuracy in resolution 384x384 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>

computer vision, object manipulation, robot learning, computer science, machine learning, tokens-to-token vit, deep learning, training vision transformers",2021,1234,computer vision|object manipulation|robot learning|computer science|machine learning|tokens-to-token vit|deep learning|training vision transformers,https://openalex.org/W3138516171|https://openalex.org/W3131500599|https://openalex.org/W4214493665|https://openalex.org/W4206706211|https://openalex.org/W3212386989
https://openalex.org/W3013529009,Deep Learning for Image Super-Resolution: A Survey,"Deep Learning for Image Super-Resolution: A Survey

Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution images and videos in computer vision. Recent years have witnessed remarkable progress super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances approaches. In general, we can roughly group existing studies SR techniques into three major categories: supervised SR, unsupervised domain-specific SR. addition, also cover some other issues, such as publicly available benchmark datasets performance evaluation metrics. Finally, conclude this by highlighting several future directions open issues which should be further addressed community future.

image analysis, computational imaging, computer science, super-resolution imaging, synthetic image generation, computer vision, high resolution, digital imaging, data science, deep learning, image representation, image super-resolution, image resolution, neural network (machine learning), machine vision, digital image processing",2021,1158,image analysis|computational imaging|computer science|super-resolution imaging|synthetic image generation|computer vision|high resolution|digital imaging|data science|deep learning|image representation|image super-resolution|image resolution|neural network (machine learning)|machine vision|digital image processing,
https://openalex.org/W4214493665,CvT: Introducing Convolutions to Vision Transformers,"CvT: Introducing Convolutions to Vision Transformers

We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision (ViT) performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: hierarchy Transformers containing convolutional token embedding, block leveraging projection. These changes introduce desirable properties neural networks (CNNs) architecture (i.e. shift, scale, distortion invariance) while maintaining merits dynamic attention, global context, better generalization). validate CvT conducting extensive experiments, showing approach achieves state-of-the-art over other ResNets on ImageNet-1k, with fewer parameters lower FLOPs. In addition, gains are maintained when pretrained larger datasets (e.g. ImageNet-22k) fine-tuned downstream tasks. Pretrained ImageNet-22k, our CvT-W24 obtains top-1 accuracy 87.7% ImageNet-1k val set. Finally, results show positional encoding, crucial component existing Transformers, can be safely re-moved model, simplifying design for higher resolution Code will released at https://github.com/microsoft/CvT.

introducing convolutions, digital image processing, vision transformers, computer vision, computational imaging, machine vision, vision recognition, motion detection, video transformer, computer science, convolutional neural network, machine learning, deep learning, image analysis",2021,1149,introducing convolutions|digital image processing|vision transformers|computer vision|computational imaging|machine vision|vision recognition|motion detection|video transformer|computer science|convolutional neural network|machine learning|deep learning|image analysis,https://openalex.org/W4312443924|https://openalex.org/W4206706211|https://openalex.org/W3212386989
https://openalex.org/W2898200825,GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild,"GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking in the Wild

We introduce here a large tracking database that offers an unprecedentedly wide coverage of common moving objects in the wild, called GOT-10k. Specifically, GOT-10k is built upon backbone WordNet structure and it populates majority over 560 classes 87 motion patterns, magnitudes wider than most recent similar-scale counterparts. The contributions this paper are summarized following: (1) 10,000 video segments with more 1.5 million manually labeled bounding boxes, enabling unified training stable evaluation deep trackers. (2) by far first trajectory dataset uses semantic hierarchy to guide class population. (3) For time, introduces one-shot protocol for tracker evaluation, where test zero-overlapped. avoids biased results towards familiar promotes generalization development. (4) conduct extensive experiments 39 typical algorithms on analyze their paper. (5) Finally, we develop comprehensive platform community full-featured toolkits, online server, responsive leaderboard. annotations GOT-10k's data kept private avoid tuning parameters it. database, server baseline available at http://got-10k.aitestunion.com.

pattern recognition, computer science, machine learning, image analysis, information fusion, generic object, cognitive science, object detection, data science, moving object tracking, large-scale datasets, benchmark datasets, localization, deep learning, machine vision, object tracking, computer vision, large high-diversity benchmark, vehicular technology",2021,1106,pattern recognition|computer science|machine learning|image analysis|information fusion|generic object|cognitive science|object detection|data science|moving object tracking|large-scale datasets|benchmark datasets|localization|deep learning|machine vision|object tracking|computer vision|large high-diversity benchmark|vehicular technology,
https://openalex.org/W3121661546,EnlightenGAN: Deep Light Enhancement Without Paired Supervision,"EnlightenGAN: Deep Light Enhancement Without Paired Supervision

Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light enhancement problem, where practice it extremely challenging to simultaneously take normal-light photo same visual scene. We propose highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light pairs, yet proves generalize very well on various real-world test images. Instead supervising learning using ground truth data, we regularize unpaired information extracted from input itself, benchmark series innovations for including global-local discriminator structure, self-regularized perceptual loss fusion, attention mechanism. Through extensive experiments, our proposed approach outperforms recent under variety metrics terms quality subjective user study. Thanks great flexibility brought by training, EnlightenGAN demonstrated easily adaptable enhancing images domains. Our codes pre-trained models available at: https://github.com/VITA-Group/EnlightenGAN.

computational imaging, deep light enhancement, computer science, image enhancement, computer vision, machine learning, paired supervision, deep learning",2021,1092,computational imaging|deep light enhancement|computer science|image enhancement|computer vision|machine learning|paired supervision|deep learning,
https://openalex.org/W3167976421,RepVGG: Making VGG-style ConvNets Great Again,"RepVGG: Making VGG-style ConvNets Great Again

We present a simple but powerful architecture of convolutional neural network, which has VGG-like inference-time body composed nothing stack 3 × convolution and ReLU, while the training-time model multi-branch topology. Such decoupling is realized by structural re-parameterization technique so that named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy, first time for plain model, to best our knowledge. NVIDIA 1080Ti GPU, models run 83% faster than ResNet-50 or 101% ResNet-101 with higher accuracy show favorable accuracy-speed trade-off compared state-of-the-art like EfficientNet RegNet. The code trained are available at https://github.com/megvii-model/RepVGG.

computer vision, pattern recognition, vgg-style convnets, computer science, convolutional neural network, machine learning, feature detection, deep learning",2021,1042,computer vision|pattern recognition|vgg-style convnets|computer science|convolutional neural network|machine learning|feature detection|deep learning,
https://openalex.org/W4214612132,ViViT: A Video Vision Transformer,"ViViT: A Video Vision Transformer

We present pure-transformer based models for video classification, drawing upon the recent success of such in image classification. Our model extracts spatiotemporal tokens from input video, which are then encoded by a series transformer layers. In order to handle long sequences encountered we propose several, efficient variants our factorise spatial- and temporal-dimensions input. Although transformer-based known only be effective when large training datasets available, show how can effectively regularise during leverage pretrained able train on comparatively small datasets. conduct thorough ablation studies, achieve state-of-the-art results multiple classification benchmarks including Kinetics 400 600, Epic Kitchens, Something-Something v2 Moments Time, outperforming prior methods deep 3D convolutional networks.

video processing, computer vision, video manipulation, video understanding, video transformer, computer science, video observation, video vision transformer",2021,986,video processing|computer vision|video manipulation|video understanding|video transformer|computer science|video observation|video vision transformer,https://openalex.org/W4214893857|https://openalex.org/W4312560592
https://openalex.org/W3210586215,TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios,"TPH-YOLOv5: Improved YOLOv5 Based on Transformer Prediction Head for Object Detection on Drone-captured Scenarios

Object detection on drone-captured scenarios is a recent popular task. As drones always navigate in different altitudes, the object scale varies violently, which burdens optimization of networks. Moreover, high-speed and low-altitude flight bring motion blur densely packed objects, leads to great challenge distinction. To solve two issues mentioned above, we propose TPH-YOLOv5. Based YOLOv5, add one more prediction head detect different-scale objects. Then replace original heads with Transformer Prediction Heads (TPH) explore potential self-attention mechanism. We also integrate convolutional block attention model (CBAM) find region dense achieve improvement our proposed TPH-YOLOv5, provide bags useful strategies such as data augmentation, multi-scale testing, multi-model integration utilizing extra classifier. Extensive experiments dataset VisDrone2021 show that TPH-YOLOv5 have good performance impressive interpretability scenarios. On DET-test-challenge dataset, AP result are 39.18%, better than previous SOTA method (DPNetV3) by 1.81%. VisDrone Challenge 2021, wins 5 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">th</sup> place achieves well-matched results 1 xmlns:xlink=""http://www.w3.org/1999/xlink"">st</sup> (AP 39.43%). Compared baseline (YOLOv5), improves about 7%, encouraging competitive.

computer vision, unmanned aerial vehicle, machine vision, drone-captured scenarios, computer science, transformer prediction head, drone, machine learning, object detection, deep learning",2021,985,computer vision|unmanned aerial vehicle|machine vision|drone-captured scenarios|computer science|transformer prediction head|drone|machine learning|object detection|deep learning,
https://openalex.org/W3157506437,MLP-Mixer: An all-MLP Architecture for Vision,"MLP-Mixer: An all-MLP Architecture for Vision

Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as Vision Transformer, have also become popular. In this paper we show that while convolutions and attention both sufficient good performance, neither of them necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types layers: one with MLPs applied independently to image patches (i.e. ""mixing"" per-location features), across spatial information). When trained large datasets, or modern regularization schemes, attains competitive scores classification benchmarks, pre-training inference cost comparable state-of-the-art models. hope these results spark further research beyond realms well established CNNs Transformers.

image analysis, computer science, all-mlp architecture, computer vision, deep learning, visual perception, image representation, vision recognition, machine vision",2021,940,image analysis|computer science|all-mlp architecture|computer vision|deep learning|visual perception|image representation|vision recognition|machine vision,https://openalex.org/W3138516171|https://openalex.org/W3212386989
https://openalex.org/W4285531802,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)

Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoderdecoder architecture.The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts larger receptive fields.Since context modeling is critical for segmentation, latest efforts have been focused on increasing field, through either dilated/atrous convolutions or inserting attention modules.However, encoder-decoder based FCN architecture remains unchanged.In this paper, we aim to provide alternative perspective by treating as sequence-to-sequence prediction task.Specifically, deploy pure transformer (i.e., without convolution reduction) encode image sequence of patches.With global modeled in every layer transformer, can be combined simple decoder powerful model, termed SEgmentation TRansformer (SETR).Extensive experiments show that SETR achieves new state art ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) competitive results Cityscapes.Particularly, achieve first position highly test server leaderboard day submission.

pattern recognition, computer science, computer vision, feature (computer vision)",2021,893,pattern recognition|computer science|computer vision|feature (computer vision),
https://openalex.org/W4214893857,Segmenter: Transformer for Semantic Segmentation,"Segmenter: Transformer for Semantic Segmentation

Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. contrast convolution-based methods, our approach allows global context already first layer throughout network. We build on recent Vision Transformer (ViT) extend it To do so, rely output embeddings corresponding obtain class labels from these embed-dings with point-wise linear decoder or mask trans-former decoder. leverage models pre-trained classification show that can fine-tune them moderate sized datasets available The excellent results already, but performance be further improved by generating masks. conduct an extensive ablation study impact different parameters, in particular better large small patch sizes. Segmenter attains It outperforms state art both ADE20K Pascal Context competitive Cityscapes.

image analysis, computer science, scene understanding, feature extraction, computer vision, machine learning, deep learning, image representation, image segmentation, machine vision, digital image processing, semantic segmentation",2021,838,image analysis|computer science|scene understanding|feature extraction|computer vision|machine learning|deep learning|image representation|image segmentation|machine vision|digital image processing|semantic segmentation,https://openalex.org/W4206706211|https://openalex.org/W4312815172
https://openalex.org/W4312443924,A ConvNet for the 2020s,"A ConvNet for the 2020s

The ""Roaring 20s"" of visual recognition began with the introduction Vision Transformers (ViTs), which quickly superseded ConvNets as state-of-the-art image classification model. A vanilla ViT, on other hand, faces difficulties when applied to general computer vision tasks such object detection and semantic segmentation. It is hierarchical (e.g., Swin Transformers) that reintroduced several ConvNet priors, making practically viable a generic backbone demonstrating remarkable performance wide variety tasks. However, effectiveness hybrid approaches still largely credited intrinsic superiority Transformers, rather than inherent inductive biases convolutions. In this work, we reexamine design spaces test limits what pure can achieve. We gradually ""modernize"" standard ResNet toward Transformer, discover key components contribute difference along way. outcome exploration family models dubbed ConvNeXt. Constructed entirely from modules, ConvNeXts compete favorably in terms accuracy scalability, achieving 87.8% ImageNet top-1 outperforming COCO ADE20K segmentation, while maintaining simplicity efficiency ConvNets.

computer vision, machine vision, neural network (machine learning), computer science, convolutional neural network, machine learning, deep learning, image analysis",2022,2053,computer vision|machine vision|neural network (machine learning)|computer science|convolutional neural network|machine learning|deep learning|image analysis,
https://openalex.org/W4313156423,Masked Autoencoders Are Scalable Vision Learners,"Masked Autoencoders Are Scalable Vision Learners

This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct missing pixels. It based on two core designs. First, develop an asymmetric encoder-decoder architecture, with encoder operates only visible subset (without tokens), along a lightweight decoder reconstructs original from latent representation tokens. Second, find masking high proportion image, e.g., 75%, yields nontrivial meaningful self-supervisory task. Coupling these designs enables us to train large models efficiently effectively: accelerate training (by 3× or more) improve accuracy. allows learning high-capacity generalize well: vanilla ViT-Huge model achieves best accuracy (87.8%) among methods use ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining promising scaling behavior.

image analysis, autoencoders, computer science, convolutional neural network, scalable vision learners, scene understanding, computer vision, machine learning, digital learning, deep learning, image representation, autonomous learning, neural network (machine learning), machine vision",2022,2022,image analysis|autoencoders|computer science|convolutional neural network|scalable vision learners|scene understanding|computer vision|machine learning|digital learning|deep learning|image representation|autonomous learning|neural network (machine learning)|machine vision,
https://openalex.org/W4206706211,Transformers in Vision: A Survey,"Transformers in Vision: A Survey

Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application computer problems. Among salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of as compared recurrent networks e.g., Long short-term memory (LSTM). Different convolutional networks, require minimal inductive biases for design are naturally suited set-functions. Furthermore, straightforward allows multiple modalities (e.g., images, videos, text speech) using similar blocks demonstrates excellent scalability very large capacity huge datasets. These strengths led exciting progress a number networks. This survey aims provide comprehensive overview in discipline. We start with an introduction fundamental concepts behind success i.e., self-attention, large-scale pre-training, bidirectional encoding. then cover extensive applications transformers including popular recognition image classification, object detection, action recognition, segmentation), generative modeling, multi-modal visual-question answering, visual reasoning, grounding), video activity forecasting), low-level super-resolution, enhancement, colorization) 3D analysis point cloud classification segmentation). compare respective advantages limitations techniques both terms architectural experimental value. Finally, we open research directions possible future works.

vision rehabilitation, image analysis, 3d vision, computer vision, cognitive science, visual system, visual perception, image representation, visual science, machine vision, 3d computer vision, computer stereo vision",2022,1324,vision rehabilitation|image analysis|3d vision|computer vision|cognitive science|visual system|visual perception|image representation|visual science|machine vision|3d computer vision|computer stereo vision,https://openalex.org/W3212386989
https://openalex.org/W3212386989,Attention mechanisms in computer vision: A survey,"Attention mechanisms in computer vision: A survey

Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating aspect human visual system. Such an mechanism be regarded as a dynamic weight adjustment process based on features input image. Attention have achieved great success many tasks, including image classification, object detection, semantic segmentation, video understanding, generation, 3D vision, multi-modal tasks self-supervised learning. In survey, we provide comprehensive review various categorize them according to approach, such channel attention, spatial temporal branch attention; related repository https://github.com/MenghaoGuo/Awesome-Vision-Attentions is dedicated collecting work. We also suggest future directions for research.

computer vision, cognitive science, attention mechanisms, visual processing, attention",2022,1035,computer vision|cognitive science|attention mechanisms|visual processing|attention,
https://openalex.org/W4212875960,UNETR: Transformers for 3D Medical Image Segmentation,"UNETR: Transformers for 3D Medical Image Segmentation

Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since past decade. In FCNNs, encoder plays an integral role by learning both global local features contextual representations which can be utilized semantic output prediction decoder. Despite their success, locality convolutional layers in limits capability long-range spatial dependencies. Inspired recent success transformers Natural Language Processing (NLP) sequence learning, we reformulate task volumetric (3D) as a sequence-to-sequence problem. We introduce novel architecture, dubbed UNEt TRansformers (UNETR), that utilizes transformer to learn input volume effectively capture multi-scale information, while also following successful ""U-shaped"" network design The is directly connected decoder via skip connections at different resolutions compute final output. validated performance our method on Multi Atlas Labeling Beyond Cranial Vault (BTCV) dataset multi-organ Medical Segmentation Decathlon (MSD) brain tumor spleen tasks. Our benchmarks demonstrate new state-of-the-art BTCV leaderboard.

medical image computing, medical imaging, biomedical imaging, medical image analysis, computer vision, biomedical engineering, 3d reconstruction, 3d imaging, image segmentation, medical image segmentation, digital image processing, 3d computer vision",2022,942,medical image computing|medical imaging|biomedical imaging|medical image analysis|computer vision|biomedical engineering|3d reconstruction|3d imaging|image segmentation|medical image segmentation|digital image processing|3d computer vision,
https://openalex.org/W2799041689,Deep Facial Expression Recognition: A Survey,"Deep Facial Expression Recognition: A Survey

With the transition of facial expression recognition (FER) from laboratory-controlled to challenging in-the-wild conditions and recent success deep learning techniques in various fields, neural networks have increasingly been leveraged learn discriminative representations for automatic FER. Recent FER systems generally focus on two important issues: overfitting caused by a lack sufficient training data expression-unrelated variations, such as illumination, head pose identity bias. In this paper, we provide comprehensive survey FER, including datasets algorithms that insights into these intrinsic problems. First, describe standard pipeline system with related background knowledge suggestions applicable implementations each stage. We then introduce available are widely used literature accepted selection evaluation principles datasets. For state art review existing novel strategies designed based both static images dynamic image sequences, discuss their advantages limitations. Competitive performances benchmarks also summarized section. extend our additional issues application scenarios. Finally, remaining challenges corresponding opportunities field well future directions design robust systems.

image analysis, pattern recognition, computer science, affective computing, computer vision, machine learning, facial recognition system, data science, computational intelligence, deep learning, emotion recognition, machine vision, face detection, facial expression recognition",2022,717,image analysis|pattern recognition|computer science|affective computing|computer vision|machine learning|facial recognition system|data science|computational intelligence|deep learning|emotion recognition|machine vision|face detection|facial expression recognition,
https://openalex.org/W4226051885,Event-Based Vision: A Survey,"Event-Based Vision: A Survey

Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output stream events encode the time, location sign changes. offer attractive properties compared to traditional high temporal resolution (in order <inline-formula><tex-math notation=""LaTeX"">$\mu$</tex-math></inline-formula> s), very dynamic range (140 dB versus 60 dB), low power consumption, pixel bandwidth (on kHz) resulting in reduced motion blur. Hence, event have large potential for robotics computer vision challenging scenarios cameras, such as low-latency, speed, range. However, novel methods required process unconventional these unlock their potential. This paper provides comprehensive overview emerging field event-based vision, with focus on applications algorithms developed outstanding cameras. We present working principle, actual available tasks been used for, low-level (feature detection tracking, optic flow, etc.) high-level (reconstruction, segmentation, recognition). also discuss techniques events, including learning-based techniques, well specialized processors sensors, spiking neural networks. Additionally, we highlight challenges remain be tackled opportunities lie ahead search more efficient, way machines perceive interact world.

image analysis, event processing, scene interpretation, event-driven monitoring, scene understanding, complex event processing, computer vision, cognitive science, visual perception, visual science, narrative, event-based vision",2022,692,image analysis|event processing|scene interpretation|event-driven monitoring|scene understanding|complex event processing|computer vision|cognitive science|visual perception|visual science|narrative|event-based vision,
https://openalex.org/W4312560592,Video Swin Transformer,"Video Swin Transformer

The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These models are all built layers that globally connect patches across spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in which leads better speed-accuracy trade-off compared previous approaches compute self-attention even with spatial-temporal factorization. proposed architecture realized by adapting Swin designed for image domain, while continuing leverage power pre-trained models. Our approach achieves state-of-the-art broad range benchmarks, including action (84.9 top-l Kinetics-400 85.9 Kinetics-600 ~20× less pre-training data ~3× smaller model size) (69.6 Something-Something v2).

computer science, video transmission, electrical engineering, computer vision, video manipulation, video swin transformer, video adaptation, video communication, video transformer, computer engineering",2022,652,computer science|video transmission|electrical engineering|computer vision|video manipulation|video swin transformer|video adaptation|video communication|video transformer|computer engineering,
https://openalex.org/W3081167590,Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer,"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer

The success of monocular depth estimation relies on large and diverse training sets. Due to the challenges associated with acquiring dense ground-truth across different environments at scale, a number datasets distinct characteristics biases have emerged. We develop tools that enable mixing multiple during training, even if their annotations are incompatible. In particular, we propose robust objective is invariant changes in range advocate use principled multi-objective learning combine data from sources, highlight importance pretraining encoders auxiliary tasks. Armed these tools, experiment five datasets, including new, massive source: 3D films. To demonstrate generalization power our approach zero-shot cross-dataset transfer, i.e. evaluate were not seen training. experiments confirm complementary sources greatly improves estimation. Our clearly outperforms competing methods setting new state art for

image analysis, computational imaging, 3d vision, mixing datasets, biomedical imaging, monocular depth estimation, stereoscopic processing, zero-shot cross-dataset transfer, computer vision, machine learning, data science, deep learning, large-scale datasets, machine vision, digital image processing, computer stereo vision",2022,650,image analysis|computational imaging|3d vision|mixing datasets|biomedical imaging|monocular depth estimation|stereoscopic processing|zero-shot cross-dataset transfer|computer vision|machine learning|data science|deep learning|large-scale datasets|machine vision|digital image processing|computer stereo vision,
https://openalex.org/W4312815172,Masked-attention Mask Transformer for Universal Image Segmentation,"Masked-attention Mask Transformer for Universal Image Segmentation

Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the each task differ, current research focuses on designing spe-cialized architectures for We present Masked- attention Mask Transformer (Mask2Former), new archi-tecture capable addressing any image (panoptic, semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing effort at least three times, it outperforms best specialized significant margin four popular datasets. Most no-tably, Mask2Former sets state-of-the-art panoptic (57.8 PQ COCO), (50.1 AP COCO) and semantic (57.7 mIoU onADE20K).

computer science, masked-attention mask transformer, computer vision, image segmentation, universal image segmentation",2022,627,computer science|masked-attention mask transformer|computer vision|image segmentation|universal image segmentation,
https://openalex.org/W2938260698,Salient Object Detection in the Deep Learning Era: An In-Depth Survey,"Salient Object Detection in the Deep Learning Era: An In-Depth Survey

As an essential problem in computer vision, salient object detection (SOD) has attracted increasing amount of research attention over the years. Recent advances SOD are predominantly led by deep learning-based solutions (named SOD). To enable in-depth understanding SOD, this paper, we provide a comprehensive survey covering various aspects, ranging from algorithm taxonomy to unsolved issues. In particular, first review algorithms different perspectives, including network architecture, level supervision, learning paradigm, and object-/instance-level detection. Following that, summarize analyze existing datasets evaluation metrics. Then, benchmark large group representative models, detailed analyses comparison results. Moreover, study performance under attribute settings, which not been thoroughly explored previously, constructing novel dataset with rich annotations types, challenging factors, scene categories. We further analyze, for time field, robustness models random input perturbations adversarial attacks. also look into generalization difficulty datasets. Finally, discuss several open issues outline future directions. All saliency prediction maps, our constructed annotations, codes publicly available at https://github.com/wenguanwang/SODsurvey.

pattern recognition, computer science, machine learning, image analysis, information fusion, cognitive science, object detection, data science, image representation, vision recognition, computational imaging, scene understanding, localization, deep learning, machine vision, visual question answering, salient object detection, object recognition, computer vision, deep learning era",2022,546,pattern recognition|computer science|machine learning|image analysis|information fusion|cognitive science|object detection|data science|image representation|vision recognition|computational imaging|scene understanding|localization|deep learning|machine vision|visual question answering|salient object detection|object recognition|computer vision|deep learning era,
https://openalex.org/W3155072588,Image Super-Resolution Via Iterative Refinement,"Image Super-Resolution Via Iterative Refinement

We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models (Ho et al. 2020), (Sohl-Dickstein 2015) image-to-image translation, and performs super-resolution through a stochastic iterative process. Output images are initialized with pure Gaussian noise iteratively refined using U-Net architecture that is trained on at various levels, conditioned low-resolution input image. exhibits strong performance tasks different magnification factors, faces natural images. conduct human evaluation standard 8× face task CelebA-HQ for which achieves fool rate close 50%, suggesting photo-realistic outputs, while GAN baselines do not exceed of 34%. evaluate 4× ImageNet, where outperforms in classification accuracy ResNet-50 classifier high-resolution further show the effectiveness cascaded generation, generative model chained synthesize competitive FID scores class-conditional 256×256 ImageNet generation challenge.

image analysis, computational imaging, computer science, super-resolution imaging, computer vision, iterative refinement, image representation, image super-resolution, image resolution, digital image processing",2022,497,image analysis|computational imaging|computer science|super-resolution imaging|computer vision|iterative refinement|image representation|image super-resolution|image resolution|digital image processing,
https://openalex.org/W2965898445,Hierarchical Deep Click Feature Prediction for Fine-Grained Image Recognition,"Hierarchical Deep Click Feature Prediction for Fine-Grained Image Recognition

The click feature of an image, defined as the user frequency vector image on a predefined word vocabulary, is known to effectively reduce semantic gap for fine-grained recognition. Unfortunately, data are usually absent in practice. It remains challenging predict from visual feature, because always noisy and sparse. In this paper, we devise Hierarchical Deep Word Embedding (HDWE) model by integrating sparse constraints improved RELU operator address prediction features. HDWE coarse-to-fine predictor that learned with help auxiliary dataset containing information. can therefore discover hierarchy semantics. We evaluate three dog one bird datasets, which Clickture-Dog Clickture-Bird utilized datasets provide data, respectively. Our empirical studies show has 1) higher recognition accuracy, 2) larger compression ratio, 3) good one-shot learning ability scalability unseen categories.

image analysis, pattern recognition, computer science, computational imaging, object recognition, feature learning, feature extraction, computer vision, machine learning, feature detection, image sequence analysis, data science, fine-grained image recognition, deep learning",2022,459,image analysis|pattern recognition|computer science|computational imaging|object recognition|feature learning|feature extraction|computer vision|machine learning|feature detection|image sequence analysis|data science|fine-grained image recognition|deep learning,
https://openalex.org/W3212516020,Palette: Image-to-Image Diffusion Models,"Palette: Image-to-Image Diffusion Models

This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this four challenging tasks, namely colorization, inpainting, uncropping, JPEG restoration. Our simple implementation of outperforms strong GAN regression baselines all without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss sophisticated new techniques needed. We uncover the impact an L2 vs. L1 in denoising objective sample diversity, demonstrate importance self-attention neural through empirical studies. Importantly, we advocate evaluation protocol ImageNet, with human quality scores (FID, Inception Score, Classification Accuracy pre-trained ResNet-50, Perceptual Distance against original images). expect standardized to play role advancing research. Finally, show that generalist, multi-task model performs as well better than specialist counterparts. Check out https://diffusion-palette.github.io/ overview results code.

computer science, diffusion, computer vision, information diffusion, image-to-image diffusion models",2022,453,computer science|diffusion|computer vision|information diffusion|image-to-image diffusion models,https://openalex.org/W3155072588
https://openalex.org/W3136761610,Align Deep Features for Oriented Object Detection,"Align Deep Features for Oriented Object Detection

The past decade has witnessed significant progress on detecting objects in aerial images that are often distributed with large-scale variations and arbitrary orientations. However, most of existing methods rely heuristically defined anchors different scales, angles, aspect ratios, usually suffer from severe misalignment between anchor boxes (ABs) axis-aligned convolutional features, which lead to the common inconsistency classification score localization accuracy. To address this issue, we propose a <italic xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">single-shot alignment network</i> (S <sup xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> A-Net) consisting two modules: feature module (FAM) an oriented detection (ODM). FAM can generate high-quality refinement network adaptively align features according ABs novel convolution. ODM first adopts active rotating filters encode orientation information then produces orientation-sensitive orientation-invariant alleviate Besides, further explore approach detect large-size images, leads better trade-off speed Extensive experiments demonstrate our method achieve state-of-the-art performance commonly used objects' data sets (i.e., DOTA HRSC2016) while keeping high efficiency.

image analysis, align deep features, computer science, feature extraction, computer vision, machine learning, feature detection, object detection, deep learning, oriented object detection, machine vision, feature (computer vision)",2022,422,image analysis|align deep features|computer science|feature extraction|computer vision|machine learning|feature detection|object detection|deep learning|oriented object detection|machine vision|feature (computer vision),
https://openalex.org/W3130754787,SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images,"SNUNet-CD: A Densely Connected Siamese Network for Change Detection of VHR Images

Change detection is an important task in remote sensing (RS) image analysis. It widely used natural disaster monitoring and assessment, land resource planning, other fields. As a pixel-to-pixel prediction task, change sensitive about the utilization of original position information. Recent methods always focus on extraction deep semantic feature, but ignore importance shallow-layer information containing high-resolution fine-grained features, this often leads to uncertainty pixels at edge changed target determination miss small targets. In letter, we propose densely connected siamese network for detection, namely SNUNet-CD (the combination Siamese NestedUNet). alleviates loss localization layers neural through compact transmission between encoder decoder, decoder decoder. addition, Ensemble Channel Attention Module (ECAM) proposed supervision. Through ECAM, most representative features different levels can be refined final classification. Experimental results show that our method improves greatly many evaluation criteria has better tradeoff accuracy calculation amount than state-of-the-art (SOTA) methods.

image analysis, computer science, information fusion, spatial verification, vhr images, scene understanding, computer vision, machine learning, multimedia retrieval, image sequence analysis, data science, human image synthesis, deep learning, machine learning research, siamese network, machine vision, digital image processing, change detection",2022,408,image analysis|computer science|information fusion|spatial verification|vhr images|scene understanding|computer vision|machine learning|multimedia retrieval|image sequence analysis|data science|human image synthesis|deep learning|machine learning research|siamese network|machine vision|digital image processing|change detection,
https://openalex.org/W4226178544,U2Fusion: A Unified Unsupervised Image Fusion Network,"U2Fusion: A Unified Unsupervised Image Fusion Network

This study proposes a novel unified and unsupervised end-to-end image fusion network, termed as U2Fusion, which is capable of solving different problems, including multi-modal, multi-exposure, multi-focus cases. Using feature extraction information measurement, U2Fusion automatically estimates the importance corresponding source images comes up with adaptive preservation degrees. Hence, tasks are in same framework. Based on degrees, network trained to preserve similarity between result images. Therefore, stumbling blocks applying deep learning for fusion, e.g., requirement ground-truth specifically designed metrics, greatly mitigated. By avoiding loss previous capabilities when training single model sequentially, we obtain that applicable multiple tasks. Moreover, new aligned infrared visible dataset, RoadScene (available at https://github.com/hanna-xu/RoadScene), released provide option benchmark evaluation. Qualitative quantitative experimental results three typical validate effectiveness universality U2Fusion. Our code publicly available https://github.com/hanna-xu/U2Fusion.

image analysis, computational imaging, computer science, information fusion, fusion learning, computer vision, machine learning, data fusion, feature fusion, multi-sensor information fusion, deep learning, multi-image fusion, machine learning research, digital image processing",2022,396,image analysis|computational imaging|computer science|information fusion|fusion learning|computer vision|machine learning|data fusion|feature fusion|multi-sensor information fusion|deep learning|multi-image fusion|machine learning research|digital image processing,
https://openalex.org/W3167568784,Plug-and-Play Image Restoration With Deep Denoiser Prior,"Plug-and-Play Image Restoration With Deep Denoiser Prior

Recent works on plug-and-play image restoration have shown that a denoiser can implicitly serve as the prior for model-based methods to solve many inverse problems. Such property induces considerable advantages (e.g., integrating flexibility of method and effectiveness learning-based methods) when is discriminatively learned via deep convolutional neural network (CNN) with large modeling capacity. However, while deeper larger CNN models are rapidly gaining popularity, existing hinders its performance due lack suitable prior. In order push limits restoration, we set up benchmark by training highly flexible effective denoiser. We then plug modular part into half quadratic splitting based iterative algorithm various We, meanwhile, provide thorough analysis parameter setting, intermediate results empirical convergence better understand working mechanism. Experimental three representative tasks, including deblurring, super-resolution demosaicing, demonstrate proposed not only significantly outperforms other state-of-the-art but also achieves competitive or even superior against methods. The source code available at https://github.com/cszn/DPIR.

image analysis, computational imaging, computer science, deep denoiser, image restoration, computer vision, digital restoration, plug-and-play image restoration, deblurring, digital image processing",2022,396,image analysis|computational imaging|computer science|deep denoiser|image restoration|computer vision|digital restoration|plug-and-play image restoration|deblurring|digital image processing,
